<!doctype html>
<html><head><meta charset="utf-8"><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.0/css/bootstrap.min.css"></head><body style="margin: 4em 10%"><h1>Common Test</h1><h1>Common Test</h1><h4>Scope</h4><p><strong>Common Test</strong> is a portable application for automated 
testing. It is suitable for:</p><ul><li><p>Black-box testing of target systems of any type (that
is, not necessarily implemented in Erlang). This is performed
through standard O&amp;M interfaces (such as SNMP, HTTP, CORBA,
and Telnet) and, if necessary, through user-specific interfaces
(often called test ports).</p></li><li><p>White-box testing of Erlang/OTP programs. This is easily
done by calling the target API functions directly from the test
case functions.</p></li></ul><p><strong>Common Test</strong> also integrates use of the OTP
<a href="./cover">cover</a> tool in application 
Tools for code coverage analysis of Erlang/OTP programs.</p><p><strong>Common Test</strong> executes test suite programs automatically,
without operator interaction. Test progress and results are
printed to logs in HTML format, easily browsed with a standard
web browser. <strong>Common Test</strong> also sends notifications about progress
and results through an OTP event manager to event handlers plugged
in to the system. This way, users can integrate their own
programs for, for example, logging, database storing, or supervision with
<strong>Common Test</strong>.</p><p><strong>Common Test</strong> provides libraries with useful support 
functions to fill various testing needs and requirements. 
There is, for example, support for flexible test declarations 
through test specifications. There is also support 
for central configuration and control of multiple 
independent test sessions (to different target systems)
running in parallel.</p><h4>Prerequisites</h4><p>It is assumed that the reader is familiar with the Erlang
programming language.</p><a name="basics"></a><h4>General</h4><p>The <strong>Common Test</strong> framework is a tool that supports
implementation and automated execution of test cases to any
types of target systems. <strong>Common Test</strong> is the main tool being used 
in all testing- and verification activities that are part of Erlang/OTP 
system development and maintenance.
</p><p>Test cases can be executed individually or in batches. <strong>Common Test</strong>
also features a distributed testing mode with central control and logging.
With this feature, multiple systems can be tested independently in
one common session. This is useful, for example, when running automated 
large-scale regression tests.
</p><p>
The System Under Test (SUT) can consist of one or more target
nodes. <strong>Common Test</strong> contains a generic test server that, 
together with other test utilities, is used to perform test case execution. 
The tests can be started from a GUI, from the OS shell, or from an
Erlang shell. <em>Test suites</em> are files (Erlang
modules) that contain the <em>test cases</em> (Erlang functions)
to be executed. <em>Support modules</em> provide functions
that the test cases use to do the tests.
</p><p>In a black-box testing scenario, <strong>Common Test</strong>-based test programs connect to
the target system(s) through standard O&amp;M and CLI protocols. <strong>Common Test</strong>
provides implementations of, and wrapper interfaces to, some of these
protocols (most of which exist as standalone components and
applications in OTP). The wrappers simplify configuration and add
verbosity for logging purposes. <strong>Common Test</strong> is continously extended with
useful support modules. However, notice that it is
a straightforward task to use any Erlang/OTP component
for testing purposes with <strong>Common Test</strong>, without needing a <strong>Common Test</strong> 
wrapper for it. It is as simple as calling Erlang functions. A number of 
target-independent interfaces are supported in <strong>Common Test</strong>, such as
Generic Telnet and FTP. These can be specialized or used
directly for controlling instruments, traffic load generators, and so on.
</p><p><strong>Common Test</strong> is also a very useful tool for white-box testing Erlang
code (for example, module testing), as the test programs can call exported Erlang
functions directly. There is very little overhead required for
implementing basic test suites and executing simple tests. For black-box
testing Erlang software, Erlang RPC and standard O&amp;M interfaces
can be used for example.
</p><p>A test case can handle several connections to one or
more target systems, instruments, and traffic generators in
parallel to perform the necessary actions for a test. 
The handling of many connections in parallel is one of
the major strengths of <strong>Common Test</strong>, thanks to the efficient
support for concurrency in the Erlang runtime system, which <strong>Common Test</strong> 
users can take great advantage of.
</p><h4>Test Suite Organisation</h4><p>
Test suites are organized in test directories and each test suite
can have a separate data directory. Typically, these files and directories
are version-controlled similar to other forms of source code (possibly by
a version control system like GIT or Subversion). However, <strong>Common Test</strong> 
does not itself put any requirements on (or has any awareness of) 
possible file and directory versions.
</p><h4>Support Libraries</h4><p>
Support libraries contain functions that are useful for all test suites,
or for test suites in a specific functional area or subsystem.
In addition to the general support libraries provided by the
<strong>Common Test</strong> framework, and the various libraries and applications provided by
Erlang/OTP, there can also be a need for customized (user specific) 
support libraries. 
</p><h4>Suites and Test Cases</h4><p>
Testing is performed by running test suites (sets of test cases) or 
individual test cases. A test suite is implemented as an Erlang module named 
<strong>&lt;suite_name&gt;_SUITE.erl</strong> which contains a number of test cases.
A test case is an Erlang function that tests one or more things. 
The test case is the smallest unit that the <strong>Common Test</strong> test server deals with.
</p><p>
Sets of test cases, called test case groups, can also be defined. A test case
group can have execution properties associated with it. Execution properties 
specify if the test cases in the group are to be executed in
random order, in parallel, or in sequence, and if the execution of the group 
is to be repeated. Test case groups can also be nested (that is, a group can,
besides test cases, contain subgroups).
</p><p>
Besides test cases and groups, the test suite can also contain configuration 
functions. These functions are meant to be used for setting up (and verifying)
environment and state in the SUT (and/or the <strong>Common Test</strong> host node), 
required for the tests to execute correctly. Examples of operations are: 
Opening a connection to the SUT, initializing a database, running an installation 
script, and so on. Configuration can be performed per suite, per test case group,
and per individual test case.
</p><p>
The test suite module must conform to a
<a href="common_test">callback interface</a>
specified by the <strong>Common Test</strong> test server. For details, see section
<a href="./write_test_chapter#intro">Writing Test Suites</a>.
</p><p>
A test case is considered successful if it returns to the caller, no matter 
what the returned value is. However, a few return values have special meaning
as follows:</p><ul><li><strong>{skip,Reason}</strong> indicates that the test case is skipped.</li><li><strong>{comment,Comment}</strong> prints a comment in the log for the test case.</li><li><strong>{save_config,Config}</strong> makes the <strong>Common Test</strong> test server pass  <strong>Config</strong> to the next test case.</li></ul><p>
A test case failure is specified as a runtime error (a crash), no matter what 
the reason for termination is. If you use Erlang pattern matching effectively,
you can take advantage of this property. The result is concise and 
readable test case functions that look much more like scripts than actual programs. 
A simple example:
</p><pre>
 session(_Config) -&gt;
     {started,ServerId} = my_server:start(),
     {clients,[]} = my_server:get_clients(ServerId),
     MyId = self(),
     connected = my_server:connect(ServerId, MyId),
     {clients,[MyId]} = my_server:get_clients(ServerId),
     disconnected = my_server:disconnect(ServerId, MyId),
     {clients,[]} = my_server:get_clients(ServerId),
     stopped = my_server:stop(ServerId).</pre><p>
As a test suite runs, all information (including output to <strong>stdout</strong>) is 
recorded in many different log files. A minimum of information is displayed 
in the user console (only start and stop information, plus a note 
for each failed test case).
</p><p>
The result from each test case is recorded in a dedicated HTML log file, created 
for the particular test run. An overview page displays each test case represented 
by a table row showing total execution time, if the case was successful,
failed, or skipped, plus an optional user comment. For a failed test case, the 
reason for termination is also printed in the comment field. The overview page
has a link to each test case log file, providing simple navigation with any standard
HTML browser.
</p><a name="External_Interfaces"></a><h4>External Interfaces</h4><p>
The <strong>Common Test</strong> test server requires that the test suite defines and exports the 
following mandatory or optional callback functions:
</p><dl><dt><strong>all()</strong></dt><dd><p>Returns a list of all test cases and groups in the suite. (Mandatory)</p></dd><dt><strong>suite()</strong></dt><dd><p>Information function used to return properties for the suite. (Optional)</p></dd><dt><strong>groups()</strong></dt><dd><p>For declaring test case groups. (Optional)</p></dd><dt><strong>init_per_suite(Config)</strong></dt><dd><p>Suite level configuration function, executed before the first 
test case. (Optional)</p></dd><dt><strong>end_per_suite(Config)</strong></dt><dd><p>Suite level configuration function, executed after the last 
test case. (Optional)</p></dd><dt><strong>group(GroupName)</strong></dt><dd><p>Information function used to return properties for a test case group. (Optional)</p></dd><dt><strong>init_per_group(GroupName, Config)</strong></dt><dd><p>Configuration function for a group, executed before the first 
test case. (Optional)</p></dd><dt><strong>end_per_group(GroupName, Config)</strong></dt><dd><p>Configuration function for a group, executed after the last 
test case. (Optional)</p></dd><dt><strong>init_per_testcase(TestCase, Config)</strong></dt><dd><p>Configuration function for a testcase, executed before each 
test case. (Optional)</p></dd><dt><strong>end_per_testcase(TestCase, Config)</strong></dt><dd><p>Configuration function for a testcase, executed after each 
test case. (Optional)</p></dd></dl><p>
For each test case, the <strong>Common Test</strong> test server expects the
following functions:
</p><dl><dt>Testcasename()</dt><dd><p>Information function that returns a list of test case properties. (Optional)</p></dd><dt>Testcasename(Config)</dt><dd><p>The test case function.</p></dd></dl><h4>Introduction for Newcomers</h4><p>
The purpose of this section is to let the newcomer get started in
quickly writing and executing some first simple tests with a 
"learning by example" approach. Most explanations are left for later sections. 
If you are not much into "learning by example" and prefer more technical
details, go ahead and skip to the next section.
</p><p>
This section demonstrates how simple it is to write a basic 
(yet for many module testing purposes, often sufficiently complex) 
test suite and execute its test cases. This is not necessarily
obvious when you read the remaining sections in this User's Guide.
</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>
To understand what is discussed and examplified here, we recommended 
you to first read section
<a href="./basics_chapter#basics">Common Test Basics</a>.
</p></div><h4>Test Case Execution</h4><p>Execution of test cases is handled as follows:</p><img src="tc_execution.gif" title="
	Successful and Unsuccessful Test Case Execution
"></img><p>For each test case that <strong>Common Test</strong> is ordered to execute, it spawns a
dedicated process on which the test case function starts
running. (In parallel to the test case process, an idle waiting timer
process is started, which is linked to the test case process. If the timer
process runs out of waiting time, it sends an exit signal to terminate
the test case process. This is called a <em>timetrap</em>).
</p><p>In scenario 1, the test case process terminates normally after 
<strong>case A</strong> has finished executing its test code without detecting 
any errors. The test case function returns a value and <strong>Common Test</strong> 
logs the test case as successful.
</p><p>In scenario 2, an error is detected during test <strong>case B</strong> execution.
This causes the test <strong>case B</strong> function to generate an exception
and, as a result, the test case process exits with reason other than normal. 
<strong>Common Test</strong> logs this as an unsuccessful (Failed) test case.
</p><p>As you can understand from the illustration, <strong>Common Test</strong> requires
a test case to generate a runtime error to indicate failure (for example,
by causing a bad match error or by calling <strong>exit/1</strong>, preferably
through the help function 
<a href="./ct#fail-1">ct#fail-1</a>). A successful 
execution is indicated by a normal return from the test case function.
</p><h4>A Simple Test Suite</h4><p>As shown in section 
<a href="./basics_chapter#External_Interfaces">Common Test Basics</a>,
the test suite module implements
<a href="common_test">callback functions</a>
(mandatory or optional) for various purposes, for example:
</p><ul><li>Init/end configuration function for the test suite</li><li>Init/end configuration function for a test case</li><li>Init/end configuration function for a test case group</li><li>Test cases</li></ul><p> 
The configuration functions are optional. The following example is a test suite 
without configuration functions, including one simple test case, to 
check that module <strong>mymod</strong> exists (that is, can be successfully loaded by the 
code server):
</p><pre>
 -module(my1st_SUITE).
 -compile(export_all).

 all() -&gt;
     [mod_exists].

 mod_exists(_) -&gt;
     {module,mymod} = code:load_file(mymod).</pre><p>
If the operation fails, a bad match error occurs that terminates the test case.
</p><h4>A Test Suite with Configuration Functions</h4><p>
If you need to perform configuration operations to run your test, you can
implement configuration functions in your suite. The result from a
configuration function is configuration data, or <strong>Config</strong>.
This is a list of key-value tuples that get passed from the configuration
function to the test cases (possibly through configuration functions on
"lower level"). The data flow looks as follows:
</p><img src="config.gif" title="
	Configuration Data Flow in a Suite
"></img><p>
The following example shows a test suite that uses configuration functions
to open and close a log file for the test cases (an operation that is
unnecessary and irrelevant to perform by each test case):
</p><pre>
 -module(check_log_SUITE).
 -export([all/0, init_per_suite/1, end_per_suite/1]).
 -export([check_restart_result/1, check_no_errors/1]).

 -define(value(Key,Config), proplists:get_value(Key,Config)).

 all() -&gt; [check_restart_result, check_no_errors].

 init_per_suite(InitConfigData) -&gt;
     [{logref,open_log()} | InitConfigData].

 end_per_suite(ConfigData) -&gt;
     close_log(?value(logref, ConfigData)).

 check_restart_result(ConfigData) -&gt;
     TestData = read_log(restart, ?value(logref, ConfigData)),
     {match,_Line} = search_for("restart successful", TestData).

 check_no_errors(ConfigData) -&gt;
     TestData = read_log(all, ?value(logref, ConfigData)),
     case search_for("error", TestData) of
	 {match,Line} -&gt; ct:fail({error_found_in_log,Line});
	 nomatch -&gt; ok
     end.</pre><p>
The test cases verify, by parsing a log file, that our SUT has performed 
a successful restart and that no unexpected errors are printed.
</p><p>To execute the test cases in the recent test suite, type the 
following on the UNIX/Linux command line (assuming that the suite module
is in the current working directory):
</p><pre>
 $ ct_run -dir .</pre><p>or:</p><pre>
 $ ct_run -suite check_log_SUITE</pre><p>To use the Erlang shell to run our test, you can evaluate the following call:
</p><pre>
 1&gt; ct:run_test([{dir, "."}]).</pre><p>or:</p><pre>
 1&gt; ct:run_test([{suite, "check_log_SUITE"}]).</pre><p>
The result from running the test is printed in log files in HTML format
(stored in unique log directories on a different level). The following 
illustration shows the log file structure:
</p><img src="html_logs.gif" title="
	HTML Log File Structure
"></img><h4>Questions and Answers</h4><p>Here follows some questions that you might have after reading this section 
with corresponding tips and links to the answers:
</p><ul><li><p><em>Question:</em> 
"How and where can I specify variable data for my tests that must not 
be hard-coded in the test suites (such as hostnames, addresses, and
user login data)?"</p> <p><em>Answer:</em> 
See section <a href="./config_file_chapter#top">External Configuration Data</a>.</p> </li><li><p><em>Question:</em> "Is there a way to declare different tests and run them
in one session without having to write my own scripts? Also, can such
declarations be used for regression testing?"</p> <p><em>Answer:</em> See section
<a href="./run_test_chapter#test_specifications">Test Specifications</a>
in section Running Tests and Analyzing Results.
</p> </li><li><p><em>Question:</em> "Can test cases and/or test runs be automatically repeated?"</p> <p><em>Answer:</em> Learn more about
<a href="./write_test_chapter#test_case_groups">Test Case Groups</a>
and read about start flags/options in section
<a href="./run_test_chapter#ct_run">Running Tests</a> and in
the Reference Manual.</p> </li><li><p><em>Question:</em> "Does <strong>Common Test</strong> execute my test cases in sequence or in parallel?"</p> <p><em>Answer:</em> See
<a href="./write_test_chapter#test_case_groups">Test Case Groups</a>
in section Writing Test Suites.</p> </li><li><p><em>Question:</em> "What is the syntax for timetraps (mentioned earlier), and how do I set them?"</p> <p><em>Answer:</em> This is explained in the
<a href="./write_test_chapter#timetraps">Timetrap Time-Outs</a>
part of section Writing Test Suites.</p> </li><li><p><em>Question:</em> "What functions are available for logging and printing?"</p> <p><em>Answer:</em> See
<a href="./write_test_chapter#logging">Logging</a>
in section Writing Test Suites.</p> </li><li><p><em>Question:</em> "I need data files for my tests. Where do I store them preferably?"</p> <p><em>Answer:</em> See
<a href="./write_test_chapter#data_priv_dir">Data and Private Directories</a>.</p> </li><li><p><em>Question:</em> "Can I start with a test suite example, please?"</p> <p><em>Answer:</em> <a href="./example_chapter#top">Welcome!</a></p> </li></ul><p>You probably want to get started on your own first test suites now, while
at the same time digging deeper into the <strong>Common Test</strong> User's Guide and Reference Manual.
There are much more to learn about the things that have been introduced
in this section. There are also many other useful features to learn, 
so please continue to the other sections and have fun.
</p><a name="general"></a><h4>General Information</h4><p>The two main interfaces for running tests with <strong>Common Test</strong>
are an executable program named 
<a href="ct_run">ct_run</a> and the
Erlang module <a href="ct">ct</a>. 
<strong>ct_run</strong> is compiled for the underlying operating system (for example,
Unix/Linux or Windows) during the build of the Erlang/OTP system, 
and is installed automatically with other executable programs in
the top level <strong>bin</strong> directory of Erlang/OTP.
The <strong>ct</strong> interface functions can be called from the Erlang shell,
or from any Erlang function, on any supported platform.</p><p>The <strong>Common Test</strong> application is installed with the Erlang/OTP
system. No extra installation step is required to start using
<strong>Common Test</strong> through the <strong>ct_run</strong> executable program, 
and/or the interface functions in the <strong>ct</strong> module.</p><a name="intro"></a><h4>Support for Test Suite Authors</h4><p>The <a href="ct">ct</a> module provides the main 
interface for writing test cases. This includes for example, the following:</p><ul><li>Functions for printing and logging</li><li>Functions for reading configuration data</li><li>Function for terminating a test case with error reason</li><li>Function for adding comments to the HTML overview page</li></ul><p>For details about these functions, see module <a href="ct">ct</a>.</p><p>The <strong>Common Test</strong> application also includes other modules named 
<strong>ct_&lt;component&gt;</strong>, which
provide various support, mainly simplified use of communication
protocols such as RPC, SNMP, FTP, Telnet, and others.</p><h4>Test Suites</h4><p>A test suite is an ordinary Erlang module that contains test
cases. It is recommended that the module has a name on the form
<strong>*_SUITE.erl</strong>. Otherwise, the directory and auto compilation 
function in <strong>Common Test</strong> cannot locate it (at least not by default).
</p><p>It is also recommended that the <strong>ct.hrl</strong> header file is included
in all test suite modules.
</p><p>Each test suite module must export function 
<a href="./common_test#Module:all-0">common_test#Module:all-0</a>,
which returns the list of all test case groups and test cases 
to be executed in that module. 
</p><p>The callback functions to be implemented by the test suite are
all listed in module <a href="common_test">common_test </a>. They are also described in more detail later in this User's Guide.
</p><h4>Init and End per Suite</h4><p>Each test suite module can contain the optional configuration functions
<a href="./common_test#Module:init_per_suite-1">common_test#Module:init_per_suite-1</a>
and <a href="./common_test#Module:end_per_suite-1">common_test#Module:end_per_suite-1</a>. 
If the init function is defined, so must the end function be.
</p><p>If <strong>init_per_suite</strong> exists, it is called initially before the
test cases are executed. It typically contains initializations common
for all test cases in the suite, which are only to be performed once. 
<strong>init_per_suite</strong> is recommended for setting up and verifying state 
and environment on the System Under Test (SUT) or the <strong>Common Test</strong> 
host node, or both, so that the test cases in the suite executes correctly. 
The following are examples of initial configuration operations:
</p><ul><li>Opening a connection to the SUT</li><li>Initializing a database</li><li>Running an installation script</li></ul><p><strong>end_per_suite</strong> is called as the final stage of the test suite execution
(after the last test case has finished). The function is meant to be used 
for cleaning up after <strong>init_per_suite</strong>. 
</p><p><strong>init_per_suite</strong> and <strong>end_per_suite</strong> execute on dedicated
Erlang processes, just like the test cases do. The result of these functions
is however not included in the test run statistics of successful, failed, and
skipped cases.
</p><p>The argument to <strong>init_per_suite</strong> is <strong>Config</strong>, that is, the
same key-value list of runtime configuration data that each test case takes
as input argument. <strong>init_per_suite</strong> can modify this parameter with 
information that the test cases need. The possibly modified <strong>Config</strong>
list is the return value of the function.
</p><p>If <strong>init_per_suite</strong> fails, all test cases in the test
suite are skipped automatically (so called <em>auto skipped</em>), 
including <strong>end_per_suite</strong>.
</p><p>Notice that if <strong>init_per_suite</strong> and <strong>end_per_suite</strong> do not exist
in the suite, <strong>Common Test</strong> calls dummy functions (with the same names)
instead, so that output generated by hook functions can be saved to the log
files for these dummies. For details, see
<a href="./ct_hooks_chapter#manipulating">Common Test Hooks</a>.
</p><a name="per_testcase"></a><h4>Init and End per Test Case</h4><p>Each test suite module can contain the optional configuration functions
<a href="./common_test#Module:init_per_testcase-2">common_test#Module:init_per_testcase-2</a>
and <a href="./common_test#Module:end_per_testcase-2">common_test#Module:end_per_testcase-2</a>. 
If the init function is defined, so must the end function be.</p><p>If <strong>init_per_testcase</strong> exists, it is called before each
test case in the suite. It typically contains initialization that
must be done for each test case (analog to <strong>init_per_suite</strong> for the 
suite).</p><p><strong>end_per_testcase/2</strong> is called after each test case has
finished, enabling cleanup after <strong>init_per_testcase</strong>.</p><p>The first argument to these functions is the name of the test
case. This value can be used with pattern matching in function clauses
or conditional expressions to choose different initialization and cleanup
routines for different test cases, or perform the same routine for many,
or all, test cases.</p><p>The second argument is the <strong>Config</strong> key-value list of runtime
configuration data, which has the same value as the list returned by
<strong>init_per_suite</strong>. <strong>init_per_testcase/2</strong> can modify this
parameter or return it "as is". The return value of <strong>init_per_testcase/2</strong> 
is passed as parameter <strong>Config</strong> to the test case itself.</p><p>The return value of <strong>end_per_testcase/2</strong> is ignored by the
test server, with exception of the 
<a href="./dependencies_chapter#save_config">dependencies_chapter#save_config</a>
and <strong>fail</strong> tuple.</p><p><strong>end_per_testcase</strong> can check if the test case was successful. 
(which in turn can determine how cleanup is to be performed). 
This is done by reading the value tagged with <strong>tc_status</strong> from 
<strong>Config</strong>. The value is one of the following:
</p><ul><li> <p><strong>ok</strong></p> </li><li> <p><strong>{failed,Reason}</strong></p> <p>where <strong>Reason</strong> is <strong>timetrap_timeout</strong>, information from <strong>exit/1</strong>, 
or details of a runtime error</p></li><li> <p><strong>{skipped,Reason}</strong></p> <p>where <strong>Reason</strong> is a user-specific term</p></li></ul><p>Function <strong>end_per_testcase/2</strong> is even called if a
test case terminates because of a call to 
<a href="./ct#abort_current_testcase-1">ct#abort_current_testcase-1</a>,
or after a timetrap time-out. However, <strong>end_per_testcase</strong>
then executes on a different process than the test case
function. In this situation, <strong>end_per_testcase</strong> cannot
change the reason for test case termination by returning <strong>{fail,Reason}</strong>
or save data with <strong>{save_config,Data}</strong>.</p><p>The test case is skipped in the following two cases:
</p><ul><li>If <strong>init_per_testcase</strong> crashes (called <em>auto skipped</em>).</li><li>If <strong>init_per_testcase</strong> returns a tuple <strong>{skip,Reason}</strong>  (called <em>user skipped</em>).</li></ul><p>The test case can also be marked as failed without executing it
by returning a tuple <strong>{fail,Reason}</strong> from <strong>init_per_testcase</strong>.</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>If <strong>init_per_testcase</strong> crashes, or returns <strong>{skip,Reason}</strong>
or <strong>{fail,Reason}</strong>, function <strong>end_per_testcase</strong> is not called.
</p></div><p>If it is determined during execution of <strong>end_per_testcase</strong> that
the status of a successful test case is to be changed to failed, 
<strong>end_per_testcase</strong> can return the tuple <strong>{fail,Reason}</strong>
(where <strong>Reason</strong> describes why the test case fails).</p><p>As <strong>init_per_testcase</strong> and <strong>end_per_testcase</strong> execute on the
same Erlang process as the test case, printouts from these
configuration functions are included in the test case log file.</p><a name="test_cases"></a><h4>Test Cases</h4><p>The smallest unit that the test server is concerned with is a
test case. Each test case can test many things, for
example, make several calls to the same interface function with
different parameters.
</p><p>The author can choose to put many or few tests into each test
case. Some things to keep in mind follows:
</p><ul><li><p>Many small test cases tend to result in extra, and possibly
duplicated code, as well as slow test execution because of
large overhead for initializations and cleanups. Avoid duplicated 
code, for example, by using common help functions. Otherwise,
the resulting suite becomes difficult to read and understand, and
expensive to maintain.
</p></li><li><p>Larger test cases make it harder to tell what went wrong if it
fails. Also, large portions of test code risk being skipped
when errors occur.</p> </li><li><p>Readability and maintainability suffer 
when test cases become too large and extensive. It is not certain 
that the resulting log files reflect very well the  number of tests 
performed.
</p></li></ul><p>The test case function takes one argument, <strong>Config</strong>, which
contains configuration information such as <strong>data_dir</strong> and
<strong>priv_dir</strong>. (For details about these, see section 
<a href="#data_priv_dir">Data and Private Directories</a>.
The value of <strong>Config</strong> at the time of the call, is the same 
as the return value from <strong>init_per_testcase</strong>, mentioned earlier.
</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>The test case function argument <strong>Config</strong> is not to be 
confused with the information that can be retrieved from the
configuration files (using <a href="./ct#get_config-1">ct#get_config-1</a>). The test case argument <strong>Config</strong>
is to be used for runtime configuration of the test suite and the 
test cases, while configuration files are to contain data 
related to the SUT. These two types of configuration data are handled 
differently.</p></div><p>As parameter <strong>Config</strong> is a list of key-value tuples, that is,
a data type called a property list, it can be handled by the
<a href="./proplists">stdlib/proplists</a> module.
A value can, for example, be searched for and returned with function 
<a href="../stdlib/proplists#get_value-2">stdlib/proplists#get_value-2</a>.
Also, or alternatively, the general <a href="./lists">stdlib/lists</a>
module contains useful functions. Normally, the only operations 
performed on <strong>Config</strong> is insert (adding a tuple to the head of the list) 
and lookup. <strong>Common Test</strong> provides a simple macro named <strong>?config</strong>, 
which returns a value of an item in <strong>Config</strong> given the key (exactly like 
<strong>proplists:get_value</strong>). Example: <strong>PrivDir = ?config(priv_dir, Config)</strong>.
</p><p>If the test case function crashes or exits purposely, it is considered 
<em>failed</em>. If it returns a value (no matter what value), it is 
considered successful. An exception to this rule is the return value 
<strong>{skip,Reason}</strong>. If this tuple is returned, the test case is considered 
skipped and is logged as such.</p><p>If the test case returns the tuple <strong>{comment,Comment}</strong>, the case
is considered successful and <strong>Comment</strong> is printed in the overview 
log file. This is equal to calling 
<a href="./ct#comment-1">ct#comment-1</a>.
</p><a name="info_function"></a><h4>Test Case Information Function</h4><p>For each test case function there can be an extra function
with the same name but without arguments. This is the test case
information function. It is expected to return a list of tagged 
tuples that specifies various properties regarding the test case.
</p><p>The following tags have special meaning:</p><dl><dt><strong>timetrap</strong></dt><dd> <p>
Sets the maximum time the test case is allowed to execute. If
this time is exceeded, the test case fails with
reason <strong>timetrap_timeout</strong>. Notice that <strong>init_per_testcase</strong> 
and <strong>end_per_testcase</strong> are included in the timetrap time.
For details, see section 
<a href="./write_test_chapter#timetraps">Timetrap Time-Outs</a>.
</p> </dd><dt><strong>userdata</strong></dt><dd> <p>
Specifies any data related to the test case. This
data can be retrieved at any time using the 
<a href="./ct#userdata-3">ct#userdata-3</a>
utility function.
</p> </dd><dt><strong>silent_connections</strong></dt><dd> <p>
For details, see section 
<a href="./run_test_chapter#silent_connections">Silent Connections</a>.
</p> </dd><dt><strong>require</strong></dt><dd> <p>
Specifies configuration variables required by the
test case. If the required configuration variables are not
found in any of the test system configuration files, the test case is
skipped.</p>  <p>
A required variable can also be given a default value to 
be used if the variable is not found in any configuration file. To specify 
a default value, add a tuple on the form 
<strong>{default_config,ConfigVariableName,Value}</strong> to the test case information list 
(the position in the list is irrelevant).
</p> <p><em>Examples:</em></p> <pre>
 testcase1() -&gt; 
     [{require, ftp},
      {default_config, ftp, [{ftp, "my_ftp_host"},
                             {username, "aladdin"},
                             {password, "sesame"}]}}].</pre> <pre>
 testcase2() -&gt; 
     [{require, unix_telnet, unix},
      {require, {unix, [telnet, username, password]}},
      {default_config, unix, [{telnet, "my_telnet_host"},
                              {username, "aladdin"},
                              {password, "sesame"}]}}].</pre> </dd></dl><p>For more information about <strong>require</strong>, see section
<a href="./config_file_chapter#require_config_data"> Requiring and Reading Configuration Data</a>
in section External Configuration Data and function 
<a href="./ct#require-1">ct#require-1</a>.</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>Specifying a default value for a required variable can result
in a test case always getting executed. This might not be a desired behavior.</p></div><p>If <strong>timetrap</strong> or <strong>require</strong>, or both, is not set specifically for
a particular test case, default values specified by function
<a href="./common_test#Module:suite-0">common_test#Module:suite-0</a> 
are used.
</p><p>Tags other than the earlier mentioned are ignored by the test server.
</p><p>
An example of a test case information function follows:
</p><pre>
 reboot_node() -&gt;
     [
      {timetrap,{seconds,60}},
      {require,interfaces},
      {userdata,
          [{description,"System Upgrade: RpuAddition Normal RebootNode"},
           {fts,"http://someserver.ericsson.se/test_doc4711.pdf"}]}                  
     ].</pre><a name="suite"></a><h4>Test Suite Information Function</h4><p>Function <a href="./common_test#Module:suite-0">common_test#Module:suite-0</a> 
can, for example, be used in a test suite module to set a default 
<strong>timetrap</strong> value and to <strong>require</strong> external configuration data. 
If a test case, or a group information function also specifies any of the information tags, it
overrides the default values set by <strong>suite/0</strong>. For details, 
see 
<a href="#info_function">Test Case Information Function</a> and
<a href="#test_case_groups">Test Case Groups</a>.
</p><p>The following options can also be specified with the suite information list:</p><ul><li><strong>stylesheet</strong>,  see <a href="./run_test_chapter#html_stylesheet">HTML Style Sheets</a></li><li><strong>userdata</strong>,  see <a href="#info_function">Test Case Information Function</a></li><li><strong>silent_connections</strong>,  see <a href="./run_test_chapter#silent_connections">Silent Connections</a></li></ul><p>
An example of the suite information function follows:
</p><pre>
 suite() -&gt;
     [
      {timetrap,{minutes,10}},
      {require,global_names},
      {userdata,[{info,"This suite tests database transactions."}]},
      {silent_connections,[telnet]},
      {stylesheet,"db_testing.css"}
     ].</pre><a name="test_case_groups"></a><h4>Test Case Groups</h4><p>A test case group is a set of test cases sharing configuration 
functions and execution properties. Test case groups are defined by
function 
<a href="./common_test#Module:groups-0">common_test#Module:groups-0</a>
according to the following syntax:</p><pre>
 groups() -&gt; GroupDefs

 Types:

 GroupDefs = [GroupDef]
 GroupDef = {GroupName,Properties,GroupsAndTestCases}
 GroupName = atom()
 GroupsAndTestCases = [GroupDef | {group,GroupName} | TestCase |
                      {testcase,TestCase,TCRepeatProps}]
 TestCase = atom()
 TCRepeatProps = [{repeat,N} | {repeat_until_ok,N} | {repeat_until_fail,N}]</pre><p><strong>GroupName</strong> is the name of the group and must be unique within
the test suite module. Groups can be nested, by including a group definition 
within the <strong>GroupsAndTestCases</strong> list of another group. 
<strong>Properties</strong> is the list of execution 
properties for the group. The possible values are as follows:</p><pre>
 Properties = [parallel | sequence | Shuffle | {GroupRepeatType,N}]
 Shuffle = shuffle | {shuffle,Seed}
 Seed = {integer(),integer(),integer()}
 GroupRepeatType = repeat | repeat_until_all_ok | repeat_until_all_fail |
                   repeat_until_any_ok | repeat_until_any_fail
 N = integer() | forever</pre><p><em>Explanations:</em></p><dl><dt><strong>parallel</strong></dt><dd><p><strong>Common Test</strong> executes all test cases in the group in parallel.</p></dd><dt><strong>sequence</strong></dt><dd><p>The cases are executed in a sequence as described in section
<a href="./dependencies_chapter#sequences">Sequences</a> in section
Dependencies Between Test Cases and Suites.</p></dd><dt><strong>shuffle</strong></dt><dd><p>The cases in the group are executed in random order.</p></dd><dt><strong>repeat, repeat_until_*</strong></dt><dd><p>Orders <strong>Common Test</strong> to repeat execution of all the cases in the 
group a given number of times, or until any, or all, cases fail or succeed.</p></dd></dl><p><em>Example:</em></p><pre>
 groups() -&gt; [{group1, [parallel], [test1a,test1b]},
              {group2, [shuffle,sequence], [test2a,test2b,test2c]}].</pre><p>To specify in which order groups are to be executed (also with respect
to test cases that are not part of any group), add tuples on the form 
<strong>{group,GroupName}</strong> to the <strong>all/0</strong> list.</p><p><em>Example:</em></p><pre>
 all() -&gt; [testcase1, {group,group1}, {testcase,testcase2,[{repeat,10}]}, {group,group2}].</pre><p>Execution properties with a group tuple in 
<strong>all/0</strong>: <strong>{group,GroupName,Properties}</strong> can also be specified. 
These properties override those specified in the group definition (see
<strong>groups/0</strong> earlier). This way, the same set of tests can be run,
but with different properties, without having to make copies of the group
definition in question.</p><p>If a group contains subgroups, the execution properties for these can
also be specified in the group tuple:
<strong>{group,GroupName,Properties,SubGroups}</strong>
Where, <strong>SubGroups</strong> is a list of tuples, <strong>{GroupName,Properties}</strong> or
<strong>{GroupName,Properties,SubGroups}</strong> representing the subgroups.
Any subgroups defined in <strong>group/0</strong> for a group, that are not specified
in the <strong>SubGroups</strong> list, executes with their predefined
properties.</p><p><em>Example:</em></p><pre>
 groups() -&gt; {tests1, [], [{tests2, [], [t2a,t2b]},
                           {tests3, [], [t31,t3b]}]}.</pre><p>To execute group <strong>tests1</strong> twice with different properties for <strong>tests2</strong>
each time:</p><pre>
 all() -&gt;
    [{group, tests1, default, [{tests2, [parallel]}]},
     {group, tests1, default, [{tests2, [shuffle,{repeat,10}]}]}].</pre><p>This is equivalent to the following specification:</p><pre>
 all() -&gt;
    [{group, tests1, default, [{tests2, [parallel]},
                               {tests3, default}]},
     {group, tests1, default, [{tests2, [shuffle,{repeat,10}]},
                               {tests3, default}]}].</pre><p>Value <strong>default</strong> states that the predefined properties
are to be used.</p><p>The following example shows how to override properties in a scenario
with deeply nested groups:</p><pre>
 groups() -&gt;
    [{tests1, [], [{group, tests2}]},
     {tests2, [], [{group, tests3}]},
     {tests3, [{repeat,2}], [t3a,t3b,t3c]}].

 all() -&gt;
    [{group, tests1, default, 
      [{tests2, default,
        [{tests3, [parallel,{repeat,100}]}]}]}].</pre><p>The described syntax can also be used in test specifications
to change group properties at the time of execution,
without having to edit the test suite. For more information, see
section <a href="./run_test_chapter#test_specifications">Test Specifications</a> in section Running Tests and Analyzing Results.</p><p>As illustrated, properties can be combined. If, for example,
<strong>shuffle</strong>, <strong>repeat_until_any_fail</strong>, and <strong>sequence</strong>
are all specified, the test cases in the group are executed
repeatedly, and in random order, until a test case fails. Then
execution is immediately stopped and the remaining cases are skipped.</p><p>Before execution of a group begins, the configuration function
<a href="./common_test#Module:init_per_group-2">common_test#Module:init_per_group-2</a> 
is called. The list of tuples returned from this function is passed to the 
test cases in the usual manner by argument <strong>Config</strong>. 
<strong>init_per_group/2</strong> is meant to be used for initializations common 
for the test cases in the group. After execution of the group is finished, function
<a href="./common_test#Module:end_per_group-2">common_test#Module:end_per_group-2</a> 
is called. This function is meant to be used for cleaning up after 
<strong>init_per_group/2</strong>. If the init function is defined, so must the end function be.</p><p>Whenever a group is executed, if <strong>init_per_group</strong> and
<strong>end_per_group</strong> do not exist in the suite, <strong>Common Test</strong> calls
dummy functions (with the same names) instead. Output generated by
hook functions are saved to the log files for these dummies.
For more information, see section 
<a href="./ct_hooks_chapter#manipulating">Manipulating Tests</a>
in section Common Test Hooks.
</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p><strong>init_per_testcase/2</strong> and <strong>end_per_testcase/2</strong>
are always called for each individual test case, no matter if the case 
belongs to a group or not.</p></div><p>The properties for a group are always printed in the top of the HTML log 
for <strong>init_per_group/2</strong>. The total execution time for a group is
included at the bottom of the log for <strong>end_per_group/2</strong>.</p><p>Test case groups can be nested so sets of groups can be 
configured with the same <strong>init_per_group/2</strong> and <strong>end_per_group/2</strong>
functions. Nested groups can be defined by including a group definition,
or a group name reference, in the test case list of another group.</p><p><em>Example:</em></p><pre>
 groups() -&gt; [{group1, [shuffle], [test1a,
                                   {group2, [], [test2a,test2b]},
                                   test1b]},
              {group3, [], [{group,group4},
                            {group,group5}]},
              {group4, [parallel], [test4a,test4b]},
              {group5, [sequence], [test5a,test5b,test5c]}].</pre><p>In the previous example, if <strong>all/0</strong> returns group name references
in the order <strong>[{group,group1},{group,group3}]</strong>, the order of the 
configuration functions and test cases becomes the following (notice that
<strong>init_per_testcase/2</strong> and <strong>end_per_testcase/2:</strong> are also
always called, but not included in this example for simplification):</p><pre>
 init_per_group(group1, Config) -&gt; Config1  (*)
      test1a(Config1)
      init_per_group(group2, Config1) -&gt; Config2
           test2a(Config2), test2b(Config2)
      end_per_group(group2, Config2)
      test1b(Config1)
 end_per_group(group1, Config1) 
 init_per_group(group3, Config) -&gt; Config3
      init_per_group(group4, Config3) -&gt; Config4
           test4a(Config4), test4b(Config4)  (**)
      end_per_group(group4, Config4)
      init_per_group(group5, Config3) -&gt; Config5
           test5a(Config5), test5b(Config5), test5c(Config5)
      end_per_group(group5, Config5)
 end_per_group(group3, Config3)</pre><p>(*) The order of test case <strong>test1a</strong>, <strong>test1b</strong>, and <strong>group2</strong> is
undefined, as <strong>group1</strong> has a shuffle property.</p><p>(**) These cases are not executed in order, but in parallel.</p><p>Properties are not inherited from top-level groups to nested 
subgroups. For instance, in the previous example, the test cases in <strong>group2</strong> 
are not executed in random order (which is the property of <strong>group1</strong>).</p><h4>Parallel Property and Nested Groups</h4><p>If a group has a parallel property, its test cases are spawned
simultaneously and get executed in parallel. However, a test case is not 
allowed to execute in parallel with <strong>end_per_group/2</strong>, which means
that the time to execute a parallel group is equal to the
execution time of the slowest test case in the group. A negative side
effect of running test cases in parallel is that the HTML summary pages
are not updated with links to the individual test case logs until function 
<strong>end_per_group/2</strong> for the group has finished.</p><p>A group nested under a parallel group starts executing in parallel 
with previous (parallel) test cases (no matter what properties the nested 
group has). However, as test cases are never executed in parallel with 
<strong>init_per_group/2</strong> or <strong>end_per_group/2</strong> of the same group, it is 
only after a nested group has finished that remaining parallel cases 
in the previous group become spawned.</p><h4>Parallel Test Cases and I/O</h4><p>A parallel test case has a private I/O server as its group leader. 
(For a description of the group leader concept, see
<a href="./index">ERTS</a>).
The central I/O server process, which handles the output from 
regular test cases and configuration functions, does not respond to I/O messages
during execution of parallel groups. This is important to understand
to avoid certain traps, like the following:</p><p>If a process, <strong>P</strong>, is spawned during execution of, for example,
<strong>init_per_suite/1</strong>, it inherits the group leader of the
<strong>init_per_suite</strong> process. This group leader is the central I/O server
process mentioned earlier. If, at a later time, <em>during parallel test case execution</em>, some event triggers process <strong>P</strong> to call
<strong>io:format/1/2</strong>, that call never returns (as the group leader
is in a non-responsive state) and causes <strong>P</strong> to hang.
</p><h4>Repeated Groups</h4><a name="repeated_groups"></a><p>A test case group can be repeated a certain number of times
(specified by an integer) or indefinitely (specified by <strong>forever</strong>).
The repetition can also be stopped too early if any or all cases
fail or succeed, that is, if any of the properties <strong>repeat_until_any_fail</strong>,
<strong>repeat_until_any_ok</strong>, <strong>repeat_until_all_fail</strong>, or 
<strong>repeat_until_all_ok</strong> is used. If the basic <strong>repeat</strong>
property is used, status of test cases is irrelevant for the repeat 
operation.</p><p>The status of a subgroup can be returned (<strong>ok</strong> or
<strong>failed</strong>), to affect the execution of the group on the level above. 
This is accomplished by, in <strong>end_per_group/2</strong>, looking up the value
of <strong>tc_group_properties</strong> in the <strong>Config</strong> list and checking the
result of the test cases in the group. If status <strong>failed</strong> is to be
returned from the group as a result, <strong>end_per_group/2</strong> is to return
the value <strong>{return_group_result,failed}</strong>. The status of a subgroup
is taken into account by <strong>Common Test</strong> when evaluating if execution of a
group is to be repeated or not (unless the basic <strong>repeat</strong>
property is used).</p><p>The value of <strong>tc_group_properties</strong> is a list of status tuples, 
each with the key <strong>ok</strong>, <strong>skipped</strong>, and <strong>failed</strong>. The
value of a status tuple is a list with names of test cases 
that have been executed with the corresponding status as result.</p><p>The following is an example of how to return the status from a group:</p><pre>
 end_per_group(_Group, Config) -&gt;
     Status = ?config(tc_group_result, Config),
     case proplists:get_value(failed, Status) of
         [] -&gt;                                   % no failed cases 
             {return_group_result,ok};
         _Failed -&gt;                              % one or more failed
             {return_group_result,failed}
     end.</pre><p>It is also possible, in <strong>end_per_group/2</strong>, to check the status of
a subgroup (maybe to determine what status the current group is to
return). This is as simple as illustrated in the previous example, only the
group name is stored in a tuple <strong>{group_result,GroupName}</strong>,
which can be searched for in the status lists.</p><p><em>Example:</em></p><pre>
 end_per_group(group1, Config) -&gt;
     Status = ?config(tc_group_result, Config),
     Failed = proplists:get_value(failed, Status),
     case lists:member({group_result,group2}, Failed) of
           true -&gt;
               {return_group_result,failed};
           false -&gt;                                                    
               {return_group_result,ok}
     end; 
 ...</pre><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>When a test case group is repeated, the configuration 
functions <strong>init_per_group/2</strong> and <strong>end_per_group/2</strong> are 
also always called with each repetition.</p></div><h4>Shuffled Test Case Order</h4><p>The order in which test cases in a group are executed is under normal
circumstances the same as the order specified in the test case list 
in the group definition. With property <strong>shuffle</strong> set, however,
<strong>Common Test</strong> instead executes the test cases in random order.</p><p>You can provide a seed value (a tuple of three integers) with
the shuffle property <strong>{shuffle,Seed}</strong>. This way, the same shuffling
order can be created every time the group is executed. If no seed value
is specified, <strong>Common Test</strong> creates a "random" seed for the shuffling operation 
(using the return value of <strong>erlang:timestamp/0</strong>). The seed value is always
printed to the <strong>init_per_group/2</strong> log file so that it can be used to
recreate the same execution order in a subsequent test run.</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>If a shuffled test case group is repeated, the seed is not
reset between turns.</p></div><p>If a subgroup is specified in a group with a <strong>shuffle</strong> property,
the execution order of this subgroup in relation to the test cases 
(and other subgroups) in the group, is random. The order of the
test cases in the subgroup is however not random (unless the 
subgroup has a <strong>shuffle</strong> property).</p><a name="group_info"></a><h4>Group Information Function</h4><p>The test case group information function, <strong>group(GroupName)</strong>,
serves the same purpose as the suite- and test case information
functions previously described. However, the scope for
the group information function, is all test cases and subgroups in the
group in question (<strong>GroupName</strong>).</p><p><em>Example:</em></p><pre>
 group(connection_tests) -&gt;
    [{require,login_data},
     {timetrap,1000}].</pre><p>The group information properties override those set with the
suite information function, and can in turn be overridden by test
case information properties. For a list of valid information properties 
and more general information, see the
<a href="#info_function">Test Case Information Function</a>.
</p><h4>Information Functions for Init- and End-Configuration</h4><p>Information functions can also be used for functions <strong>init_per_suite</strong>,
<strong>end_per_suite</strong>, <strong>init_per_group</strong>, and <strong>end_per_group</strong>,
and they work the same way as with the
<a href="#info_function">Test Case Information Function</a>. 
This is useful, for example, for setting timetraps and requiring 
external configuration data relevant only for the configuration 
function in question (without affecting properties set for groups 
and test cases in the suite).</p><p>The information function <strong>init/end_per_suite()</strong> is called for
<strong>init/end_per_suite(Config)</strong>, and information function
<strong>init/end_per_group(GroupName)</strong> is called for
<strong>init/end_per_group(GroupName,Config)</strong>. However, information functions
cannot be used with <strong>init/end_per_testcase(TestCase, Config)</strong>,
as these configuration functions execute on the test case process
and use the same properties as the test case (that is, the properties
set by the test case information function, <strong>TestCase()</strong>). For a list 
of valid information properties and more general information, see the
<a href="#info_function">Test Case Information Function</a>.
</p><a name="data_priv_dir"></a><h4>Data and Private Directories</h4><p>In the data directory, <strong>data_dir</strong>, the test module has 
its own files needed for the testing. The name of <strong>data_dir</strong> 
is the the name of the test suite followed by <strong>"_data"</strong>. 
For example, <strong>"some_path/foo_SUITE.beam"</strong> has the data directory
<strong>"some_path/foo_SUITE_data/"</strong>. Use this directory for portability,
that is, to avoid hardcoding directory names in your suite. As the data
directory is stored in the same directory as your test suite, you can
rely on its existence at runtime, even if the path to your
test suite directory has changed between test suite implementation and
execution.
</p><p>
<strong>priv_dir</strong> is the private directory for the test cases.
This directory can be used whenever a test case (or configuration function)
needs to write something to file. The name of the private directory is
generated by <strong>Common Test</strong>, which also creates the directory.
</p><p>By default, <strong>Common Test</strong> creates one central private directory
per test run, shared by all test cases. This is not always suitable.
Especially if the same test cases are executed multiple times during
a test run (that is, if they belong to a test case group with property
<strong>repeat</strong>) and there is a risk that files in the private directory get
overwritten. Under these circumstances, <strong>Common Test</strong> can be 
configured to create one dedicated private directory per
test case and execution instead. This is accomplished with
the flag/option <strong>create_priv_dir</strong> (to be used with the
<a href="ct_run">ct_run</a> program, the 
<a href="./ct#run_test-1">ct#run_test-1</a> function, or
as test specification term). There are three possible values
for this option as follows:
</p><ul><li><strong>auto_per_run</strong></li><li><strong>auto_per_tc</strong></li><li><strong>manual_per_tc</strong></li></ul><p>
The first value indicates the default <strong>priv_dir</strong> behavior, that is,
one private directory created per test run. The two latter
values tell <strong>Common Test</strong> to generate a unique test directory name
per test case and execution. If the auto version is used, <em>all</em>
private directories are created automatically. This can become very 
inefficient for test runs with many test cases or repetitions, or both. 
Therefore, if the manual version is used instead, the test case must tell 
<strong>Common Test</strong> to create <strong>priv_dir</strong> when it needs it.
It does this by calling the function 
<a href="./ct#make_priv_dir-0">ct#make_priv_dir-0</a>.
</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>Do not depend on the current working directory for
reading and writing data files, as this is not portable. All 
scratch files are to be written in the <strong>priv_dir</strong> and all 
data files are to be located in <strong>data_dir</strong>. Also, 
the <strong>Common Test</strong> server sets the current working directory to 
the test case log directory at the start of every case.
</p></div><h4>Execution Environment</h4><p>Each test case is executed by a dedicated Erlang process. The
process is spawned when the test case starts, and terminated when
the test case is finished. The configuration functions 
<strong>init_per_testcase</strong> and <strong>end_per_testcase</strong> execute on the 
same process as the test case.
</p><p>The configuration functions <strong>init_per_suite</strong> and 
<strong>end_per_suite</strong> execute, like test cases, on dedicated Erlang
processes.
</p><a name="timetraps"></a><h4>Timetrap Time-Outs</h4><p>The default time limit for a test case is 30 minutes, unless a
<strong>timetrap</strong> is specified either by the suite-, group-,
or test case information function. The timetrap time-out value defined by
<strong>suite/0</strong> is the value that is used for each test case
in the suite (and for the configuration functions
<strong>init_per_suite/1</strong>, <strong>end_per_suite/1</strong>, <strong>init_per_group/2</strong>,
and <strong>end_per_group/2</strong>). A timetrap value defined by
<strong>group(GroupName)</strong> overrides one defined by <strong>suite()</strong>
and is used for each test case in group <strong>GroupName</strong>, and any
of its subgroups. If a timetrap value is defined by <strong>group/1</strong>
for a subgroup, it overrides that of its higher level groups. Timetrap
values set by individual test cases (by the test case information
function) override both group- and suite- level timetraps.</p><p>A timetrap can also be set or reset dynamically during the
execution of a test case, or configuration function. 
This is done by calling
<a href="./ct#timetrap-1">ct#timetrap-1</a>. 
This function cancels the current timetrap and starts a new one 
(that stays active until time-out, or end of the current function).</p><p>Timetrap values can be extended with a multiplier value specified at
startup with option <strong>multiply_timetraps</strong>. It is also possible
to let the test server decide to scale up timetrap time-out values
automatically. That is, if tools such as <strong>cover</strong> or <strong>trace</strong> 
are running during the test. This feature is disabled by default and 
can be enabled with start option <strong>scale_timetraps</strong>.</p><p>If a test case needs to suspend itself for a time that also gets
multipled by <strong>multiply_timetraps</strong> (and possibly also scaled up if
<strong>scale_timetraps</strong> is enabled), the function 
<a href="./ct#sleep-1">ct#sleep-1</a>
can be used (instead of, for example, <strong>timer:sleep/1</strong>).</p><p>A function (<strong>fun/0</strong> or <strong>{Mod,Func,Args}</strong> (MFA) tuple) can be 
specified as timetrap value in the suite-, group- and test case information 
function, and as argument to function 
<a href="./ct#timetrap-1">ct#timetrap-1</a>.</p><p><em>Examples:</em></p><p><strong>{timetrap,{my_test_utils,timetrap,[?MODULE,system_start]}}</strong></p><p><strong>ct:timetrap(fun() -&gt; my_timetrap(TestCaseName, Config) end)</strong></p><p>The user timetrap function can be used for two things as follows:</p><ul><li>To act as a timetrap. The time-out is triggered when the function returns.</li><li>To return a timetrap time value (other than a function).</li></ul><p>Before execution of the timetrap function (which is performed
on a parallel, dedicated timetrap process), <strong>Common Test</strong> cancels
any previously set timer for the test case or configuration function.    
When the timetrap function returns, the time-out is triggered, <em>unless</em>
the return value is a valid timetrap time, such as an integer,
or a <strong>{SecMinOrHourTag,Time}</strong> tuple (for details, see module
<a href="common_test">common_test</a>). If a time value 
is returned, a new timetrap is started to generate a time-out after 
the specified time.</p><p>The user timetrap function can return a time value after a delay.
The effective timetrap time is then the delay time <em>plus</em> the
returned time.</p><a name="logging"></a><h4>Logging - Categories and Verbosity Levels</h4><p><strong>Common Test</strong> provides the following three main functions for 
printing strings:</p><ul><li><strong>ct:log(Category, Importance, Format, FormatArgs, Opts)</strong></li><li><strong>ct:print(Category, Importance, Format, FormatArgs)</strong></li><li><strong>ct:pal(Category, Importance, Format, FormatArgs)</strong></li></ul><p>The <a href="./ct#log-1">ct#log-1</a> function 
prints a string to the test case log file. 
The <a href="./ct#print-1">ct#print-1</a> function 
prints the string to screen.
The <a href="./ct#pal-1">ct#pal-1</a> function 
prints the same string both to file and screen. The functions are described 
in module <a href="ct">ct</a>.
</p><p>The optional <strong>Category</strong> argument can be used to categorize the
log printout. Categories can be used for two things as follows:</p><ul><li>To compare the importance of the printout to a specific verbosity level.</li><li>To format the printout according to a user-specific HTML Style Sheet (CSS).</li></ul><p>Argument <strong>Importance</strong> specifies a level of importance
that, compared to a verbosity level (general and/or set per category),
determines if the printout is to be visible. <strong>Importance</strong>
is any integer in the range 0..99. Predefined constants
exist in the <strong>ct.hrl</strong> header file. The default importance level,
<strong>?STD_IMPORTANCE</strong> (used if argument <strong>Importance</strong> is not
provided), is 50. This is also the importance used for standard I/O,
for example, from printouts made with <strong>io:format/2</strong>, 
<strong>io:put_chars/1</strong>, and so on.</p><p><strong>Importance</strong> is compared to a verbosity level set by the
<strong>verbosity</strong> start flag/option. The level can be set per
category or generally, or both. If <strong>verbosity</strong> is not set by the user,
a level of 100 (<strong>?MAX_VERBOSITY</strong> = all printouts visible) is used as
default value. <strong>Common Test</strong> performs the following test:</p><pre>
Importance &gt;= (100-VerbosityLevel)</pre><p>The constant <strong>?STD_VERBOSITY</strong> has value 50 (see <strong>ct.hrl</strong>).
At this level, all standard I/O gets printed. If a lower verbosity level
is set, standard I/O printouts are ignored. Verbosity level 0 effectively
turns all logging off (except from printouts made by <strong>Common Test</strong>
itself).</p><p>The general verbosity level is not associated with any particular
category. This level sets the threshold for the standard I/O printouts,
uncategorized <strong>ct:log/print/pal</strong> printouts, and
printouts for categories with undefined verbosity level.</p><p><em>Examples:</em></p><p>Some printouts during test case execution:</p><pre>
 io:format("1. Standard IO, importance = ~w~n", [?STD_IMPORTANCE]),
 ct:log("2. Uncategorized, importance = ~w", [?STD_IMPORTANCE]),
 ct:log(info, "3. Categorized info, importance = ~w", [?STD_IMPORTANCE]),
 ct:log(info, ?LOW_IMPORTANCE, "4. Categorized info, importance = ~w", [?LOW_IMPORTANCE]),
 ct:log(error, ?HI_IMPORTANCE, "5. Categorized error, importance = ~w", [?HI_IMPORTANCE]),
 ct:log(error, ?MAX_IMPORTANCE, "6. Categorized error, importance = ~w", [?MAX_IMPORTANCE]),</pre><p>If starting the test with a general verbosity level of 50 (<strong>?STD_VERBOSITY</strong>):</p><pre>
 $ ct_run -verbosity 50</pre><p>the following is printed:</p><pre>
 1. Standard IO, importance = 50
 2. Uncategorized, importance = 50
 3. Categorized info, importance = 50
 5. Categorized error, importance = 75
 6. Categorized error, importance = 99</pre><p>If starting the test with:</p><pre>
 $ ct_run -verbosity 1 and info 75</pre><p>the following is printed:</p><pre>
 3. Categorized info, importance = 50
 4. Categorized info, importance = 25
 6. Categorized error, importance = 99</pre><p>Note that the category argument is not required in order to only specify the
importance of a printout. Example:</p><pre>
ct:pal(?LOW_IMPORTANCE, "Info report: ~p", [Info])</pre><p>Or perhaps in combination with constants:</p><pre>
-define(INFO, ?LOW_IMPORTANCE).
-define(ERROR, ?HI_IMPORTANCE).

ct:log(?INFO, "Info report: ~p", [Info])
ct:pal(?ERROR, "Error report: ~p", [Error])</pre><p>The functions <a href="./ct#set_verbosity-2">ct#set_verbosity-2</a>
and <a href="./ct#get_verbosity-1">ct#get_verbosity-1</a> may be used
to modify and read verbosity levels during test execution.</p><p>The arguments <strong>Format</strong> and <strong>FormatArgs</strong> in <strong>ct:log/print/pal</strong> are
always passed on to the STDLIB function <strong>io:format/3</strong> (For details,
see the <a href="./io">stdlib/io</a> manual page).</p><p><strong>ct:pal/4</strong> and <strong>ct:log/5</strong> add headers to strings being printed to the
log file. The strings are also wrapped in div tags with a CSS class
attribute, so that stylesheet formatting can be applied. To disable this feature for
a printout (i.e. to get a result similar to using <strong>io:format/2</strong>),
call <strong>ct:log/5</strong> with the <strong>no_css</strong> option.</p><p>How categories can be mapped to CSS tags is documented in section
<a href="./run_test_chapter#html_stylesheet">HTML Style Sheets</a>
in section Running Tests and Analyzing Results.</p><p>Common Test will escape special HTML characters (&lt;, &gt; and &amp;) in printouts
to the log file made with <strong>ct:pal/4</strong> and <strong>io:format/2</strong>. In order to print
strings with HTML tags to the log, use the <strong>ct:log/3,4,5</strong> function. The character
escaping feature is per default disabled for <strong>ct:log/3,4,5</strong> but can be enabled with
the <strong>esc_chars</strong> option in the <strong>Opts</strong> list, see <a href="./ct#log-5">ct#log-5</a>.</p><p>If the character escaping feature needs to be disabled (typically for backwards
compatibility reasons), use the <strong>ct_run</strong> start flag <strong>-no_esc_chars</strong>, or the
<strong>ct:run_test/1</strong> start option <strong>{esc_chars,Bool}</strong> (this start option is also
supported in test specifications).</p><p>For more information about log files, see section
<a href="./run_test_chapter#log_files">Log Files</a> 
in section Running Tests and Analyzing Results.</p><h4>Illegal Dependencies</h4><p>Even though it is highly efficient to write test suites with
the <strong>Common Test</strong> framework, mistakes can be made,
mainly because of illegal dependencies. Some of the 
more frequent mistakes from our own experience with running the 
Erlang/OTP test suites follows:</p><ul><li><p>Depending on current directory, and writing there:</p> <p>This is a common error in test suites. It is assumed that
the current directory is the same as the author used as
current directory when the test case was developed. Many test
cases even try to write scratch files to this directory. Instead
<strong>data_dir</strong> and <strong>priv_dir</strong> are to be used to locate 
data and for writing scratch files.
</p> </li><li><p>Depending on execution order:</p> <p>During development of test suites, make no assumptions on the 
execution order of the test cases or suites. For example, a test 
case must not assume that a server it depends on is already 
started by a previous test case. Reasons for this follows:
</p> <ul><li>The user/operator can specify the order at will, and maybe a different execution order is sometimes more relevant or  efficient.</li><li>If the user specifies a whole directory of test suites  for the test, the execution order of the suites depends on  how the files are listed by the operating system, which varies  between systems.</li><li>If a user wants to run only a subset of a test suite,  there is no way one test case could successfully depend on  another.</li></ul> </li><li><p>Depending on Unix:</p> <p>Running Unix commands through <strong>os:cmd</strong> are likely 
not to work on non-Unix platforms.
</p> </li><li><p>Nested test cases:</p> <p>Starting a test case from another not only tests the same
thing twice, but also makes it harder to follow what is being 
tested. Also, if the called test case fails for some
reason, so do the caller. This way, one error gives cause to
several error reports, which is to be avoided.
</p> <p>Functionality common for many test case functions can be 
implemented in common help functions. If these functions are 
useful for test cases across suites, put the help functions 
into common help modules.
</p> </li><li><p>Failure to crash or exit when things go wrong:</p> <p>Making requests without checking that the return value
indicates success can be OK if the test case fails
later, but it is never acceptable just to print an error
message (into the log file) and return successfully. Such test 
cases do harm, as they create a false sense of security when 
overviewing the test results.
</p> </li><li><p>Messing up for subsequent test cases:</p> <p>Test cases are to restore as much of the execution
environment as possible, so that subsequent test cases
do not crash because of their execution order. 
The function 
<a href="./common_test#Module:end_per_testcase-2">common_test#Module:end_per_testcase-2</a> 
is suitable for this.
</p> </li></ul><h4>General</h4><p>A test is performed by running one or more test suites. A test suite
consists of test cases, configuration functions, and information 
functions. Test cases can be grouped in so called test case groups. 
A test suite is an Erlang module and test cases are implemented as 
Erlang functions. Test suites are stored in test directories.</p><a name="skipping_test_cases"></a><h4>Skipping Test Cases</h4><p>Certain test cases can be skipped, for example, if you
know beforehand that a specific test case fails. The reason can be
functionality that is not yet implemented, a bug that is known but
not yet fixed, or some functionality that does not work or is not
applicable on a specific platform.</p><p>Test cases can be skipped in the following ways:</p><ul><li>Using <strong>skip_suites</strong> and <strong>skip_cases</strong> terms in  <a href="./run_test_chapter#test_specifications">test specifications</a>. </li><li>Returning <strong>{skip,Reason}</strong> from function <a href="./common_test#Module:init_per_testcase-2">common_test#Module:init_per_testcase-2</a> or  <a href="./common_test#Module:init_per_suite-1">common_test#Module:init_per_suite-1</a>.</li><li>Returning <strong>{skip,Reason}</strong> from the execution clause of the test case. The execution clause is called, so the author  must ensure that the test case does not run.</li></ul><p>When a test case is skipped, it is noted as <strong>SKIPPED</strong>
in the HTML log.</p><h4>Definition of Terms</h4><dl><dt><em>Auto-skipped test case</em></dt><dd> <p>When a configuration function fails (that is, terminates unexpectedly), 
the test cases depending on the configuration function are
skipped automatically by <strong>Common Test</strong>. The status of the test cases 
is then "auto-skipped".	Test cases are also "auto-skipped" by
<strong>Common Test</strong> if the required configuration data is unavailable at
runtime.</p> </dd><dt><em>Configuration function</em></dt><dd> <p>A function in a test suite that is meant to be used for
setting up, cleaning up, and/or verifying the state and 
environment on the System Under Test (SUT) and/or the <strong>Common Test</strong> 
host node, so that a test case (or a set of test cases) can 
execute correctly.</p>      </dd><dt><em>Configuration file</em></dt><dd> <p>A file containing data related to a test and/or an SUT,
for example, protocol server addresses, client
login details, and hardware interface addresses. That is, any data
that is to be handled as variable in the suite and not
be hard-coded.</p>    </dd><dt><em>Configuration variable</em></dt><dd> <p>A name (an Erlang atom) associated with a data value read from
a configuration file.</p> </dd><dt><strong>data_dir</strong></dt><dd> <p>Data directory for a test suite. This directory contains
any files used by the test suite, for example, extra Erlang
modules, binaries, or data files.</p> </dd><dt><em>Information function</em></dt><dd> <p>A function in a test suite that returns a list of properties
(read by the <strong>Common Test</strong> server) that describes the conditions 
for executing the test cases in the suite.</p> </dd><dt><em>Major log file</em></dt><dd> <p>An overview and summary log file for one or more test suites.</p> </dd><dt><em>Minor log file</em></dt><dd> <p>A log file for one particular test case. Also called the 
test case log file.</p> </dd><dt><strong>priv_dir</strong></dt><dd> <p>Private directory for a test suite. This directory is to
be used when the test suite needs to write to files.</p> </dd><dt><strong>ct_run</strong></dt><dd> <p>The name of an executable program that can be
used as an interface for specifying and running
tests with <strong>Common Test</strong>.</p> </dd><dt><em>Test case</em></dt><dd> <p>A single test included in a test suite. A test case is
implemented as a function in a test suite module.</p> </dd><dt><em>Test case group</em></dt><dd> <p>A set of test cases sharing configuration functions and 
execution properties. The execution properties specify if 
the test cases in the group are to be executed in random order,
in parallel, or in sequence, and if the execution of the group 
is be repeated. Test case groups can also be nested. That is, 
a group can, besides test cases, contain subgroups.</p> </dd><dt><em>Test suite</em></dt><dd> <p>An Erlang module containing a collection of test cases for
a specific functional area.</p> </dd><dt><em>Test directory</em></dt><dd> <p>A directory containing one or more test suite modules,
that is, a group of test suites.</p> </dd><dt><em>Argument</em> <strong>Config</strong></dt><dd> <p>A list of key-value tuples (that is, a property list) containing
runtime configuration data passed from the configuration
functions to the test cases.</p> </dd><dt><em>User-skipped test case</em></dt><dd> <p>The status of a test case explicitly skipped in any of 
the ways described in section
<a href="#skipping_test_cases">Skipping Test Cases</a>.
</p> </dd></dl><a name="top"></a><h4>Test Suite Example</h4><p>The following example test suite shows some tests of a database server:
</p><pre><code class="">
 -module(db_data_type_SUITE).

 -include_lib("common_test/include/ct.hrl").

 %% Test server callbacks
 -export([suite/0, all/0, 
	  init_per_suite/1, end_per_suite/1,
	  init_per_testcase/2, end_per_testcase/2]).

 %% Test cases
 -export([string/1, integer/1]).

 -define(CONNECT_STR, "DSN=sqlserver;UID=alladin;PWD=sesame").

 %%--------------------------------------------------------------------
 %% COMMON TEST CALLBACK FUNCTIONS
 %%--------------------------------------------------------------------

 %%--------------------------------------------------------------------
 %% Function: suite() -&gt; Info
 %%
 %% Info = [tuple()]
 %%   List of key/value pairs.
 %%
 %% Description: Returns list of tuples to set default properties
 %%              for the suite.
 %%--------------------------------------------------------------------
 suite() -&gt; 
     [{timetrap,{minutes,1}}].  

 %%--------------------------------------------------------------------
 %% Function: init_per_suite(Config0) -&gt; Config1
 %%
 %% Config0 = Config1 = [tuple()]
 %%   A list of key/value pairs, holding the test case configuration.
 %%
 %% Description: Initialization before the suite.
 %%--------------------------------------------------------------------
 init_per_suite(Config) -&gt; 
     {ok, Ref} = db:connect(?CONNECT_STR, []),
     TableName = db_lib:unique_table_name(),	
     [{con_ref, Ref },{table_name, TableName}| Config]. 

 %%--------------------------------------------------------------------
 %% Function: end_per_suite(Config) -&gt; term()
 %%
 %% Config = [tuple()]
 %%   A list of key/value pairs, holding the test case configuration.
 %%
 %% Description: Cleanup after the suite.
 %%--------------------------------------------------------------------
 end_per_suite(Config) -&gt;    
     Ref = ?config(con_ref, Config),
     db:disconnect(Ref),
     ok.
 
 %%--------------------------------------------------------------------
 %% Function: init_per_testcase(TestCase, Config0) -&gt; Config1
 %%
 %% TestCase = atom()
 %%   Name of the test case that is about to run.
 %% Config0 = Config1 = [tuple()]
 %%   A list of key/value pairs, holding the test case configuration.
 %%
 %% Description: Initialization before each test case.
 %%--------------------------------------------------------------------
 init_per_testcase(Case, Config) -&gt;
     Ref = ?config(con_ref, Config),   
     TableName = ?config(table_name, Config),
     ok = db:create_table(Ref, TableName, table_type(Case)),
     Config.

 %%--------------------------------------------------------------------
 %% Function: end_per_testcase(TestCase, Config) -&gt; term()
 %%
 %% TestCase = atom()
 %%   Name of the test case that is finished.
 %% Config = [tuple()]
 %%   A list of key/value pairs, holding the test case configuration.
 %%
 %% Description: Cleanup after each test case.
 %%--------------------------------------------------------------------
 end_per_testcase(_Case, Config) -&gt; 
     Ref = ?config(con_ref, Config),   
     TableName = ?config(table_name, Config),
     ok = db:delete_table(Ref, TableName),   
     ok. 

 %%--------------------------------------------------------------------
 %% Function: all() -&gt; GroupsAndTestCases
 %%
 %% GroupsAndTestCases = [{group,GroupName} | TestCase]
 %% GroupName = atom()
 %%   Name of a test case group.
 %% TestCase = atom()
 %%   Name of a test case.
 %%
 %% Description: Returns the list of groups and test cases that
 %%              are to be executed.
 %%--------------------------------------------------------------------
 all() -&gt;
     [string, integer]. 


 %%--------------------------------------------------------------------
 %% TEST CASES
 %%--------------------------------------------------------------------

 string(Config) -&gt; 
     insert_and_lookup(dummy_key, "Dummy string", Config).

 integer(Config) -&gt; 
     insert_and_lookup(dummy_key, 42, Config).


 insert_and_lookup(Key, Value, Config) -&gt;
     Ref = ?config(con_ref, Config),   
     TableName = ?config(table_name, Config),
     ok = db:insert(Ref, TableName, Key, Value),
     [Value] = db:lookup(Ref, TableName, Key),
     ok = db:delete(Ref, TableName, Key),
     [] = db:lookup(Ref, TableName, Key),
     ok.</code></pre><h4>Test Suite Templates</h4><p>The Erlang mode for the Emacs editor includes two <strong>Common Test</strong> test 
suite templates, one with extensive information in the function headers, and
one with minimal information. A test suite template provides a quick start
for implementing a suite from scratch and gives a good overview
of the available callback functions. The two templates follows:
</p><p><em>Large Common Test Suite</em></p><pre><code class="">
 %%%-------------------------------------------------------------------
 %%% File    : example_SUITE.erl
 %%% Author  : 
 %%% Description : 
 %%%
 %%% Created : 
 %%%-------------------------------------------------------------------
 -module(example_SUITE).

 %% Note: This directive should only be used in test suites.
 -compile(export_all).

 -include_lib("common_test/include/ct.hrl").

 %%--------------------------------------------------------------------
 %% COMMON TEST CALLBACK FUNCTIONS
 %%--------------------------------------------------------------------

 %%--------------------------------------------------------------------
 %% Function: suite() -&gt; Info
 %%
 %% Info = [tuple()]
 %%   List of key/value pairs.
 %%
 %% Description: Returns list of tuples to set default properties
 %%              for the suite.
 %%
 %% Note: The suite/0 function is only meant to be used to return
 %% default data values, not perform any other operations.
 %%--------------------------------------------------------------------
 suite() -&gt;
     [{timetrap,{minutes,10}}].

 %%--------------------------------------------------------------------
 %% Function: init_per_suite(Config0) -&gt;
 %%               Config1 | {skip,Reason} | {skip_and_save,Reason,Config1}
 %%
 %% Config0 = Config1 = [tuple()]
 %%   A list of key/value pairs, holding the test case configuration.
 %% Reason = term()
 %%   The reason for skipping the suite.
 %%
 %% Description: Initialization before the suite.
 %%
 %% Note: This function is free to add any key/value pairs to the Config
 %% variable, but should NOT alter/remove any existing entries.
 %%--------------------------------------------------------------------
 init_per_suite(Config) -&gt;
     Config.

 %%--------------------------------------------------------------------
 %% Function: end_per_suite(Config0) -&gt; term() | {save_config,Config1}
 %%
 %% Config0 = Config1 = [tuple()]
 %%   A list of key/value pairs, holding the test case configuration.
 %%
 %% Description: Cleanup after the suite.
 %%--------------------------------------------------------------------
 end_per_suite(_Config) -&gt;
     ok.

 %%--------------------------------------------------------------------
 %% Function: init_per_group(GroupName, Config0) -&gt;
 %%               Config1 | {skip,Reason} | {skip_and_save,Reason,Config1}
 %%
 %% GroupName = atom()
 %%   Name of the test case group that is about to run.
 %% Config0 = Config1 = [tuple()]
 %%   A list of key/value pairs, holding configuration data for the group.
 %% Reason = term()
 %%   The reason for skipping all test cases and subgroups in the group.
 %%
 %% Description: Initialization before each test case group.
 %%--------------------------------------------------------------------
 init_per_group(_GroupName, Config) -&gt;
     Config.

 %%--------------------------------------------------------------------
 %% Function: end_per_group(GroupName, Config0) -&gt;
 %%               term() | {save_config,Config1}
 %%
 %% GroupName = atom()
 %%   Name of the test case group that is finished.
 %% Config0 = Config1 = [tuple()]
 %%   A list of key/value pairs, holding configuration data for the group.
 %%
 %% Description: Cleanup after each test case group.
 %%--------------------------------------------------------------------
 end_per_group(_GroupName, _Config) -&gt;
     ok.

 %%--------------------------------------------------------------------
 %% Function: init_per_testcase(TestCase, Config0) -&gt;
 %%               Config1 | {skip,Reason} | {skip_and_save,Reason,Config1}
 %%
 %% TestCase = atom()
 %%   Name of the test case that is about to run.
 %% Config0 = Config1 = [tuple()]
 %%   A list of key/value pairs, holding the test case configuration.
 %% Reason = term()
 %%   The reason for skipping the test case.
 %%
 %% Description: Initialization before each test case.
 %%
 %% Note: This function is free to add any key/value pairs to the Config
 %% variable, but should NOT alter/remove any existing entries.
 %%--------------------------------------------------------------------
 init_per_testcase(_TestCase, Config) -&gt;
     Config.

 %%--------------------------------------------------------------------
 %% Function: end_per_testcase(TestCase, Config0) -&gt;
 %%               term() | {save_config,Config1} | {fail,Reason}
 %%
 %% TestCase = atom()
 %%   Name of the test case that is finished.
 %% Config0 = Config1 = [tuple()]
 %%   A list of key/value pairs, holding the test case configuration.
 %% Reason = term()
 %%   The reason for failing the test case.
 %%
 %% Description: Cleanup after each test case.
 %%--------------------------------------------------------------------
 end_per_testcase(_TestCase, _Config) -&gt;
     ok.

 %%--------------------------------------------------------------------
 %% Function: groups() -&gt; [Group]
 %%
 %% Group = {GroupName,Properties,GroupsAndTestCases}
 %% GroupName = atom()
 %%   The name of the group.
 %% Properties = [parallel | sequence | Shuffle | {RepeatType,N}]
 %%   Group properties that may be combined.
 %% GroupsAndTestCases = [Group | {group,GroupName} | TestCase]
 %% TestCase = atom()
 %%   The name of a test case.
 %% Shuffle = shuffle | {shuffle,Seed}
 %%   To get cases executed in random order.
 %% Seed = {integer(),integer(),integer()}
 %% RepeatType = repeat | repeat_until_all_ok | repeat_until_all_fail |
 %%              repeat_until_any_ok | repeat_until_any_fail
 %%   To get execution of cases repeated.
 %% N = integer() | forever
 %%
 %% Description: Returns a list of test case group definitions.
 %%--------------------------------------------------------------------
 groups() -&gt;
     [].

 %%--------------------------------------------------------------------
 %% Function: all() -&gt; GroupsAndTestCases | {skip,Reason}
 %%
 %% GroupsAndTestCases = [{group,GroupName} | TestCase]
 %% GroupName = atom()
 %%   Name of a test case group.
 %% TestCase = atom()
 %%   Name of a test case.
 %% Reason = term()
 %%   The reason for skipping all groups and test cases.
 %%
 %% Description: Returns the list of groups and test cases that
 %%              are to be executed.
 %%--------------------------------------------------------------------
 all() -&gt; 
     [my_test_case].


 %%--------------------------------------------------------------------
 %% TEST CASES
 %%--------------------------------------------------------------------

 %%--------------------------------------------------------------------
 %% Function: TestCase() -&gt; Info
 %%
 %% Info = [tuple()]
 %%   List of key/value pairs.
 %%
 %% Description: Test case info function - returns list of tuples to set
 %%              properties for the test case.
 %%
 %% Note: This function is only meant to be used to return a list of
 %% values, not perform any other operations.
 %%--------------------------------------------------------------------
 my_test_case() -&gt; 
     [].

 %%--------------------------------------------------------------------
 %% Function: TestCase(Config0) -&gt;
 %%               ok | exit() | {skip,Reason} | {comment,Comment} |
 %%               {save_config,Config1} | {skip_and_save,Reason,Config1}
 %%
 %% Config0 = Config1 = [tuple()]
 %%   A list of key/value pairs, holding the test case configuration.
 %% Reason = term()
 %%   The reason for skipping the test case.
 %% Comment = term()
 %%   A comment about the test case that will be printed in the html log.
 %%
 %% Description: Test case function. (The name of it must be specified in
 %%              the all/0 list or in a test case group for the test case
 %%              to be executed).
 %%--------------------------------------------------------------------
 my_test_case(_Config) -&gt; 
     ok.</code></pre><br/><p><em>Small Common Test Suite</em></p><pre><code class="">
 %%%-------------------------------------------------------------------
 %%% File    : example_SUITE.erl
 %%% Author  : 
 %%% Description : 
 %%%
 %%% Created : 
 %%%-------------------------------------------------------------------
 -module(example_SUITE).

 -compile(export_all).

 -include_lib("common_test/include/ct.hrl").

 %%--------------------------------------------------------------------
 %% Function: suite() -&gt; Info
 %% Info = [tuple()]
 %%--------------------------------------------------------------------
 suite() -&gt;
     [{timetrap,{seconds,30}}].

 %%--------------------------------------------------------------------
 %% Function: init_per_suite(Config0) -&gt;
 %%               Config1 | {skip,Reason} | {skip_and_save,Reason,Config1}
 %% Config0 = Config1 = [tuple()]
 %% Reason = term()
 %%--------------------------------------------------------------------
 init_per_suite(Config) -&gt;
     Config.

 %%--------------------------------------------------------------------
 %% Function: end_per_suite(Config0) -&gt; term() | {save_config,Config1}
 %% Config0 = Config1 = [tuple()]
 %%--------------------------------------------------------------------
 end_per_suite(_Config) -&gt;
     ok.

 %%--------------------------------------------------------------------
 %% Function: init_per_group(GroupName, Config0) -&gt;
 %%               Config1 | {skip,Reason} | {skip_and_save,Reason,Config1}
 %% GroupName = atom()
 %% Config0 = Config1 = [tuple()]
 %% Reason = term()
 %%--------------------------------------------------------------------
 init_per_group(_GroupName, Config) -&gt;
     Config.

 %%--------------------------------------------------------------------
 %% Function: end_per_group(GroupName, Config0) -&gt;
 %%               term() | {save_config,Config1}
 %% GroupName = atom()
 %% Config0 = Config1 = [tuple()]
 %%--------------------------------------------------------------------
 end_per_group(_GroupName, _Config) -&gt;
     ok.

 %%--------------------------------------------------------------------
 %% Function: init_per_testcase(TestCase, Config0) -&gt;
 %%               Config1 | {skip,Reason} | {skip_and_save,Reason,Config1}
 %% TestCase = atom()
 %% Config0 = Config1 = [tuple()]
 %% Reason = term()
 %%--------------------------------------------------------------------
 init_per_testcase(_TestCase, Config) -&gt;
     Config.

 %%--------------------------------------------------------------------
 %% Function: end_per_testcase(TestCase, Config0) -&gt;
 %%               term() | {save_config,Config1} | {fail,Reason}
 %% TestCase = atom()
 %% Config0 = Config1 = [tuple()]
 %% Reason = term()
 %%--------------------------------------------------------------------
 end_per_testcase(_TestCase, _Config) -&gt;
     ok.

 %%--------------------------------------------------------------------
 %% Function: groups() -&gt; [Group]
 %% Group = {GroupName,Properties,GroupsAndTestCases}
 %% GroupName = atom()
 %% Properties = [parallel | sequence | Shuffle | {RepeatType,N}]
 %% GroupsAndTestCases = [Group | {group,GroupName} | TestCase]
 %% TestCase = atom()
 %% Shuffle = shuffle | {shuffle,{integer(),integer(),integer()}}
 %% RepeatType = repeat | repeat_until_all_ok | repeat_until_all_fail |
 %%              repeat_until_any_ok | repeat_until_any_fail
 %% N = integer() | forever
 %%--------------------------------------------------------------------
 groups() -&gt;
     [].

 %%--------------------------------------------------------------------
 %% Function: all() -&gt; GroupsAndTestCases | {skip,Reason}
 %% GroupsAndTestCases = [{group,GroupName} | TestCase]
 %% GroupName = atom()
 %% TestCase = atom()
 %% Reason = term()
 %%--------------------------------------------------------------------
 all() -&gt; 
     [my_test_case].

 %%--------------------------------------------------------------------
 %% Function: TestCase() -&gt; Info
 %% Info = [tuple()]
 %%--------------------------------------------------------------------
 my_test_case() -&gt; 
     [].

 %%--------------------------------------------------------------------
 %% Function: TestCase(Config0) -&gt;
 %%               ok | exit() | {skip,Reason} | {comment,Comment} |
 %%               {save_config,Config1} | {skip_and_save,Reason,Config1}
 %% Config0 = Config1 = [tuple()]
 %% Reason = term()
 %% Comment = term()
 %%--------------------------------------------------------------------
 my_test_case(_Config) -&gt; 
     ok.</code></pre><h4>Using the Common Test Framework</h4><p>The <strong>Common Test</strong> framework provides a high-level
operator interface for testing, providing the following features:</p><ul><li>Automatic compilation of test suites (and help modules)</li><li>Creation of extra HTML pages for improved overview.</li><li>Single-command interface for running all available tests</li><li>Handling of configuration files specifying data related to the System Under Test (SUT) (and any other variable data)</li><li>Mode for running multiple independent test sessions in parallel with central control and configuration</li></ul><h4>Automatic Compilation of Test Suites and Help Modules</h4><p>When <strong>Common Test</strong> starts, it automatically attempts to compile any
suites included in the specified tests. If particular
suites are specified, only those suites are compiled. If a
particular test object directory is specified (meaning all suites
in this directory are to be part of the test), <strong>Common Test</strong> runs
function <strong>make:all/1</strong> in the directory to compile the suites.</p><p>If compilation fails for one or more suites, the compilation errors
are printed to tty and the operator is asked if the test run is to proceed
without the missing suites, or be aborted. If the operator chooses to proceed, 
the tests having missing suites are noted in the HTML log. If <strong>Common Test</strong> is
unable to prompt the user after compilation failure (if <strong>Common Test</strong> does not
control <strong>stdin</strong>), the test run proceeds automatically without the missing
suites. This behavior can however be modified with the
<strong>ct_run</strong> flag <strong>-abort_if_missing_suites</strong>, 
or the <a href="./ct#run_test-1">ct#run_test-1</a> option
<strong>{abort_if_missing_suites,TrueOrFalse}</strong>. If 
<strong>abort_if_missing_suites</strong> is set to <strong>true</strong>, the test run
stops immediately if some suites fail to compile.</p><p>Any help module (that is, regular Erlang module with name not ending with
"_SUITE") that resides in the same test object directory as a suite, 
which is part of the test, is also automatically compiled. A help
module is not mistaken for a test suite (unless it has a "_SUITE" name).
All help modules in a particular test object directory
are compiled, no matter if all or only particular suites in the directory 
are part of the test.</p><p>If test suites or help modules include header files stored in other
locations than the test directory, these include directories can be specified
by using flag <strong>-include</strong> with 
<a href="ct_run">ct_run</a>, 
or option <strong>include</strong> with <strong>ct:run_test/1</strong>.
Also, an include path can be specified with an OS
environment variable, <strong>CT_INCLUDE_PATH</strong>.</p><p><em>Example (bash):</em></p><p><strong>$ export CT_INCLUDE_PATH=~testuser/common_suite_files/include:~testuser/common_lib_files/include</strong></p><p><strong>Common Test</strong> passes all include directories (specified either with flag/option
<strong>include</strong>, or variable <strong>CT_INCLUDE_PATH</strong>
, or both, to the compiler.</p><p>Include directories can also be specified in test specifications,
see <a href="#test_specifications">Test Specifications</a>.</p><p>If the user wants to run all test suites for a test object (or an OTP application)
by specifying only the top directory (for example, with start flag/option <strong>dir</strong>),
<strong>Common Test</strong> primarily looks for test suite modules in a subdirectory named 
<strong>test</strong>. If this subdirectory does not exist, the specified top directory
is assumed to be the test directory, and test suites are read from
there instead.</p><p>To disable the automatic compilation feature, use flag
<strong>-no_auto_compile</strong> with <strong>ct_run</strong>, or
option <strong>{auto_compile,false}</strong> with 
<strong>ct:run_test/1</strong>. With automatic compilation
disabled, the user is responsible for compiling the test suite modules 
(and any help modules) before the test run. If the modules cannot be loaded
from the local file system during startup of <strong>Common Test</strong>, the user must
preload the modules before starting the test. <strong>Common Test</strong> only verifies
that the specified test suites exist (that is, that they are, or can be, loaded).
This is useful, for example, if the test suites are transferred and loaded as 
binaries through RPC from a remote node.</p><a name="ct_run"></a><h4>Running Tests from the OS Command Line</h4><p>The <a href="ct_run">ct_run</a> program can be used 
for running tests from the OS command line, for example, as follows:
</p><ul><li><strong>ct_run -config &lt;configfilenames&gt; -dir &lt;dirs&gt;</strong></li><li><strong>ct_run -config &lt;configfilenames&gt; -suite &lt;suiteswithfullpath&gt;</strong> </li><li><strong>ct_run -userconfig &lt;callbackmodulename&gt; &lt;configfilenames&gt; -suite &lt;suiteswithfullpath&gt;</strong> </li><li><strong>ct_run -config &lt;configfilenames&gt; -suite &lt;suitewithfullpath&gt; -group &lt;groups&gt; -case &lt;casenames&gt;</strong></li></ul><p><em>Examples:</em></p><pre>
 $ ct_run -config $CFGS/sys1.cfg $CFGS/sys2.cfg -dir $SYS1_TEST $SYS2_TEST
 $ ct_run -userconfig ct_config_xml $CFGS/sys1.xml $CFGS/sys2.xml -dir $SYS1_TEST $SYS2_TEST
 $ ct_run -suite $SYS1_TEST/setup_SUITE $SYS2_TEST/config_SUITE
 $ ct_run -suite $SYS1_TEST/setup_SUITE -case start stop
 $ ct_run -suite $SYS1_TEST/setup_SUITE -group installation -case start stop</pre><p>The flags <strong>dir</strong>, <strong>suite</strong>, and <strong>group/case</strong> can be combined.
For example, to run <strong>x_SUITE</strong> and <strong>y_SUITE</strong> 
in directory <strong>testdir</strong>, as follows:</p><pre>
 $ ct_run -dir ./testdir -suite x_SUITE y_SUITE</pre><p>This has the same effect as the following:</p><pre>
 $ ct_run -suite ./testdir/x_SUITE ./testdir/y_SUITE</pre><p>For details, see 
<a href="./run_test_chapter#group_execution">Test Case Group Execution</a>.</p><p>The following flags can also be used with 
<a href="ct_run">ct_run</a>:</p><dl><dt><strong>-help</strong></dt><dd><p>Lists all available start flags.</p></dd><dt><strong>-logdir &lt;dir&gt;</strong></dt><dd><p>Specifies where the HTML log files are to be written.</p></dd><dt><strong>-label &lt;name_of_test_run&gt;</strong></dt><dd><p>Associates the test run with a name that gets printed
in the overview HTML log files.</p></dd><dt><strong>-refresh_logs</strong></dt><dd><p>Refreshes the top-level HTML index files.</p></dd><dt><strong>-vts</strong></dt><dd><p>Starts web-based GUI (described later).</p></dd><dt><strong>-shell</strong></dt><dd><p>Starts interactive shell mode (described later).</p></dd><dt><strong>-step [step_opts]</strong></dt><dd><p>Steps through test cases using the Erlang Debugger (described later).</p></dd><dt><strong>-spec &lt;testspecs&gt;</strong></dt><dd><p>Uses test specification as input (described later).</p></dd><dt><strong>-allow_user_terms</strong></dt><dd><p>Allows user-specific terms in a test specification (described later).</p></dd><dt><strong>-silent_connections [conn_types]</strong></dt><dd><p>, tells <strong>Common Test</strong> to suppress printouts for
specified connections (described later).</p></dd><dt><strong>-stylesheet &lt;css_file&gt;</strong></dt><dd><p>Points out a user HTML style sheet (described later).</p></dd><dt><strong>-cover &lt;cover_cfg_file&gt;</strong></dt><dd><p>To perform code coverage test (see 
<a href="./cover_chapter#cover">Code Coverage Analysis</a>).</p></dd><dt><strong>-cover_stop &lt;bool&gt;</strong></dt><dd><p>To specify if the <strong>cover</strong> tool is to be stopped 
after the test is completed (see
<a href="./cover_chapter#cover_stop">Code Coverage Analysis</a>).</p></dd><dt><strong>-event_handler &lt;event_handlers&gt;</strong></dt><dd><p>To install 
<a href="./event_handler_chapter#event_handling">event handlers</a>.</p></dd><dt><strong>-event_handler_init &lt;event_handlers&gt;</strong></dt><dd><p>To install
<a href="./event_handler_chapter#event_handling">event handlers</a> 
including start arguments.</p></dd><dt><strong>-ct_hooks &lt;ct_hooks&gt;</strong></dt><dd><p>To install
<a href="./ct_hooks_chapter#installing">Common Test Hooks</a> 
including start arguments.</p></dd><dt><strong>-enable_builtin_hooks &lt;bool&gt;</strong></dt><dd><p>To enable or disable
<a href="./ct_hooks_chapter#builtin_cths">Built-in Common Test Hooks</a>. 
Default is <strong>true</strong>.</p></dd><dt><strong>-include</strong></dt><dd><p>Specifies include directories (described earlier).</p></dd><dt><strong>-no_auto_compile</strong></dt><dd><p>Disables the automatic test suite compilation feature (described earlier).</p></dd><dt><strong>-abort_if_missing_suites</strong></dt><dd><p>Aborts the test run if one or more suites fail to compile (described earlier).</p></dd><dt><strong>-multiply_timetraps &lt;n&gt;</strong></dt><dd><p>Extends <a href="./write_test_chapter#timetraps">timetrap time-out</a> values.</p></dd><dt><strong>-scale_timetraps &lt;bool&gt;</strong></dt><dd><p>Enables automatic <a href="./write_test_chapter#timetraps">timetrap time-out</a> scaling.</p></dd><dt><strong>-repeat &lt;n&gt;</strong></dt><dd><p>Tells <strong>Common Test</strong> to repeat the tests <strong>n</strong> times (described later).</p></dd><dt><strong>-duration &lt;time&gt;</strong></dt><dd><p>Tells <strong>Common Test</strong> to repeat the tests for duration of time (described later).</p></dd><dt><strong>-until &lt;stop_time&gt;</strong></dt><dd><p>Tells <strong>Common Test</strong> to repeat the tests until <strong>stop_time</strong> (described later).</p></dd><dt><strong>-force_stop [skip_rest]</strong></dt><dd><p>On time-out, the test run is aborted when the current test job is finished. If <strong>skip_rest</strong> 
is provided, the remaining test cases in the current test job are skipped (described later).</p></dd><dt><strong>-decrypt_key &lt;key&gt;</strong></dt><dd><p>Provides a decryption key for 
<a href="./config_file_chapter#encrypted_config_files">encrypted configuration files</a>.</p></dd><dt><strong>-decrypt_file &lt;key_file&gt;</strong></dt><dd><p>Points out a file containing a decryption key for 
<a href="./config_file_chapter#encrypted_config_files">encrypted configuration files</a>.</p></dd><dt><strong>-basic_html</strong></dt><dd><p>Switches off HTML enhancements that can be incompatible with older browsers.</p></dd><dt><strong>-logopts &lt;opts&gt;</strong></dt><dd><p>Enables modification of the logging behavior, see
<a href="./run_test_chapter#logopts">Log options</a>.</p></dd><dt><strong>-verbosity &lt;levels&gt;</strong></dt><dd><p>Sets <a href="./write_test_chapter#logging">verbosity levels for printouts</a>.</p></dd><dt><strong>-no_esc_chars</strong></dt><dd><p>Disables automatic escaping of special HTML characters.
See the <a href="./write_test_chapter#logging">Logging chapter</a>.</p></dd></dl><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>Directories passed to <strong>Common Test</strong> can have either relative or absolute paths.</p></div><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>Any start flags to the Erlang runtime system (application ERTS) can also be passed as
parameters to <strong>ct_run</strong>. It is, for example, useful to be able to
pass directories to be added to the Erlang code server search path
with flag <strong>-pa</strong> or <strong>-pz</strong>. If you have common help- or library 
modules for test suites (separately compiled), stored in other directories 
than the test suite directories, these <strong>help/lib</strong> directories are preferably
added to the code path this way.</p><p><em>Example:</em></p><p><strong>$ ct_run -dir ./chat_server -logdir ./chat_server/testlogs -pa $PWD/chat_server/ebin</strong></p><p>The absolute path of directory <strong>chat_server/ebin</strong>
is here passed to the code server. This is essential because relative
paths are stored by the code server as relative, and <strong>Common Test</strong> changes 
the current working directory of ERTS during the test run.</p></div><p>The <strong>ct_run</strong> program sets the exit status before shutting down. The following values
are defined:</p><ul><li><strong>0</strong> indicates a successful testrun, that is, without failed or auto-skipped test cases.</li><li><strong>1</strong> indicates that one or more test cases have failed, or have been auto-skipped.</li><li><strong>2</strong> indicates that the test execution has failed because of, for example, compilation errors, or an illegal return value from an information function.</li></ul><p>If auto-skipped test cases do not affect the exit status. The default
behavior can be changed using start flag:</p><pre>
 -exit_status ignore_config</pre><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>Executing <strong>ct_run</strong> without start flags is equal to the command:
<strong>ct_run -dir ./</strong></p></div><p>For more information about the <strong>ct_run</strong> program, see module
<a href="ct_run">ct_run</a> and section
<a href="./install_chapter#general">Installation</a>.
</p><a name="erlang_shell_or_program"></a><h4>Running Tests from the Erlang Shell or from an Erlang Program</h4><p><strong>Common Test</strong> provides an Erlang API for running tests. The main 
(and most flexible) function for specifying and executing tests is
<a href="./ct#run_test-1">ct#run_test-1</a>.
It takes the same start parameters as
<a href="./run_test_chapter#ct_run">run_test_chapter#ct_run</a>,
but the flags are instead specified as options in a list of key-value tuples. 
For example, a test specified with <strong>ct_run</strong> as follows:</p><p><strong>$ ct_run -suite ./my_SUITE -logdir ./results</strong></p><p>is with <a href="./ct#run_test-1">ct#run_test-1</a> specified as:</p><p><strong>1&gt; ct:run_test([{suite,"./my_SUITE"},{logdir,"./results"}]).</strong></p><p>The function returns the test result, represented by the tuple
<strong>{Ok,Failed,{UserSkipped,AutoSkipped}}</strong>, where each element is an
integer. If test execution fails, the function returns the tuple
<strong>{error,Reason}</strong>, where the term <strong>Reason</strong> explains the
failure.</p><p>The default start option <strong>{dir,Cwd}</strong> (to run all suites in the current
working directory) is used if the function is called with an empty
list of options.</p><h4>Releasing the Erlang Shell</h4><p>During execution of tests started with
<a href="./ct#run_test-1">ct#run_test-1</a>,
the Erlang shell process, controlling <strong>stdin</strong>, remains the top-level
process of the <strong>Common Test</strong> system of processes. Consequently,
the Erlang shell is not available for interaction during
the test run. If this is not desirable, for example, because the shell 
is needed for debugging purposes or for interaction with the SUT during test
execution, set start option <strong>release_shell</strong> to
<strong>true</strong> (in the call to <strong>ct:run_test/1</strong> or by
using the corresponding test specification term, described later). This
makes <strong>Common Test</strong> release the shell immediately after the test suite
compilation stage. To accomplish this, a test runner process
is spawned to take control of the test execution. The effect is that
<strong>ct:run_test/1</strong> returns the pid of this process rather than the
test result, which instead is printed to tty at the end of the test run.</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>To use the functions
<a href="./ct#break-1">ct#break-1</a> and
<a href="./ct#continue-0">ct#continue-0</a>,
<strong>release_shell</strong> <em>must</em> be set to <strong>true</strong>.</p></div><p>For details, see
<a href="./ct#run_test-1">ct#run_test-1</a> manual page.</p><a name="group_execution"></a><h4>Test Case Group Execution</h4><p>With the <strong>ct_run</strong> flag, or <strong>ct:run_test/1</strong> option <strong>group</strong>,
one or more test case groups can be specified, optionally in combination
with specific test cases. The syntax for specifying groups on the command line
is as follows:</p><pre>
 $ ct_run -group &lt;group_names_or_paths&gt; [-case &lt;cases&gt;]</pre><p>The syntax in the Erlang shell is as follows:</p><pre>
 1&gt; ct:run_test([{group,GroupsNamesOrPaths}, {case,Cases}]).</pre><p>Parameter <strong>group_names_or_paths</strong> specifies one
or more group names and/or one or more group paths. At startup,
<strong>Common Test</strong> searches for matching groups in the group definitions
tree (that is, the list returned from <strong>Suite:groups/0</strong>; for details, see section
<a href="./write_test_chapter#test_case_groups">Test Case Groups</a>.
</p><p>Given a group name, say <strong>g</strong>, <strong>Common Test</strong> searches for all paths
leading to <strong>g</strong>. By path is meant a sequence of nested groups,
which must be followed to get from the top-level
group to <strong>g</strong>. To execute the test cases in group <strong>g</strong>, 
<strong>Common Test</strong> must call the <strong>init_per_group/2</strong> function for 
each group in the path to <strong>g</strong>, and all corresponding <strong>end_per_group/2</strong>
functions afterwards. This is because the configuration
of a test case in <strong>g</strong> (and its <strong>Config</strong> input data) depends on
<strong>init_per_testcase(TestCase, Config)</strong> and its return value, which
in turn depends on <strong>init_per_group(g, Config)</strong> and its return value,
which in turn depends on <strong>init_per_group/2</strong> of the group above
<strong>g</strong>, and so on, all the way up to the top-level group.</p><p>This means that if there is more than one way to locate a group 
(and its test cases) in a path, the result of the group search operation 
is a number of tests, all of which are to be performed.
<strong>Common Test</strong> interprets a group specification that consists of a
single name as follows:</p><p>"Search and find all paths in the group definitions tree that lead
to the specified group and, for each path, create a test that does the following, 
in order:</p><ul><li>Executes all configuration functions in the path to the specified group.</li><li>Executes all, or all matching, test cases in this group.</li><li>Executes all, or all matching, test cases in all subgroups of the group."</li></ul><p>The user can specify a specific group path with
parameter <strong>group_names_or_paths</strong>. With this type of specification
execution of unwanted groups (in otherwise matching paths),
and/or the execution of subgroups can be avoided. The command line syntax of the 
group path is a list of group names in the path, for example:
</p><p><strong>$ ct_run -suite "./x_SUITE" -group [g1,g3,g4] -case tc1 tc5</strong></p><p>The syntax in the Erlang shell is as follows (requires a list within the groups list):</p><p><strong>1&gt; ct:run_test([{suite,"./x_SUITE"}, {group,[[g1,g3,g4]]}, {testcase,[tc1,tc5]}]).</strong></p><p>The last group in the specified path is the terminating group in
the test, that is, no subgroups following this group are executed. In the
previous example, <strong>g4</strong> is the terminating group. Hence, <strong>Common Test</strong>
executes a test that calls all <strong>init</strong> configuration functions in the path to
<strong>g4</strong>, that is, <strong>g1..g3..g4</strong>. It then calls test cases <strong>tc1</strong>
and <strong>tc5</strong> in <strong>g4</strong>, and finally all <strong>end</strong> configuration functions 
in order <strong>g4..g3..g1</strong>.</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>The group path specification does not necessarily
have to include <em>all</em> groups in the path to the terminating group.
<strong>Common Test</strong> searches for all matching paths if an incomplete 
group path is specified.</p></div><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>Group names and group paths can be combined with parameter
<strong>group_names_or_paths</strong>. Each element is treated as an individual specification 
in combination with parameter <strong>cases</strong>.
The following examples illustrates this.</p></div><p><em>Examples:</em></p><pre>
 -module(x_SUITE).
 ...
 %% The group definitions:      
 groups() -&gt;
   [{top1,[],[tc11,tc12,
	      {sub11,[],[tc12,tc13]},
	      {sub12,[],[tc14,tc15,
			 {sub121,[],[tc12,tc16]}]}]},

    {top2,[],[{group,sub21},{group,sub22}]},
    {sub21,[],[tc21,{group,sub2X2}]},
    {sub22,[],[{group,sub221},tc21,tc22,{group,sub2X2}]},
    {sub221,[],[tc21,tc23]},
    {sub2X2,[],[tc21,tc24]}].</pre><p>The following executes two tests, one for all cases and all subgroups 
under <strong>top1</strong>, and one for all under <strong>top2</strong>:</p><pre>
 $ ct_run -suite "x_SUITE" -group all
 1&gt; ct:run_test([{suite,"x_SUITE"}, {group,all}]).</pre><p>Using <strong>-group top1 top2</strong>, or <strong>{group,[top1,top2]}</strong> gives the same result.</p><p>The following executes one test for all cases and subgroups under <strong>top1</strong>:</p><pre>
 $ ct_run -suite "x_SUITE" -group top1
 1&gt; ct:run_test([{suite,"x_SUITE"}, {group,[top1]}]).</pre><p>The following runs a test executing <strong>tc12</strong> in <strong>top1</strong> and any subgroup
under <strong>top1</strong> where it can be found (<strong>sub11</strong> and <strong>sub121</strong>):</p><pre>
 $ ct_run -suite "x_SUITE" -group top1 -case tc12
 1&gt; ct:run_test([{suite,"x_SUITE"}, {group,[top1]}, {testcase,[tc12]}]).</pre><p>The following executes <strong>tc12</strong> <em>only</em> in group <strong>top1</strong>:</p><pre>
 $ ct_run -suite "x_SUITE" -group [top1] -case tc12
 1&gt; ct:run_test([{suite,"x_SUITE"}, {group,[[top1]]}, {testcase,[tc12]}]).</pre><p>The following searches <strong>top1</strong> and all its subgroups for <strong>tc16</strong> resulting
in that this test case executes in group <strong>sub121</strong>:</p><pre>
 $ ct_run -suite "x_SUITE" -group top1 -case tc16
 1&gt; ct:run_test([{suite,"x_SUITE"}, {group,[top1]}, {testcase,[tc16]}]).</pre><p>Using the specific path <strong>-group [sub121]</strong> or <strong>{group,[[sub121]]}</strong> gives
the same result in this example.</p><p>The following executes two tests, one including all cases and subgroups under
<strong>sub12</strong>, and one with <em>only</em> the test cases in <strong>sub12</strong>:</p><pre>
 $ ct_run -suite "x_SUITE" -group sub12 [sub12]
 1&gt; ct:run_test([{suite,"x_SUITE"}, {group,[sub12,[sub12]]}]).</pre><p>In the following example, <strong>Common Test</strong> finds and executes two tests, 
one for the path from <strong>top2</strong> to <strong>sub2X2</strong> through <strong>sub21</strong>, 
and one from <strong>top2</strong> to <strong>sub2X2</strong> through <strong>sub22</strong>:</p><pre>
 $ ct_run -suite "x_SUITE" -group sub2X2
 1&gt; ct:run_test([{suite,"x_SUITE"}, {group,[sub2X2]}]).</pre><p>In the following example, by specifying the unique path <strong>top2 -&gt; sub21 -&gt; sub2X2</strong>, 
only one test is executed. The second possible path, from <strong>top2</strong> to <strong>sub2X2</strong> 
(from the former example) is discarded:</p><pre>
 $ ct_run -suite "x_SUITE" -group [sub21,sub2X2]
 1&gt; ct:run_test([{suite,"x_SUITE"}, {group,[[sub21,sub2X2]]}]).</pre><p>The following executes only the test cases for <strong>sub22</strong> and in reverse order 
compared to the group definition:</p><pre>
 $ ct_run -suite "x_SUITE" -group [sub22] -case tc22 tc21
 1&gt; ct:run_test([{suite,"x_SUITE"}, {group,[[sub22]]}, {testcase,[tc22,tc21]}]).</pre><p>If a test case belonging to a group (according to the group definition) is executed
without a group specification, that is, simply by
(using the command line):</p><p><strong>$ ct_run -suite "my_SUITE" -case my_tc</strong></p><p>or (using the Erlang shell):</p><p><strong>1&gt; ct:run_test([{suite,"my_SUITE"}, {testcase,my_tc}]).</strong></p><p>then <strong>Common Test</strong> ignores the group definition and executes the test case 
in the scope of the test suite only (no group configuration functions are called).</p><p>The group specification feature, as presented in this section, can also
be used in <a href="./run_test_chapter#test_specifications">Test Specifications</a> (with some extra features added).</p><h4>Running the Interactive Shell Mode</h4><p>You can start <strong>Common Test</strong> in an interactive shell mode where no
automatic testing is performed. Instead, <strong>Common Test</strong>
starts its utility processes, installs configuration data (if any),
and waits for the user to call functions (typically test case support
functions) from the Erlang shell.</p><p>The shell mode is useful, for example, for debugging test suites, analyzing
and debugging the SUT during "simulated" test case execution, and 
trying out various operations during test suite development.</p><p>To start the interactive shell mode, start an Erlang shell 
manually and call <a href="./ct#install-1">ct#install-1</a> 
to install any configuration data you might need (use <strong>[]</strong> as argument otherwise). 
Then call <a href="./ct#start_interactive-0">ct#start_interactive-0</a> 
to start <strong>Common Test</strong>.</p><p>If you use the <strong>ct_run</strong> program, you can start 
the Erlang shell and <strong>Common Test</strong> in one go by using the flag <strong>-shell</strong> and, 
optionally, flag <strong>-config</strong> and/or <strong>-userconfig</strong>.</p><p><em>Examples:</em></p><ul><li><strong>ct_run -shell</strong></li><li><strong>ct_run -shell -config cfg/db.cfg</strong></li><li><strong>ct_run -shell -userconfig db_login testuser x523qZ</strong></li></ul><p>If no configuration file is specified with command <strong>ct_run</strong>,
a warning is displayed. If <strong>Common Test</strong> has been run from the same
directory earlier, the same configuration file(s) are used again. If <strong>Common Test</strong> 
has not been run from this directory before, no configuration files are available.</p><p>If any functions using "required configuration data" (for example, functions 
<strong>ct_telnet</strong> or <strong>ct_ftp</strong>) are to be called from the Erlang shell, first require 
configuration data with <a href="./ct#require-1">ct#require-1</a>. This is equivalent to a <strong>require</strong> statement 
in the <a href="./write_test_chapter#suite">Test Suite Information Function</a> 
or in the <a href="./write_test_chapter#info_function">Test Case Information Function</a>.</p><p><em>Example:</em></p><pre> 
 1&gt; ct:require(unix_telnet, unix).
 ok
 2&gt; ct_telnet:open(unix_telnet).
 {ok,&lt;0.105.0&gt;}
 4&gt; ct_telnet:cmd(unix_telnet, "ls .").
 {ok,["ls .","file1  ...",...]}</pre><p>Everything that <strong>Common Test</strong> normally prints in the test case logs,
are in the interactive mode written to a log named <strong>ctlog.html</strong> 
in directory  <strong>ct_run.&lt;timestamp&gt;</strong>. A link to this 
file is available in the file named <strong>last_interactive.html</strong> in the 
directory from which you execute <strong>ct_run</strong>. Specifying a different
root directory for the logs than the current working directory
is not supported.</p><p>If you wish to exit the interactive mode (for example, to start an automated 
test run with <a href="./ct#run_test-1">ct#run_test-1</a>), 
call function
<a href="./ct#stop_interactive-0">ct#stop_interactive-0</a>. 
This shuts down the running <strong>ct</strong> application. Associations between
configuration names and data created with <strong>require</strong> are 
consequently deleted. Function
<a href="./ct#start_interactive-0">ct#start_interactive-0</a> 
takes you back into interactive mode, but the previous state is not restored.</p><h4>Step-by-Step Execution of Test Cases with the Erlang Debugger</h4><p>Using <strong>ct_run -step [opts]</strong>, or by passing option <strong>{step,Opts}</strong> 
to <a href="./ct#run_test-1">ct#run_test-1</a>, 
the following is possible:</p><ul><li>Get the Erlang Debugger started automatically.</li><li>Use its graphical interface to investigate the state of the current test case.</li><li>Execute the test case step-by-step and/or set execution breakpoints.</li></ul><p>If no extra options are specified with flag/option <strong>step</strong>,
breakpoints are set automatically on the test cases that
are to be executed by <strong>Common Test</strong>, and those functions only. If
step option <strong>config</strong> is specified, breakpoints are also initially 
set on the configuration functions in the suite, that is,
<strong>init_per_suite/1</strong>, <strong>end_per_suite/1</strong>,
<strong>init_per_group/2</strong>, <strong>end_per_group/2</strong>,
<strong>init_per_testcase/2</strong> and <strong>end_per_testcase/2</strong>.</p><p><strong>Common Test</strong> enables the Debugger auto-attach feature, which means
that for every new interpreted test case function that starts to execute, 
a new trace window automatically pops up (as each test 
case executes on a dedicated Erlang process). Whenever a new test case starts,
<strong>Common Test</strong> attempts to close the inactive trace window of the previous 
test case. However, if you prefer <strong>Common Test</strong> to leave inactive trace 
windows, use option <strong>keep_inactive</strong>.</p><p>The step functionality can be used together with flag/option <strong>suite</strong> and 
<strong>suite</strong> + <strong>case/testcase</strong>, but not together with <strong>dir</strong>.</p><a name="test_specifications"></a><h4>Test Specifications</h4><h4>General Description</h4><p>The most flexible way to specify what to test, is to use a
test specification, which is a sequence of
Erlang terms. The terms are normally declared in one or more text files
(see <a href="./ct#run_test-1">ct#run_test-1</a>), but
can also be passed to <strong>Common Test</strong> on the form of a list (see
<a href="./ct#run_testspec-1">ct#run_testspec-1</a>).
There are two general types of terms: configuration terms and test
specification terms.</p><p>With configuration terms it is, for example, possible to do the following:</p><ul><li>Label the test run (similar to <strong>ct_run -label</strong>).</li><li>Evaluate any expressions before starting the test.</li><li>Import configuration data (similar to <strong>ct_run -config/-userconfig</strong>).</li><li>Specify the top-level HTML log directory (similar to <strong>ct_run -logdir</strong>).</li><li>Enable code coverage analysis (similar to <strong>ct_run -cover</strong>).</li><li>Install <strong>Common Test Hooks</strong> (similar to <strong>ct_run -ch_hooks</strong>).</li><li>Install <strong>event_handler</strong> plugins (similar to <strong>ct_run -event_handler</strong>).</li><li>Specify include directories to be passed to the compiler for  automatic compilation (similar to <strong>ct_run -include</strong>).</li><li>Disable the auto-compilation feature (similar to <strong>ct_run -no_auto_compile</strong>).</li><li>Set verbosity levels (similar to <strong>ct_run -verbosity</strong>).</li></ul><p>Configuration terms can be combined with <strong>ct_run</strong> start flags
or <strong>ct:run_test/1</strong> options. The result is, for some flags/options
and terms, that the values are merged (for example, configuration files,
include directories, verbosity levels, and silent connections) and for
others that the start flags/options override the test specification
terms (for example, log directory, label, style sheet, and auto-compilation).</p><p>With test specification terms, it is possible to state exactly
which tests to run and in which order. A test term specifies
either one or more suites, one or more test case groups (possibly nested),
or one or more test cases in a group (or in multiple groups) or in a suite.</p><p>Any number of test terms can be declared in sequence.
<strong>Common Test</strong> compiles by default the terms into one or more tests 
to be performed in one resulting test run. A term that
specifies a set of test cases "swallows" one that only
specifies a subset of these cases. For example, the result of merging
one term specifying that all cases in suite S are to be
executed, with another term specifying only test case X and Y in
S, is a test of all cases in S. However, if a term specifying
test case X and Y in S is merged with a term specifying case Z
in S, the result is a test of X, Y, and Z in S. To disable this
behavior, that is, to instead perform each test sequentially in a 
"script-like" manner, set term <strong>merge_tests</strong> to <strong>false</strong> 
in the test specification.</p><p>A test term can also specify one or more test suites, groups,
or test cases to be skipped. Skipped suites, groups, and cases
are not executed and show up in the HTML log files as <strong>SKIPPED</strong>.</p><h4>Using Multiple Test Specification Files</h4><p>When multiple test specification files are specified at startup (either
with <strong>ct_run -spec file1 file2 ...</strong> or 
<strong>ct:run_test([{spec, [File1,File2,...]}])</strong>), 
<strong>Common Test</strong> either executes one test run per specification file, 
or joins the files and performs all tests within one single test run. 
The first behavior is the default one. The latter requires that start
flag/option <strong>join_specs</strong> is provided, for example,
<strong>run_test -spec ./my_tests1.ts ./my_tests2.ts -join_specs</strong>.</p><p>Joining a number of specifications, or running them separately, can
also be accomplished with (and can be combined with) test specification
file inclusion.</p><h4>Test Specification File Inclusion</h4><p>With the term <strong>specs</strong>, a test specification can include 
other specifications. An included specification can either be joined 
with the source specification or used to produce a separate test run 
(as with start flag/option <strong>join_specs</strong> above).</p><p><em>Example:</em></p><pre>
 %% In specification file "a.spec"
 {specs, join, ["b.spec", "c.spec"]}.
 {specs, separate, ["d.spec", "e.spec"]}.
 %% Config and test terms follow
 ...</pre><p>In this example, the test terms defined in files "b.spec" and "c.spec"
are joined with the terms in source specification "a.spec"
(if any). The inclusion of specifications "d.spec" and
"e.spec" results in two separate, and independent, test runs
(one for each included specification).</p><p>Option <strong>join</strong> does not imply that the test terms
are merged, only that all tests are executed in one single test run.</p><p>Joined specifications share common configuration settings, such as
the list of <strong>config</strong> files or <strong>include</strong> directories.
For configurations that cannot be combined, such as settings for <strong>logdir</strong>
or <strong>verbosity</strong>, it is up to the user to ensure there are no clashes
when the test specifications are joined. Specifications included with
option <strong>separate</strong> do not share configuration settings with the
source specification. This is useful, for example, if there are clashing
configuration settings in included specifications, making it them impossible
to join.</p><p>If <strong>{merge_tests,true}</strong> is set in the source specification
(which is the default setting), terms in joined specifications are
merged with terms in the source specification (according to the
description of <strong>merge_tests</strong> earlier).</p><p>Notice that it is always the <strong>merge_tests</strong> setting in the source
specification that is used when joined with other specifications.
Say, for example, that a source specification A, with tests TA1 and TA2, has
<strong>{merge_tests,false}</strong> set, and that it includes another specification,
B, with tests TB1 and TB2, that has <strong>{merge_tests,true}</strong> set.
The result is that the test series <strong>TA1,TA2,merge(TB1,TB2)</strong>
is executed. The opposite <strong>merge_tests</strong> settings would result in
the test series <strong>merge(merge(TA1,TA2),TB1,TB2)</strong>.</p><p>The term <strong>specs</strong> can be used to nest specifications,
that is, have one specification include other specifications, which in turn
include others, and so no</p><h4>Test Case Groups</h4><p>When a test case group is specified, the resulting test
executes function <strong>init_per_group</strong>, followed by all test
cases and subgroups (including their configuration functions), and
finally function <strong>end_per_group</strong>. Also, if particular
test cases in a group are specified, <strong>init_per_group</strong>
and <strong>end_per_group</strong>, for the group in question, are
called. If a group defined (in <strong>Suite:group/0</strong>) as
a subgroup of another group, is specified (or if particular test
cases of a subgroup are), <strong>Common Test</strong> calls the configuration
functions for the top-level groups and for the subgroup
in question (making it possible to pass configuration data all
the way from <strong>init_per_suite</strong> down to the test cases in the
subgroup).</p><p>The test specification uses the same mechanism for specifying
test case groups through names and paths, as explained in section
<a href="./run_test_chapter#group_execution">Test Case Group Execution</a>,
with the addition of element <strong>GroupSpec</strong>.</p><p>Element <strong>GroupSpec</strong> makes it possible to specify
group execution properties that overrides those in the
group definition (that is, in <strong>groups/0</strong>). Execution properties for
subgroups might be overridden as well. This feature makes it possible to
change properties of groups at the time of execution,
without having to edit the test suite. The same feature is available for 
<strong>group</strong> elements in the <strong>Suite:all/0</strong> list. For details and examples,
see section <a href="./write_test_chapter#test_case_groups"> Test Case Groups</a>.</p><h4>Test Specification Syntax</h4><p>Test specifications can be used to run tests both in a single 
test host environment and in a distributed <strong>Common Test</strong> environment 
(Large Scale Testing). The node parameters in term <strong>init</strong> are only
relevant in the latter (see section
<a href="./ct_master_chapter#test_specifications">Test Specifications</a> 
in Large Scale Testing). For details about the various terms, see the 
corresponding sections in the User's Guide, for example, the following:
</p><ul><li>The <a href="./run_test_chapter#ct_run"> program</a> for an overview of available start flags (as most flags have a corresponding configuration term)</li><li><a href="./write_test_chapter#logging">Logging</a> (for terms <strong>verbosity</strong>, <strong>stylesheet</strong>, <strong>basic_html</strong> and <strong>esc_chars</strong>)</li><li><a href="./config_file_chapter#top">External Configuration Data</a> (for terms <strong>config</strong> and <strong>userconfig</strong>)</li><li><a href="./event_handler_chapter#event_handling">Event Handling</a> (for the <strong>event_handler</strong> term)</li><li><a href="./ct_hooks_chapter#installing">Common Test Hooks</a> (for term <strong>ct_hooks</strong>)</li></ul><p><em>Configuration terms:</em></p><pre>
 {merge_tests, Bool}.

 {define, Constant, Value}.

 {specs, InclSpecsOption, TestSpecs}.

 {node, NodeAlias, Node}.

 {init, InitOptions}.
 {init, [NodeAlias], InitOptions}.

 {label, Label}.
 {label, NodeRefs, Label}.

 {verbosity, VerbosityLevels}.
 {verbosity, NodeRefs, VerbosityLevels}.

 {stylesheet, CSSFile}.
 {stylesheet, NodeRefs, CSSFile}.

 {silent_connections, ConnTypes}.
 {silent_connections, NodeRefs, ConnTypes}.

 {multiply_timetraps, N}.
 {multiply_timetraps, NodeRefs, N}.

 {scale_timetraps, Bool}.
 {scale_timetraps, NodeRefs, Bool}.

 {cover, CoverSpecFile}.
 {cover, NodeRefs, CoverSpecFile}.

 {cover_stop, Bool}.
 {cover_stop, NodeRefs, Bool}.

 {include, IncludeDirs}.
 {include, NodeRefs, IncludeDirs}.

 {auto_compile, Bool},
 {auto_compile, NodeRefs, Bool},

 {abort_if_missing_suites, Bool},
 {abort_if_missing_suites, NodeRefs, Bool},

 {config, ConfigFiles}.
 {config, ConfigDir, ConfigBaseNames}.
 {config, NodeRefs, ConfigFiles}.
 {config, NodeRefs, ConfigDir, ConfigBaseNames}.

 {userconfig, {CallbackModule, ConfigStrings}}.
 {userconfig, NodeRefs, {CallbackModule, ConfigStrings}}.

 {logdir, LogDir}.                                        
 {logdir, NodeRefs, LogDir}.

 {logopts, LogOpts}.
 {logopts, NodeRefs, LogOpts}.

 {create_priv_dir, PrivDirOption}.
 {create_priv_dir, NodeRefs, PrivDirOption}.

 {event_handler, EventHandlers}.
 {event_handler, NodeRefs, EventHandlers}.
 {event_handler, EventHandlers, InitArgs}.
 {event_handler, NodeRefs, EventHandlers, InitArgs}.

 {ct_hooks, CTHModules}.
 {ct_hooks, NodeRefs, CTHModules}.

 {enable_builtin_hooks, Bool}.

 {basic_html, Bool}.
 {basic_html, NodeRefs, Bool}.

 {esc_chars, Bool}.
 {esc_chars, NodeRefs, Bool}.

 {release_shell, Bool}.</pre><p><em>Test terms:</em></p><pre>
 {suites, Dir, Suites}.                                
 {suites, NodeRefs, Dir, Suites}.

 {groups, Dir, Suite, Groups}.
 {groups, NodeRefs, Dir, Suite, Groups}.

 {groups, Dir, Suite, Groups, {cases,Cases}}.
 {groups, NodeRefs, Dir, Suite, Groups, {cases,Cases}}.

 {cases, Dir, Suite, Cases}.                           
 {cases, NodeRefs, Dir, Suite, Cases}.

 {skip_suites, Dir, Suites, Comment}.
 {skip_suites, NodeRefs, Dir, Suites, Comment}.

 {skip_groups, Dir, Suite, GroupNames, Comment}.
 {skip_groups, NodeRefs, Dir, Suite, GroupNames, Comment}.

 {skip_cases, Dir, Suite, Cases, Comment}.
 {skip_cases, NodeRefs, Dir, Suite, Cases, Comment}.</pre><a name="types"></a><p><em>Types:</em></p><pre>
 Bool            = true | false
 Constant        = atom()
 Value           = term()
 InclSpecsOption = join | separate
 TestSpecs       = string() | [string()]
 NodeAlias       = atom()
 Node            = node()
 NodeRef         = NodeAlias | Node | master
 NodeRefs        = all_nodes | [NodeRef] | NodeRef
 InitOptions     = term()
 Label           = atom() | string()
 VerbosityLevels = integer() | [{Category,integer()}]
 Category        = atom()
 CSSFile         = string()
 ConnTypes       = all | [atom()]
 N               = integer()
 CoverSpecFile   = string()
 IncludeDirs     = string() | [string()]
 ConfigFiles     = string() | [string()]
 ConfigDir       = string()
 ConfigBaseNames = string() | [string()]
 CallbackModule  = atom()
 ConfigStrings   = string() | [string()]
 LogDir          = string()
 LogOpts         = [term()]
 PrivDirOption   = auto_per_run | auto_per_tc | manual_per_tc
 EventHandlers   = atom() | [atom()]
 InitArgs        = [term()]
 CTHModules      = [CTHModule |
		    {CTHModule, CTHInitArgs} |
		    {CTHModule, CTHInitArgs, CTHPriority}]
 CTHModule       = atom()
 CTHInitArgs     = term()
 Dir             = string()
 Suites          = atom() | [atom()] | all
 Suite           = atom()
 Groups          = GroupPath | [GroupPath] | GroupSpec | [GroupSpec] | all
 GroupPath       = [GroupName]
 GroupSpec       = GroupName | {GroupName,Properties} | {GroupName,Properties,GroupSpec}
 GroupName       = atom()
 GroupNames      = GroupName | [GroupName]
 Cases           = atom() | [atom()] | all
 Comment         = string() | ""</pre><p>The difference between the <strong>config</strong> terms above is that with
<strong>ConfigDir</strong>, <strong>ConfigBaseNames</strong> is a list of base names,
that is, without directory paths. <strong>ConfigFiles</strong> must be full names,
including paths. For example, the following two terms have the same meaning:</p><pre>
 {config, ["/home/testuser/tests/config/nodeA.cfg",
           "/home/testuser/tests/config/nodeB.cfg"]}.

 {config, "/home/testuser/tests/config", ["nodeA.cfg","nodeB.cfg"]}.</pre><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>Any relative paths, specified in the test specification, are
relative to the directory containing the test specification file if
<strong>ct_run -spec TestSpecFile ...</strong> or
<strong>ct:run:test([{spec,TestSpecFile},...])</strong>
executes the test.</p><p>The path is relative to the top-level log directory if
<strong>ct:run:testspec(TestSpec)</strong> executes the test.</p></div><h4>Constants</h4><p>The term <strong>define</strong> introduces a constant that is used to
replace the name <strong>Constant</strong> with <strong>Value</strong>, wherever it is found in
the test specification. This replacement occurs during an initial iteration
through the test specification. Constants can be used anywhere in the test
specification, for example, in any lists and tuples, and even in strings
and inside the value part of other constant definitions. A constant can
also be part of a node name, but that is the only place where a constant
can be part of an atom.</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>For the sake of readability, the name of the constant must always
begin with an uppercase letter, or a <strong>$</strong>, <strong>?</strong>, or <strong>_</strong>.
This means that it must always be single quoted (as the constant name is 
an atom, not text).</p></div><p>The main benefit of constants is that they can be used to reduce the size
(and avoid repetition) of long strings, such as file paths.</p><p><em>Examples:</em></p><pre>
 %% 1a. no constant
 {config, "/home/testuser/tests/config", ["nodeA.cfg","nodeB.cfg"]}.
 {suites, "/home/testuser/tests/suites", all}.

 %% 1b. with constant
 {define, 'TESTDIR', "/home/testuser/tests"}.
 {config, "'TESTDIR'/config", ["nodeA.cfg","nodeB.cfg"]}.
 {suites, "'TESTDIR'/suites", all}.

 %% 2a. no constants
 {config, [testnode@host1, testnode@host2], "../config", ["nodeA.cfg","nodeB.cfg"]}.
 {suites, [testnode@host1, testnode@host2], "../suites", [x_SUITE, y_SUITE]}.

 %% 2b. with constants
 {define, 'NODE', testnode}.
 {define, 'NODES', ['NODE'@host1, 'NODE'@host2]}.
 {config, 'NODES', "../config", ["nodeA.cfg","nodeB.cfg"]}.
 {suites, 'NODES', "../suites", [x_SUITE, y_SUITE]}.</pre><p>Constants make the test specification term <strong>alias</strong>, in previous
versions of <strong>Common Test</strong>, redundant. This term is deprecated but
remains supported in upcoming <strong>Common Test</strong> releases. Replacing <strong>alias</strong>
terms with <strong>define</strong> is strongly recommended though. An example
of such replacement follows:</p><pre>
 %% using the old alias term
 {config, "/home/testuser/tests/config/nodeA.cfg"}.
 {alias, suite_dir, "/home/testuser/tests/suites"}.
 {groups, suite_dir, x_SUITE, group1}.

 %% replacing with constants
 {define, 'TestDir', "/home/testuser/tests"}.
 {define, 'CfgDir', "'TestDir'/config"}.
 {define, 'SuiteDir', "'TestDir'/suites"}.
 {config, 'CfgDir', "nodeA.cfg"}.
 {groups, 'SuiteDir', x_SUITE, group1}.</pre><p>Constants can well replace term <strong>node</strong> also, but
this still has a declarative value, mainly when used in combination
with <strong>NodeRefs == all_nodes</strong> 
(see <a href="#types">Types</a>).</p><h4>Example</h4><p>Here follows a simple test specification example:</p><pre>
 {define, 'Top', "/home/test"}.
 {define, 'T1', "'Top'/t1"}.
 {define, 'T2', "'Top'/t2"}.
 {define, 'T3', "'Top'/t3"}.
 {define, 'CfgFile', "config.cfg"}.

 {logdir, "'Top'/logs"}.

 {config, ["'T1'/'CfgFile'", "'T2'/'CfgFile'", "'T3'/'CfgFile'"]}.

 {suites, 'T1', all}.
 {skip_suites, 'T1', [t1B_SUITE,t1D_SUITE], "Not implemented"}.
 {skip_cases, 'T1', t1A_SUITE, [test3,test4], "Irrelevant"}.
 {skip_cases, 'T1', t1C_SUITE, [test1], "Ignore"}.

 {suites, 'T2', [t2B_SUITE,t2C_SUITE]}.
 {cases, 'T2', t2A_SUITE, [test4,test1,test7]}.

 {skip_suites, 'T3', all, "Not implemented"}.</pre><p>The example specifies the following:</p><ul><li>The specified <strong>logdir</strong> directory is used for storing  the HTML log files (in subdirectories tagged with node name,  date, and time).</li><li>The variables in the specified test system configuration files are imported for the test.</li><li>The first test to run includes all suites for system <strong>t1</strong>.  Suites <strong>t1B</strong> and <strong>t1D</strong> are excluded from the test. Test cases  <strong>test3</strong> and <strong>test4</strong> in <strong>t1A</strong> and <strong>test1</strong> case in <strong>t1C</strong>  are also excluded from the test.</li><li>The second test to run is for system <strong>t2</strong>. The included suites are <strong>t2B</strong> and <strong>t2C</strong>. Test cases <strong>test4</strong>, <strong>test1</strong>, and <strong>test7</strong> in suite <strong>t2A</strong> are also included. The test cases are executed in the specified order.</li><li>The last test to run is for system <strong>t3</strong>. Here, all suites are skipped and this is explicitly noted in the log files.</li></ul><h4>The init Term</h4><p>With term <strong>init</strong> it is possible to specify initialization options
for nodes defined in the test specification. There are options
to start the node and to evaluate any function on the node.
For details, see section <a href="./ct_master_chapter#ct_slave">Automatic Startup of Test Target Nodes</a> in section Using Common Test for Large Scale Testing.</p><h4>User-Specific Terms</h4><p>The user can provide a test specification including (for <strong>Common Test</strong>) 
unrecognizable terms. If this is desired, use flag <strong>-allow_user_terms</strong> 
when starting tests with <strong>ct_run</strong>. This forces <strong>Common Test</strong> to ignore 
unrecognizable terms. In this mode, <strong>Common Test</strong> is not able to check the 
specification for errors as efficiently as if the scanner runs in default mode. 
If <a href="./ct#run_test-1">ct#run_test-1</a> is used
for starting the tests, the relaxed scanner mode is enabled by tuple
<strong>{allow_user_terms,true}</strong>.</p><h4>Reading Test Specification Terms</h4><p>Terms in the current test specification
(that is, the specification that has been used to configure and run the current test)
can be looked up.
The function <a href="./ct#get_testspec_terms-0">ct#get_testspec_terms-0</a> 
returns a list of all test specification terms (both configuration terms and test terms), 
and <strong>get_testspec_terms(Tags)</strong> returns the term (or a list of terms) matching the 
tag (or tags) in <strong>Tags</strong>.</p><p>For example, in the test specification:</p><pre>
 ...
 {label, my_server_smoke_test}.
 {config, "../../my_server_setup.cfg"}.
 {config, "../../my_server_interface.cfg"}.
 ...</pre><p>And in, for example, a test suite or a <strong>Common Test Hook</strong> function:</p><pre>
 ...
 [{label,[{_Node,TestType}]}, {config,CfgFiles}] =
     ct:get_testspec_terms([label,config]),

 [verify_my_server_cfg(TestType, CfgFile) || {Node,CfgFile} &lt;- CfgFiles,
					     Node == node()];
 ...</pre><h4>Running Tests from the Web-Based GUI</h4><p>The web-based GUI, Virtual Test Server (VTS), is started with the
<a href="./run_test_chapter#ct_run">run_test_chapter#ct_run</a>
program. From the GUI, you can load configuration files and select
directories, suites, and cases to run. You can also state the
configuration files, directories, suites, and cases on the command line
when starting the web-based GUI.
</p><p><em>Examples:</em></p><ul><li><strong>ct_run -vts</strong></li><li><strong>ct_run -vts -config &lt;configfilename&gt;</strong></li><li><strong>ct_run -vts -config &lt;configfilename&gt; -suite &lt;suitewithfullpath&gt; -case &lt;casename&gt;</strong></li></ul><p>From the GUI you can run tests and view the result and the logs.
</p><p><strong>ct_run -vts</strong> tries to open the <strong>Common Test</strong> start
page in an existing web browser window, or start the browser if it is
not running. Which browser to start can be specified with
the browser start command option:</p><p><strong>ct_run -vts -browser &lt;browser_start_cmd&gt;</strong></p><p><em>Example:</em></p><p><strong>$ ct_run -vts -browser 'firefox&amp;'</strong></p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>The browser must run as a separate OS process, otherwise VTS hangs.</p></div><p>If no specific browser start command is specified, Firefox is
the default browser on Unix platforms, and Internet Explorer on Windows.
If <strong>Common Test</strong> fails to start a browser automatically, or <strong>none</strong> is
specified as the value for <strong>-browser</strong> (that is, <strong>-browser none</strong>), start your
favourite browser manually and type the URL that <strong>Common Test</strong>
displays in the shell.</p><a name="log_files"></a><h4>Log Files</h4><p>As the execution of the test suites proceed, events are logged in
the following four different ways:</p><ul><li>Text to the operator console.</li><li>Suite-related information is sent to the major log file.</li><li>Case-related information is sent to the minor log file.</li><li>The HTML overview log file is updated with test results.</li><li>A link to all runs executed from a certain directory is written in the log named <strong>all_runs.html</strong> and direct links to all tests (the latest results) are written to the top-level <strong>index.html</strong>.</li></ul><p>Typically the operator, possibly running hundreds or thousands of
test cases, does not want to fill the console with details
about, or printouts from, specific test cases. By default, the 
operator only sees the following:</p><ul><li>A confirmation that the test has started and information about how  many test cases are executed in total.</li><li>A small note about each failed test case.</li><li>A summary of all the run test cases.</li><li>A confirmation when the test run is complete.</li><li>Some special information, such as error reports, progress reports, and printouts written with <strong>erlang:display/1</strong>, or <strong>io:format/3</strong> specifically addressed to a receiver other than <strong>standard_io</strong> (for example, the default group leader process <strong>user</strong>).</li></ul><p>To dig deeper into the general results, or
the result of a specific test case, the operator can do so by
following the links in the HTML presentation and read the
major or minor log files. The "all_runs.html" page is a good
starting point. It is located in <strong>logdir</strong> and contains
a link to each test run, including a quick overview (with date and time,
node name, number of tests, test names, and test result totals).</p><p>An "index.html" page is written for each test run (that is, stored in
the <strong>ct_run</strong> directory tagged with node name, date, and time). This
file provides an overview of all individual tests performed in the 
same test run. The test names follow the following convention:</p><ul><li><strong>TopLevelDir.TestDir</strong> (all suites in <strong>TestDir</strong> executed)</li><li><strong>TopLevelDir.TestDir:suites</strong> (specific suites executed)</li><li><strong>TopLevelDir.TestDir.Suite</strong> (all cases in <strong>Suite</strong> executed)</li><li><strong>TopLevelDir.TestDir.Suite:cases</strong> (specific test cases executed)</li><li><strong>TopLevelDir.TestDir.Suite.Case</strong> (only <strong>Case</strong> executed)</li></ul><p>The "test run index" page includes a link to the <strong>Common Test</strong>
Framework Log file in which information about imported
configuration data and general test progress is written. This
log file is useful to get snapshot information about the test
run during execution. It can also be helpful when
analyzing test results or debugging test suites.</p><p>The "test run index" page indicates if a test has missing
suites (that is, suites that <strong>Common Test</strong> failed to
compile). Names of the missing suites can be found in the
<strong>Common Test</strong> Framework Log file.</p><p>The major log file shows a detailed report of the test run. It
includes test suite and test case names, execution time, the 
exact reason for failures, and so on. The information is available in both
a file with textual and with HTML representation. The HTML file shows a 
summary that gives a good overview of the test run. It also has links 
to each individual test case log file for quick viewing with an HTML 
browser.</p><p>The minor log files contain full details of every single test
case, each in a separate file. This way, it is
straightforward	to compare the latest results to that of previous
test runs, even if the set of test cases changes. If application SASL
is running, its logs are also printed to the current minor log file by the
<a href="../common_test/ct_hooks_chapter#builtin_cths"> cth_log_redirect built-in hook</a>.
</p><p>The full name of the minor log file (that is, the name of the file
including the absolute directory path) can be read during execution
of the test case. It comes as value in tuple
<strong>{tc_logfile,LogFileName}</strong> in the <strong>Config</strong> list (which means it
can also be read by a pre- or post <strong>Common Test Hook</strong> function). Also,
at the start of a test case, this data is sent with an event
to any installed event handler.	For details, see section
<a href="./event_handler_chapter#event_handling">Event Handling</a>.
</p><p>The log files are written continuously during a test run and links are
always created initially when a test starts. Thevtest progress can therefore 
be followed simply by refreshing pages in the HTML browser.
Statistics totals are not presented until a test is complete however.</p><a name="logopts"></a><h4>Log Options</h4><p>With start flag <strong>logopts</strong> options that modify some aspects 
of the logging behavior can be specified.
The following options are available:</p><dl><dt><strong>no_src</strong></dt><dd><p>The HTML version of the test suite source code is not 
generated during the test run (and is consequently not available 
in the log file system).</p></dd><dt><strong>no_nl</strong></dt><dd><p><strong>Common Test</strong> does not add a newline character <strong>(\n)</strong> 
to the end of an output string that it receives from a call to, for example, 
<strong>io:format/2</strong>, and which it prints to the test case log.</p></dd></dl><p>For example, if a test is started with:</p><p><strong>$ ct_run -suite my_SUITE -logopts no_nl</strong></p><p>then printouts during the test made by successive calls to <strong>io:format("x")</strong>,
appears in the test case log as:</p><p><strong>xxx</strong></p><p>instead of each <strong>x</strong> printed on a new line, which is the default behavior.</p><a name="table_sorting"></a><h4>Sorting HTML Table Columns</h4><p>By clicking the name in the column header of any table 
(for example, "Ok", "Case", "Time", and so on), the table rows are sorted 
in whatever order makes sense for the type of value (for example,
numerical for "Ok" or "Time", and alphabetical for "Case"). The sorting is 
performed through JavaScript code, automatically inserted into the HTML 
log files. <strong>Common Test</strong> uses the <a href="http://jquery.com">jQuery</a> 
library and the
<a href="http://tablesorter.com">tablesorter</a> plugin, 
with customized sorting functions, for this implementation.</p><h4>The Unexpected I/O Log</h4><p>The test suites overview page includes a link to the Unexpected I/O Log.
In this log, <strong>Common Test</strong> saves printouts made with
<a href="./ct#log-2">ct#log-2</a> and 
<a href="./ct#pal-2">ct#pal-2</a>, as well as captured system 
error- and progress reports, which cannot be associated with particular test cases and
therefore cannot be written to individual test case log files. This occurs,
for example, if a log printout is made from an external process (not a test 
case process), <em>or</em> if an error- or progress report comes in, during a short 
interval while <strong>Common Test</strong> is not executing a test case or configuration 
function, <em>or</em> while <strong>Common Test</strong> is currently executing a parallel 
test case group.</p><a name="pre_post_test_io_log"></a><h4>The Pre- and Post Test I/O Log</h4><p>The <strong>Common Test</strong> Framework Log page includes links to the
Pre- and Post Test I/O Log. In this log, <strong>Common Test</strong> saves printouts made 
with <strong>ct:log/1,2,3,4,5</strong> and <strong>ct:pal/1,2,3,4,5</strong>, as well as captured system error-
and progress reports, which take place before, and after, the test run.
Examples of this are printouts from a CT hook init- or terminate function, or
progress reports generated when an OTP application is started from a CT hook
init function. Another example is an error report generated because of
a failure when an external application is stopped from a CT hook terminate function.
All information in these examples ends up in the Pre- and Post Test I/O Log.
For more information on how to synchronize test runs with external user
applications, see section
<a href="./ct_hooks_chapter#synchronizing">Synchronizing</a>
in section Common Test Hooks.</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>Logging to file with <strong>ct:log/1,2,3,4,5</strong> or <strong>ct:pal/1,2,3,4,5</strong>
only works when <strong>Common Test</strong> is running. Printouts with <strong>ct:pal/1,2,3,4,5</strong>
are however always displayed on screen.</p></div><a name="delete_old_logs"></a><h4>Delete Old Logs</h4><p><strong>Common Test</strong> can automatically delete old log. This
is specified with the <strong>keep_logs</strong> option. The default
value for this option is <strong>all</strong>, which means that no
logs are deleted. If the value is set to an
integer, <strong>N</strong>, <strong>Common Test</strong> deletes
all <strong>ct_run.&lt;timestamp&gt;</strong> directories, except
the <strong>N</strong> newest.</p><a name="html_stylesheet"></a><h4>HTML Style Sheets</h4><p><strong>Common Test</strong> uses an HTML Style Sheet (CSS file) to control the look of
the HTML log files generated during test runs. If the log files are not 
displayed correctly in the browser of your choice, or you prefer a more 
primitive ("pre <strong>Common Test</strong> v1.6") look of the logs, use the start 
flag/option:</p><pre>
 basic_html</pre><p>This disables the use of style sheets and JavaScripts (see
<a href="#table_sorting">Sorting HTML Table Columns</a>).</p><p><strong>Common Test</strong> includes an <em>optional</em> feature to allow
user HTML style sheets for customizing printouts. The
functions in <strong>ct</strong> that print to a test case HTML log
file (<strong>log/3,4,5</strong> and <strong>pal/3,4,5</strong>) accept <strong>Category</strong>
as first argument. With this argument a category can be specified 
that can be mapped to a named <strong>div</strong> selector in a CSS rule-set.
This is useful, especially for coloring text
differently depending on the type of (or reason for) the
printout. Say you want one particular background color for test system
configuration information, a different one for test system
state information, and finally one for errors detected by the
test case functions. The corresponding style sheet can
look as follows:</p><pre>
 div.sys_config  { background:blue }
 div.sys_state   { background:yellow }
 div.error       { background:red }</pre><p>Common Test prints the text from <strong>ct:log/3,4,5</strong> or
<strong>ct:pal/3,4,5</strong> inside a <strong>pre</strong> element
nested under the named <strong>div</strong> element. Since the <strong>pre</strong> selector
has a predefined CSS rule (in file <strong>ct_default.css</strong>) for the attributes
<strong>color</strong>, <strong>font-family</strong> and <strong>font-size</strong>, if a user wants to
change any of the predefined attribute settings, a new rule for <strong>pre</strong>
must be added to the user stylesheet. Example:</p><pre>
div.error pre { color:white }</pre><p>Here, white text is used instead of the default black for <strong>div.error</strong>
printouts (and no other attribute settings for <strong>pre</strong> are affected).</p><p>To install the CSS file (<strong>Common Test</strong> inlines the definition in the 
HTML code), the file name can be provided when executing <strong>ct_run</strong>.</p><p><em>Example:</em></p><pre>
 $ ct_run -dir $TEST/prog -stylesheet $TEST/styles/test_categories.css</pre><p>Categories in a CSS file installed with flag <strong>-stylesheet</strong>
are on a global test level in the sense that they can be used in any 
suite that is part of the test run.</p><p>Style sheets can also be installed on a per suite and
per test case basis.</p><p><em>Example:</em></p><pre>
 -module(my_SUITE).
 ...
 suite() -&gt; [..., {stylesheet,"suite_categories.css"}, ...].
 ...
 my_testcase(_) -&gt;
     ...
     ct:log(sys_config, "Test node version: ~p", [VersionInfo]),
     ...
     ct:log(sys_state, "Connections: ~p", [ConnectionInfo]),
     ...
     ct:pal(error, "Error ~p detected! Info: ~p", [SomeFault,ErrorInfo]),
     ct:fail(SomeFault).</pre><p>If the style sheet is installed as in this example, the categories are 
private to the suite in question. They can be used by all test cases in the 
suite, but cannot be used by other suites. A suite private style sheet, 
if specified, is used in favor of a global style sheet (one specified 
with flag <strong>-stylesheet</strong>). A stylesheet tuple (as returned by <strong>suite/0</strong> 
above) can also be returned from a test case information function. In this case the 
categories specified in the style sheet can only be used in that particular 
test case. A test case private style sheet is used in favor of a suite or 
global level style sheet.
</p><p>In a tuple <strong>{stylesheet,CSSFile}</strong>, if <strong>CSSFile</strong> is specified
with a path, for example, <strong>"$TEST/styles/categories.css"</strong>, this full
name is used to locate the file. However, if only the file name is specified,
for example, <strong>categories.css</strong>, the CSS file is assumed to be located
in the data directory, <strong>data_dir</strong>, of the suite. The latter use is
recommended, as it is portable compared to hard coding path names in the 
suite.</p><p>Argument <strong>Category</strong> in the previous example can have the
value (atom) <strong>sys_config</strong> (blue background), <strong>sys_state</strong>
(yellow background), or <strong>error</strong> (white text on red background).</p><a name="repeating_tests"></a><h4>Repeating Tests</h4><p>You can order <strong>Common Test</strong> to repeat the tests you specify. You can choose
to repeat tests a number of times, repeat tests for a specific period of time, 
or repeat tests until a particular stop time is reached. If repetition is controlled by
time, an action for <strong>Common Test</strong> to take upon time-out can be specified. 
Either <strong>Common Test</strong> performs all tests in the current run 
before stopping, or it stops when the current test job is finished. Repetition 
can be activated by <strong>ct_run</strong> start flags, or tuples in the <strong>ct:run:test/1</strong>
option list argument. The flags (options in parentheses) are the following:</p><ul><li><strong>-repeat N ({repeat,N})</strong>, where <strong>N</strong> is a positive integer</li><li><strong>-duration DurTime ({duration,DurTime})</strong>, where <strong>DurTime</strong> is the duration</li><li><strong>-until StopTime ({until,StopTime})</strong>, where <strong>StopTime</strong> is finish time</li><li><strong>-force_stop ({force_stop,true})</strong></li><li><strong>-force_stop skip_rest ({force_stop,skip_rest})</strong></li></ul><dl><dt><strong>DurTime</strong></dt><dd><p>The duration time is specified as <strong>HHMMSS</strong>, for example, <strong>-duration 012030</strong> 
or <strong>{duration,"012030"}</strong></p>, which means that the tests are executed and  (if time allows) repeated until time-out occurs after 1 hour, 20 minutes, and 30 seconds. </dd><dt><strong>StopTime</strong></dt><dd><p>The finish time can be specified as <strong>HHMMSS</strong> and is then interpreted as a 
time today (or possibly tomorrow), but can also be specified as <strong>YYMoMoDDHHMMSS</strong>,
for example, <strong>-until 071001120000</strong> or <strong>{until,"071001120000"}</strong>. This means
that the tests are executed and (if time allows) repeated, until 12 o'clock on the 1st 
of October 2007.</p> </dd></dl><p>When time-out occurs, <strong>Common Test</strong> never aborts the ongoing test case,
as this can leave the SUT in an undefined, and possibly bad, state.
Instead <strong>Common Test</strong>, by default, finishes the current test
run before stopping. If flag <strong>force_stop</strong> is
specified, <strong>Common Test</strong> stops when the current test job
is finished. If flag <strong>force_stop</strong> is specified with
<strong>skip_rest</strong>, <strong>Common Test</strong> only completes the current
test case and skips the remaining tests in the test job.</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>As <strong>Common Test</strong> always finishes at least the current test case,
the time specified with <strong>duration</strong> or <strong>until</strong> is never definitive.</p></div><p>Log files from every repeated test run is saved in normal <strong>Common Test</strong> 
fashion (described earlier).</p><p><strong>Common Test</strong> might later support an optional feature to only store the last (and possibly 
the first) set of logs of repeated test runs, but for now the user must be careful not 
to run out of disk space if tests are repeated during long periods of time.</p><p>For each test run that is part of a repeated session, information about the
particular test run is printed in the <strong>Common Test</strong> Framework Log. The information
includes the repetition number, remaining time, and so on.</p><p><em>Example 1:</em></p><pre>
 $ ct_run -dir $TEST_ROOT/to1 $TEST_ROOT/to2 -duration 001000 -force_stop</pre><p>Here, the suites in test directory <strong>to1</strong>, followed by the suites in <strong>to2</strong>, are 
executed in one test run. A time-out event occurs after 10 minutes. As long as there is 
time left, <strong>Common Test</strong> repeats the test run (that is, starting over with test <strong>to1</strong>). 
After time-out, <strong>Common Test</strong> stops when the current job is finished
(because of flag <strong>force_stop</strong>). As a result, the specified test run can be 
aborted after test <strong>to1</strong> and before test <strong>to2</strong>.</p><p><em>Example 2:</em></p><pre>
 $ ct_run -dir $TEST_ROOT/to1 $TEST_ROOT/to2 -duration 001000 -forces_stop skip_rest</pre><p>Here, the same tests as in Example 1 are run, but with flag <strong>force_stop</strong> set to 
<strong>skip_rest</strong>. If time-out occurs while executing tests in directory <strong>to1</strong>, 
the remaining test cases in <strong>to1</strong> are skipped and the test is aborted without 
running the tests in <strong>to2</strong> another time. If time-out occurs while executing 
tests in directory <strong>to2</strong>, the remaining test cases in <strong>to2</strong> are skipped and 
the test is aborted.</p><p><em>Example 3:</em></p><pre>
 $ date
 Fri Sep 28 15:00:00 MEST 2007

 $ ct_run -dir $TEST_ROOT/to1 $TEST_ROOT/to2 -until 160000</pre><p>Here, the same test run as in the previous examples are executed (and possibly repeated). 
However, when the time-out occurs, after 1 hour, <strong>Common Test</strong> finishes the entire 
test run before stopping (that is, both <strong>to1</strong> and <strong>to2</strong> are always executed in 
the same test run).</p><p><em>Example 4:</em></p><pre>
 $ ct_run -dir $TEST_ROOT/to1 $TEST_ROOT/to2 -repeat 5</pre><p>Here, the test run, including both the <strong>to1</strong> and the <strong>to2</strong> test, is repeated 
five times.</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>Do not confuse this feature with the <strong>repeat</strong> property of a test
case group. The options described here are used to repeat execution of entire test runs,
while the <strong>repeat</strong> property of a test case group makes it possible to repeat
execution of sets of test cases within a suite. For more information about the latter,
see section <a href="./write_test_chapter#test_case_groups">Test Case Groups </a>
in section Writing Test Suites.</p></div><a name="silent_connections"></a><h4>Silent Connections</h4><p>The protocol handling processes in <strong>Common Test</strong>, implemented by <strong>ct_telnet</strong>,
<strong>ct_ssh</strong>, <strong>ct_ftp</strong>, and so on, do verbose printing to the test case logs. 
This can be switched off with flag <strong>-silent_connections</strong>:</p><pre>
 ct_run -silent_connections [conn_types]</pre><p>Here, <strong>conn_types</strong> specifies SSH, Telnet, FTP, RPC, and/or SNMP.</p><p><em>Example 1:</em></p><pre>
 ct_run ... -silent_connections ssh telnet</pre><p>This switches off logging for SSH and Telnet connections.</p><p><em>Example 2:</em></p><pre>
 ct_run ... -silent_connections</pre><p>This switches off logging for all connection types.</p><p>Fatal communication error and reconnection attempts are always printed, even if 
logging has been suppressed for the connection type in question. However, operations
such as sending and receiving data are performed silently.</p><p><strong>silent_connections</strong> can also be specified in a test suite. This is
accomplished by returning a tuple, <strong>{silent_connections,ConnTypes}</strong>, in the
<strong>suite/0</strong> or test case information list. If <strong>ConnTypes</strong> is a list of atoms 
(SSH, Telnet, FTP, RPC and/or SNMP), output for any corresponding connections 
are suppressed. Full logging is by default enabled for any connection of type not 
specified in <strong>ConnTypes</strong>. Hence, if <strong>ConnTypes</strong> is the empty list, logging 
is enabled for all connections.</p><p><em>Example 3:</em></p><pre>
 -module(my_SUITE).

 suite() -&gt; [..., {silent_connections,[telnet,ssh]}, ...].

 ...

 my_testcase1() -&gt;
     [{silent_connections,[ssh]}].

 my_testcase1(_) -&gt;
     ...

 my_testcase2(_) -&gt;
     ...</pre><p>In this example, <strong>suite/0</strong> tells <strong>Common Test</strong> to suppress
printouts from Telnet and SSH connections. This is valid for
all test cases. However, <strong>my_testcase1/0</strong> specifies that
for this test case, only SSH is to be silent. The result is
that <strong>my_testcase1</strong> gets Telnet information (if any) printed
in the log, but not SSH information. <strong>my_testcase2</strong> gets no
information from either connection printed.</p><p><strong>silent_connections</strong> can also be specified with a term
in a test specification
(see section <a href="./run_test_chapter#test_specifications">Test Specifications</a> in section Running Tests and Analyzing Results).
Connections provided with start	flag/option <strong>silent_connections</strong>
are merged with any connections listed in the test specification.</p><p>Start flag/option <strong>silent_connections</strong> and the test
specification term override any settings made by the information functions
inside the test suite.</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>In the current <strong>Common Test</strong> version, the
<strong>silent_connections</strong> feature only works for Telnet
and SSH connections. Support for other connection types can be added
in future <strong>Common Test</strong> versions.</p></div><a name="top"></a><h4>General</h4><p>To avoid hard-coding data values related to the test and/or System
Under Test (SUT) in the test suites, the data can instead be specified through
configuration files or strings that <strong>Common Test</strong> reads before
the start of a test run. External configuration data makes it possible to
change test properties without modifying the test suites
using the data. Examples of configuration data follows:</p><ul><li>Addresses to the test plant or other instruments</li><li>User login information</li><li>Names of files needed by the test</li><li>Names of programs to be executed during the test</li><li>Any other variable needed by the test</li></ul><h4>Syntax</h4><p>A configuration file can contain any number of elements of the type:</p><pre>
 {CfgVarName,Value}.</pre><p>where</p><pre>
 CfgVarName = atom()
 Value = term() | [{CfgVarName,Value}]</pre><h4>Requiring and Reading Configuration Data</h4><a name="require_config_data"></a><p>In a test suite, one must <em>require</em> that a configuration 
variable (<strong>CfgVarName</strong> in the previous definition) exists before
attempting to read the associated value in a test case or configuration function.</p><p><strong>require</strong> is an assert statement, which can be part of the <a href="./write_test_chapter#suite">Test Suite Information Function</a> or
<a href="./write_test_chapter#info_function">Test Case Information Function</a>. If the required variable is unavailable, the
test is skipped (unless a default value has been specified, see section
<a href="./write_test_chapter#info_function">Test Case Information Function</a> for details). Also, function
<a href="./ct#require-1">ct#require-1</a> can be called 
from a test case to check if a specific variable is available. The return 
value from this function must be checked explicitly and appropriate 
action be taken depending on the result (for example, to skip the test case
if the variable in question does not exist).</p><p>A <strong>require</strong> statement in the test suite information case or test case 
information-list is to look like
<strong>{require,CfgVarName}</strong> or <strong>{require,AliasName,CfgVarName}</strong>.
The arguments <strong>AliasName</strong> and <strong>CfgVarName</strong> are the same as the
arguments to <a href="./ct#require-1">ct#require-1</a>. 
<strong>AliasName</strong> becomes an alias for the configuration variable,
and can be used as reference to the configuration data value.
The configuration variable can be associated with any
number of alias names, but each name must be unique within
the same test suite. The two main uses for alias names follows:</p><ul><li>To identify connections (described later).</li><li>To help adapt configuration data to a test suite  (or test case) and improve readability.</li></ul><p>To read the value of a configuration variable, use function
<a href="./ct#get_config-1">ct#get_config-1</a>.
</p><p><em>Example:</em></p><pre>
 suite() -&gt; 
     [{require, domain, 'CONN_SPEC_DNS_SUFFIX'}].

 ...

 testcase(Config) -&gt;
     Domain = ct:get_config(domain),
     ...</pre><h4>Using Configuration Variables Defined in Multiple Files</h4><p>If a configuration variable is defined in multiple files and you 
want to access all possible values, use function
<a href="./ct#get_config-3">ct#get_config-3</a>
and specify <strong>all</strong> in the options list. The values are then
returned in a list and the order of the elements corresponds to the order 
that the configuration files were specified at startup.</p><h4>Encrypted Configuration Files</h4><a name="encrypted_config_files"></a><p>Configuration files containing sensitive data can be encrypted 
if they must be stored in open and shared directories.</p><p>To have <strong>Common Test</strong> encrypt a
specified file using function <strong>DES3</strong> in application <strong>Crypto</strong>,
call <a href="./ct#encrypt_config_file-2">ct#encrypt_config_file-2</a>
The encrypted file can then be used as a regular configuration file
in combination with other encrypted files or normal text files. However, the 
key for decrypting the configuration file must be provided when running the test.
This can be done with flag/option <strong>decrypt_key</strong> or
<strong>decrypt_file</strong>, or a key file in a predefined location.</p><p><strong>Common Test</strong> also provides decryption functions, 
<a href="./ct#decrypt_config_file-2">ct#decrypt_config_file-2</a>, 
for recreating the original text files.</p><h4>Opening Connections Using Configuration Data</h4><p>Two different methods for opening a connection using the support functions 
in, for example, <a href="ct_ssh">ct_ssh</a>, 
<a href="ct_ftp">ct_ftp</a>, and 
<a href="ct_telnet">ct_telnet</a> follows:</p><ul><li>Using a configuration target name (an alias) as reference.</li><li>Using the configuration variable as reference.</li></ul><p>When a target name is used for referencing the configuration data
(that specifies the connection to be opened), the same name can be used 
as connection identity in all subsequent calls related to the connection
(also for closing it). Only one open connection per target name 
is possible. If you attempt to open a new connection using a name
already associated with an open connection, <strong>Common Test</strong>
returns the already existing handle so the previously opened connection
is used. This feature makes it possible to
call the function for opening a particular connection whenever 
useful. An action like this does not necessarily open any new 
connections unless it is required (which could be the case if, for example,
the previous connection has been closed unexpectedly by the server).
Using named connections also removes the need to pass handle references 
around in the suite for these connections.
</p><p>When a configuration variable name is used as reference to the data
specifying the connection, the handle returned as a result of opening
the connection must be used in all subsequent calls (also for closing
the connection). Repeated calls to the open function with the same
variable name as reference results in multiple connections being opened. 
This can be useful, for example, if a test case needs to open
multiple connections to the same server on the target node (using the
same configuration data for each connection).
</p><h4>User-Specific Configuration Data Formats</h4><p>The user can specify configuration data on a
different format than key-value tuples in a text file, as described
so far. The data can, for example, be read from any files, fetched from
the web over HTTP, or requested from a user-specific process.
To support this, <strong>Common Test</strong> provides a callback module plugin
mechanism to handle configuration data.</p><h4>Default Callback Modules for Handling Configuration Data</h4><p><strong>Common Test</strong> includes default callback modules
for handling configuration data specified in standard configuration files
(described earlier) and in XML files as follows:</p><ul><li> <strong>ct_config_plain</strong> - for reading configuration files with key-value tuples (standard format). This handler is used to parse configuration files if no user callback is specified. </li><li> <strong>ct_config_xml</strong> - for reading configuration data from XML files. </li></ul><h4>Using XML Configuration Files</h4><p>An example of an XML configuration file follows:</p><pre>
 
 &lt;config&gt;
    &lt;ftp_host&gt;
        &lt;ftp&gt;"targethost"&lt;/ftp&gt;
        &lt;username&gt;"tester"&lt;/username&gt;
        &lt;password&gt;"letmein"&lt;/password&gt;
    &lt;/ftp_host&gt;
    &lt;lm_directory&gt;"/test/loadmodules"&lt;/lm_directory&gt;
 &lt;/config&gt;</pre><p>Once read, this file produces the same configuration
variables as the following text file:</p><pre>
 {ftp_host, [{ftp,"targethost"},
             {username,"tester"},
             {password,"letmein"}]}.

 {lm_directory, "/test/loadmodules"}.</pre><h4>Implement a User-Specific Handler</h4><p>The user-specific handler can be written to handle special
configuration file formats. The parameter can be either file
names or configuration strings (the empty list is valid).</p><p>The callback module implementing the handler is responsible for
checking the correctness of configuration strings.</p><p>To validate the configuration strings, the callback module
is to have function <strong>Callback:check_parameter/1</strong> exported.</p><p>The input argument is passed from <strong>Common Test</strong>, as defined in the test
specification, or specified as an option to <strong>ct_run</strong> or <strong>ct:run_test</strong>.</p><p>The return value is to be any of the following values, indicating if the specified
configuration parameter is valid:</p><ul><li> <strong>{ok, {file, FileName}}</strong> - the parameter is a file name and the file exists. </li><li> <strong>{ok, {config, ConfigString}}</strong> - the parameter is a configuration string and it is correct. </li><li> <strong>{error, {nofile, FileName}}</strong> - there is no file with the specified name in the current directory. </li><li> <strong>{error, {wrong_config, ConfigString}}</strong> - the configuration string is wrong. </li></ul><p>The function <strong>Callback:read_config/1</strong> is to be exported from the 
callback module to read configuration data, initially before the tests
start, or as a result of data being reloaded during test execution.
The input argument is the same as for function <strong>check_parameter/1</strong>.</p><p>The return value is to be either of the following:</p><ul><li> <strong>{ok, Config}</strong> - if the configuration variables are read successfully. </li><li> <strong>{error, {Error, ErrorDetails}}</strong> - if the callback module fails to proceed with the specified configuration parameters. </li></ul><p><strong>Config</strong> is the proper Erlang key-value list, with possible
key-value sublists as values, like the earlier configuration file
example:</p><pre>
 [{ftp_host, [{ftp, "targethost"}, {username, "tester"}, {password, "letmein"}]},
  {lm_directory, "/test/loadmodules"}]</pre><h4>Examples of Configuration Data Handling</h4><p>A configuration file for using the FTP client to access files on a remote
host can look as follows:</p><pre>
 {ftp_host, [{ftp,"targethost"},
	     {username,"tester"},
	     {password,"letmein"}]}.

 {lm_directory, "/test/loadmodules"}.</pre><p>The XML version shown earlier can also be used, but it is to be
explicitly specified that the <strong>ct_config_xml</strong> callback module is to be
used by <strong>Common Test</strong>.</p><p>The following is an example of how to assert that the configuration data is available
and can be used for an FTP session:</p><pre>
 init_per_testcase(ftptest, Config) -&gt;
     {ok,_} = ct_ftp:open(ftp),
     Config.

 end_per_testcase(ftptest, _Config) -&gt;
     ct_ftp:close(ftp).

 ftptest() -&gt;
     [{require,ftp,ftp_host},
      {require,lm_directory}].

 ftptest(Config) -&gt;
     Remote = filename:join(ct:get_config(lm_directory), "loadmodX"),
     Local = filename:join(?config(priv_dir,Config), "loadmodule"),
     ok = ct_ftp:recv(ftp, Remote, Local),
     ...</pre><p>The following is an example of how the functions in the previous example 
can be rewritten if it is necessary to open multiple connections to the 
FTP server:</p><pre>
 init_per_testcase(ftptest, Config) -&gt;
     {ok,Handle1} = ct_ftp:open(ftp_host),
     {ok,Handle2} = ct_ftp:open(ftp_host),
     [{ftp_handles,[Handle1,Handle2]} | Config].

 end_per_testcase(ftptest, Config) -&gt;
     lists:foreach(fun(Handle) -&gt; ct_ftp:close(Handle) end, 
                   ?config(ftp_handles,Config)).

 ftptest() -&gt;
     [{require,ftp_host},
      {require,lm_directory}].

 ftptest(Config) -&gt;
     Remote = filename:join(ct:get_config(lm_directory), "loadmodX"),
     Local = filename:join(?config(priv_dir,Config), "loadmodule"),
     [Handle | MoreHandles] = ?config(ftp_handles,Config),
     ok = ct_ftp:recv(Handle, Remote, Local),
     ...</pre><h4>Example of User-Specific Configuration Handler</h4><p>A simple configuration handling driver, asking an external server for
configuration data, can be implemented as follows:</p><pre>
 -module(config_driver).
 -export([read_config/1, check_parameter/1]).

 read_config(ServerName)-&gt;
     ServerModule = list_to_atom(ServerName),
     ServerModule:start(),
     ServerModule:get_config().

 check_parameter(ServerName)-&gt;
     ServerModule = list_to_atom(ServerName),
     case code:is_loaded(ServerModule) of
         {file, _}-&gt;
             {ok, {config, ServerName}};
         false-&gt;
             case code:load_file(ServerModule) of
                 {module, ServerModule}-&gt;
                     {ok, {config, ServerName}};
                 {error, nofile}-&gt;
                     {error, {wrong_config, "File not found: " ++ ServerName ++ ".beam"}}
             end
     end.</pre><p>The configuration string for this driver can be <strong>config_server</strong>, if the
<strong>config_server.erl</strong> module that follows is compiled and exists in the code path
during test execution:</p><pre>
 -module(config_server).
 -export([start/0, stop/0, init/1, get_config/0, loop/0]).

 -define(REGISTERED_NAME, ct_test_config_server).

 start()-&gt;
     case whereis(?REGISTERED_NAME) of
         undefined-&gt;
             spawn(?MODULE, init, [?REGISTERED_NAME]),
             wait();
         _Pid-&gt;
         ok
     end,
     ?REGISTERED_NAME.

 init(Name)-&gt;
     register(Name, self()),
     loop().

 get_config()-&gt;
     call(self(), get_config).

 stop()-&gt;
     call(self(), stop).

 call(Client, Request)-&gt;
     case whereis(?REGISTERED_NAME) of
         undefined-&gt;
             {error, {not_started, Request}};
         Pid-&gt;
             Pid ! {Client, Request},
             receive
                 Reply-&gt;
                     {ok, Reply}
             after 4000-&gt;
                 {error, {timeout, Request}}
             end
     end.

 loop()-&gt;
     receive
         {Pid, stop}-&gt;
             Pid ! ok;
         {Pid, get_config}-&gt;
             {D,T} = erlang:localtime(),
             Pid !
                 [{localtime, [{date, D}, {time, T}]},
                  {node, erlang:node()},
                  {now, erlang:now()},
                  {config_server_pid, self()},
                  {config_server_vsn, ?vsn}],
             ?MODULE:loop()
     end.

 wait()-&gt;
     case whereis(?REGISTERED_NAME) of
         undefined-&gt;
             wait();
         _Pid-&gt;
             ok
     end.</pre><p>Here, the handler also provides for dynamically reloading of
configuration variables. If 
<a href="./ct#reload_config-1">ct#reload_config-1</a> is called from
the test case function, all variables loaded with <strong>config_driver:read_config/1</strong>
are updated with their latest values, and the new value for variable
<strong>localtime</strong> is returned.</p><a name="cover"></a><h4>General</h4><p>Although <strong>Common Test</strong> was created primarily for
black-box testing, nothing prevents it from working perfectly as
a white-box testing tool as well. This is especially true when
the application to test is written in Erlang. Then the test
ports are easily realized with Erlang function calls.</p><p>When white-box testing an Erlang application, it is useful to
be able to measure the code coverage of the test. <strong>Common Test</strong>
provides simple access to the OTP Cover tool for this
purpose. <strong>Common Test</strong> handles all necessary communication with
the Cover tool (starting, compiling, analysing, and so on).
The <strong>Common Test</strong> user only needs to specify the extent of the
code coverage analysis.</p><h4>Use</h4><p>To specify the modules to be included in the code coverage test, 
provide a cover specification file. With this file you can point 
out specific modules or specify directories containing modules to be
included in the analysis. You can also specify modules to be excluded 
from the analysis.</p><p>If you are testing a distributed Erlang application, it is
likely that code you want included in the code coverage analysis
gets executed on another Erlang node than the one <strong>Common Test</strong>
is running on. If so, you must specify these other nodes in the 
cover specification file or add them dynamically to the code coverage 
set of nodes. For details on the latter, see module 
<a href="ct_cover">ct_cover</a>.</p><p>In the cover specification file you can also specify your
required level of the code coverage analysis; <strong>details</strong> or
<strong>overview</strong>. In detailed mode, you get a coverage overview
page, showing per module and total coverage percentages.
You also get an HTML file printed for each module included in the
analysis showing exactly what parts of the code have been
executed during the test. In overview mode, only the code
coverage overview page is printed.</p><p>You can choose to export and import code coverage data between
tests. If you specify the name of an export file in the cover
specification file, <strong>Common Test</strong> exports collected coverage
data to this file at the end of the test. You can similarly
specify previously exported data to be imported and
included in the analysis for a test (multiple import files can be specified). 
This way, the total code coverage can be analyzed without necessarily 
running all tests at once.</p><p>To activate the code coverage support, specify the name of the cover 
specification file as you start <strong>Common Test</strong>.
Do this by using flag <strong>-cover</strong> with 
<a href="ct_run">ct_run</a>, 
for example:</p><pre>
 $ ct_run -dir $TESTOBJS/db -cover $TESTOBJS/db/config/db.coverspec</pre><p>You can also pass the cover specification file name in a
call to <a href="./ct#run_test-1">ct#run_test-1</a>, 
by adding a <strong>{cover,CoverSpec}</strong> tuple to argument <strong>Opts</strong>.</p><p>You can also enable code coverage in your test specifications (see section 
<a href="./run_test_chapter#test_specifications">Test Specifications</a>
in section Running Tests and Analyzing Results).</p><a name="cover_stop"></a><h4>Stopping the Cover Tool When Tests Are Completed</h4><p>By default, the Cover tool is automatically stopped when the
tests are completed. This causes the original (non-cover
compiled) modules to be loaded back into the test node. If a
process at this point still runs old code of any of the
modules that are cover compiled, meaning that it has not done
any fully qualified function call after the cover compilation,
the process is killed. To avoid this, set the value of option 
<strong>cover_stop</strong> to <strong>false</strong>. This means that the 
modules stay cover compiled. Therefore, this is only recommended 
if the Erlang nodes under test are terminated after the test is 
completed, or if cover can be manually stopped.</p><p>The option can be set by using flag <strong>-cover_stop</strong> with
<strong>ct_run</strong>, by adding <strong>{cover_stop,true|false}</strong> to argument
<strong>Opts</strong> to 
<a href="./ct#run_test-1">ct#run_test-1</a>, 
or by adding a <strong>cover_stop</strong> term in the test specification (see section
<a href="./run_test_chapter#test_specifications">Test Specifications</a>
in section Running Tests and Analyzing Results).</p><h4>The Cover Specification File</h4><p>The following terms are allowed in a cover specification file:</p><pre>
 %% List of Nodes on which cover will be active during test.
 %% Nodes = [atom()]
 {nodes, Nodes}.       

 %% Files with previously exported cover data to include in analysis.
 %% CoverDataFiles = [string()]
 {import, CoverDataFiles}.

 %% Cover data file to export from this session.
 %% CoverDataFile = string()
 {export, CoverDataFile}.

 %% Cover analysis level.
 %% Level = details | overview
 {level, Level}.       

 %% Directories to include in cover.
 %% Dirs = [string()]
 {incl_dirs, Dirs}.

 %% Directories, including subdirectories, to include.
 {incl_dirs_r, Dirs}.

 %% Specific modules to include in cover.
 %% Mods = [atom()]
 {incl_mods, Mods}.

 %% Directories to exclude in cover.
 {excl_dirs, Dirs}.

 %% Directories, including subdirectories, to exclude.
 {excl_dirs_r, Dirs}.

 %% Specific modules to exclude in cover.
 {excl_mods, Mods}.

 %% Cross cover compilation
 %% Tag = atom(), an identifier for a test run
 %% Mod = [atom()], modules to compile for accumulated analysis
 {cross,[{Tag,Mods}]}.</pre><p>The terms <strong>incl_dirs_r</strong> and <strong>excl_dirs_r</strong> tell <strong>Common Test</strong> to search the specified directories recursively and include 
or exclude any module found during the search. The terms
<strong>incl_dirs</strong> and <strong>excl_dirs</strong> result in a
non-recursive search for modules (that is, only modules found in 
the specified directories are included or excluded).</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>Directories containing Erlang modules to be included in a code 
coverage test must exist in the code server path. Otherwise, 
the Cover tool fails to recompile the modules. It is not sufficient to 
specify these directories in the cover specification file for 
<strong>Common Test</strong>.</p></div><a name="cross_cover"></a><h4>Cross Cover Analysis</h4><p>The cross cover mechanism allows cover analysis of modules
across multiple tests. It is useful if some code, for example, a 
library module, is used by many different tests and the accumulated 
cover result is desirable.</p><p>This can also be achieved in a more customized way by
using parameter <strong>export</strong> in the cover specification and
analysing the result off line. However, the cross cover mechanism is a
built-in solution that also provides logging.</p><p>The mechanism is easiest explained by an example:</p><p>Assume that there are two systems, <strong>s1</strong> and <strong>s2</strong>,
that are tested in separate test runs. System <strong>s1</strong> contains
a library module <strong>m1</strong> tested by test run <strong>s1</strong> and 
is included in the cover specification of <strong>s1</strong> as follows:</p><pre><code class="">
 s1.cover:
   {incl_mods,[m1]}.</code></pre><p>When analysing code coverage, the result for <strong>m1</strong> can be
seen in the cover log in the <strong>s1</strong> test result.</p><p>Now, imagine that as <strong>m1</strong> is a library module, it
is also often used by system <strong>s2</strong>. Test run <strong>s2</strong>
does not specifically test <strong>m1</strong>, but it can still be
interesting to see which parts of <strong>m1</strong> that are covered 
by the <strong>s2</strong> tests. To do this, <strong>m1</strong> can be included also
in the cover specification of <strong>s2</strong> as follows:</p><pre><code class="">
 s2.cover:
   {incl_mods,[m1]}.</code></pre><p>This gives an entry for <strong>m1</strong> also in the cover log
for test run <strong>s2</strong>. The problem is that this only
reflects the coverage by <strong>s2</strong> tests, not the accumulated
result over <strong>s1</strong> and <strong>s2</strong>. This is where the cross
cover mechanism comes in handy.</p><p>If instead the cover specification for <strong>s2</strong> is like
the following:</p><pre><code class="">
 s2.cover:
   {cross,[{s1,[m1]}]}.</code></pre><p>Then <strong>m1</strong> is cover compiled in test run <strong>s2</strong>,
but not shown in the coverage log. Instead, if
<a href="./ct_cover#cross_cover_analyse-2">ct_cover#cross_cover_analyse-2</a> 
is called after both <strong>s1</strong> and <strong>s2</strong> test runs are completed, 
the accumulated result for <strong>m1</strong> is available in the cross cover 
log for test run <strong>s1</strong>.</p><p>The call to the analyze function must be as follows:</p><pre><code class="">
 ct_cover:cross_cover_analyse(Level, [{s1,S1LogDir},{s2,S2LogDir}]).</code></pre><p>Here, <strong>S1LogDir</strong> and <strong>S2LogDir</strong> are the directories
named <strong>&lt;TestName&gt;.logs</strong> for each test respectively.</p><p>Notice the tags <strong>s1</strong> and <strong>s2</strong>, which are used in the
cover specification file and in the call to
<strong>ct_cover:cross_cover_analyse/2</strong>. The purpose of these is only
to map the modules specified in the cover specification to the log
directory specified in the call to the analyze function. The tag name
has no meaning beyond this.</p><h4>Logging</h4><p>To view the result of a code coverage test, click the button
labeled "COVER LOG" in the top-level index page for the test run.</p><p>Before Erlang/OTP 17.1, if your test run consisted of
multiple tests, cover would be started and stopped for each test
within the test run. Separate logs would be available through the
"Coverage log" link on the test suite result pages. These links
are still available, but now they all point to the same page as
the button on the top-level index page. The log contains the
accumulated results for the complete test run. For details about 
this change, see the release notes.</p><p>The button takes you to the code coverage overview page. If you
have successfully performed a detailed coverage analysis,
links to each individual module coverage page are found here.</p><p>If cross cover analysis is performed, and there are
accumulated coverage results for the current test, the link
"Coverdata collected over all tests" takes you to these
results.</p><a name="general"></a><h4>General</h4><p>Large-scale automated testing requires running multiple independent 
test sessions in parallel. This is accomplished by running
some <strong>Common Test</strong> nodes on one or more hosts, testing
different target systems. Configuring, starting, and controlling the
test nodes independently can be a cumbersome operation. To aid
this kind of automated large-scale testing, <strong>Common Test</strong> offers a master 
test node component, <strong>Common Test</strong> Master, which handles central configuration and control
in a system of distributed <strong>Common Test</strong> nodes.</p><p>The <strong>Common Test</strong> Master server runs on one dedicated Erlang node and uses distributed
Erlang to communicate with any number of <strong>Common Test</strong> test nodes, each hosting a regular
<strong>Common Test</strong> server. Test specifications are used as input to specify what to test on which 
test nodes, using what configuration.</p><p>The <strong>Common Test</strong> Master server writes progress information to HTML log files similarly 
to the regular <strong>Common Test</strong> server. The logs contain test statistics and links to the 
log files written by each independent <strong>Common Test</strong> server.</p><p>The <strong>Common Test</strong> Master API is exported by module 
<a href="ct_master">ct_master</a>.</p><h4>Use</h4><p><strong>Common Test</strong> Master requires all test nodes to be on the same network and share a common 
file system. <strong>Common Test</strong> Master cannot start test nodes
automatically. The nodes must be started in advance for <strong>Common Test</strong> Master to be 
able to start test sessions on them.</p><p>Tests are started by calling 
<a href="./ct_master#run-1">ct_master#run-1</a> or 
<a href="./ct_master#run-3">ct_master#run-3</a></p><p><strong>TestSpecs</strong> is either the name of a test specification file (string) or a list 
of test specifications. If it is a list, the specifications are handled (and
the corresponding tests executed) in sequence. An element in a <strong>TestSpecs</strong> list 
can also be list of test specifications. The specifications in such a list are 
merged into one combined specification before test execution.</p><p><em>Example:</em></p><pre>
 ct_master:run(["ts1","ts2",["ts3","ts4"]])</pre><p>Here, the tests specified by "ts1" run first, then the tests specified by "ts2",
and finally the tests specified by both "ts3" and "ts4".</p><p>The <strong>InclNodes</strong> argument to <strong>run/3</strong> is a list of node names. Function
<strong>run/3</strong> runs the tests in <strong>TestSpecs</strong> just like <strong>run/1</strong>, but also 
takes any test in <strong>TestSpecs</strong>, which is not explicitly tagged with a particular 
node name, and execute it on the nodes listed in <strong>InclNodes</strong>. By using <strong>run/3</strong> 
this way, any test specification can be used, with or without node information, 
in a large-scale test environment.</p><p><strong>ExclNodes</strong> is a list of nodes to be
excluded from the test. That is, tests that are specified in the test specification 
to run on a particular node are not performed if that node is 
listed in <strong>ExclNodes</strong> at runtime.</p><p>If <strong>Common Test</strong> Master fails initially to connect to any of the test nodes specified in a 
test specification or in the <strong>InclNodes</strong> list, the operator is prompted with 
the option to either start over again (after manually checking the status of the 
nodes in question), to run without the missing nodes, or to abort the operation.</p><p>When tests start, <strong>Common Test</strong> Master displays information to console about the involved nodes.  
<strong>Common Test</strong> Master also reports when tests finish, successfully or unsuccessfully. If
connection is lost to a node, the test on that node is considered finished. <strong>Common Test</strong> Master 
does not attempt to re-establish contact with the failing node.</p><p>At any time, to get the current status of the test nodes, call function 
<a href="./ct_master#progress-0">ct_master#progress-0</a>.</p><p>To stop one or more tests, use function
<a href="./ct_master#abort-0">ct_master#abort-0</a> (to stop all) or
<a href="./ct_master#abort-1">ct_master#abort-1</a>.</p><p>For details about the <strong>Common Test</strong> Master API, see module
<a href="ct_master">ct_master</a>.</p><a name="test_specifications"></a><h4>Test Specifications</h4><p>The test specifications used as input to <strong>Common Test</strong> Master are fully compatible with the
specifications used as input to the regular <strong>Common Test</strong> server. The syntax is described in section 
<a href="./run_test_chapter#test_specifications">Test Specifications</a>
in section Running Tests and Analyzing Results.</p><p>All test specification terms can have a <strong>NodeRefs</strong> element. This element
specifies which node or nodes a configuration operation or a test is to be executed 
on. <strong>NodeRefs</strong> is defined as follows:</p><p><strong>NodeRefs = all_nodes | [NodeRef] | NodeRef</strong></p><p><strong>NodeRef = NodeAlias | node() | master</strong></p><p>A <strong>NodeAlias</strong> (<strong>atom()</strong>) is used in a test specification as a 
reference to a node name (so the node name only needs to be declared once,
which also can be achieved using constants). 
The alias is declared with a <strong>node</strong> term as follows:</p><p><strong>{node, NodeAlias, NodeName}</strong></p><p>If <strong>NodeRefs</strong> has the value <strong>all_nodes</strong>, the operation or test
is performed on all specified test nodes. (Declaring a term without a <strong>NodeRefs</strong> 
element has the same effect). If <strong>NodeRefs</strong> has the value 
<strong>master</strong>, the operation is only performed on the <strong>Common Test</strong> Master node (namely set 
the log directory or install an event handler).</p><p>Consider the example in section 
<a href="./run_test_chapter#test_specifications">Test Specifications</a>
in section Running Tests and Analysing Results,
now extended with node information and intended to be executed by
<strong>Common Test</strong> Master:</p><pre>
 {define, 'Top', "/home/test"}.
 {define, 'T1', "'Top'/t1"}.
 {define, 'T2', "'Top'/t2"}.
 {define, 'T3', "'Top'/t3"}.
 {define, 'CfgFile', "config.cfg"}.
 {define, 'Node', ct_node}.

 {node, node1, 'Node@host_x'}.
 {node, node2, 'Node@host_y'}.

 {logdir, master, "'Top'/master_logs"}.
 {logdir, "'Top'/logs"}.

 {config, node1, "'T1'/'CfgFile'"}.
 {config, node2, "'T2'/'CfgFile'"}.
 {config, "'T3'/'CfgFile'"}.

 {suites, node1, 'T1', all}.
 {skip_suites, node1, 'T1', [t1B_SUITE,t1D_SUITE], "Not implemented"}.
 {skip_cases, node1, 'T1', t1A_SUITE, [test3,test4], "Irrelevant"}.
 {skip_cases, node1, 'T1', t1C_SUITE, [test1], "Ignore"}.

 {suites, node2, 'T2', [t2B_SUITE,t2C_SUITE]}.
 {cases, node2, 'T2', t2A_SUITE, [test4,test1,test7]}.

 {skip_suites, 'T3', all, "Not implemented"}.</pre><p>This example specifies the same tests as the original example. But 
now if started with a call to <strong>ct_master:run(TestSpecName)</strong>, test 
<strong>t1</strong> is executed on node <strong>ct_node@host_x</strong> (<strong>node1</strong>), test
<strong>t2</strong> on <strong>ct_node@host_y</strong> (<strong>node2</strong>) and test <strong>t3</strong>
on both <strong>node1</strong> and <strong>node2</strong>. Configuration file <strong>t1</strong> is only read on
<strong>node1</strong> and configuration file <strong>t2</strong> only on <strong>node2</strong>, while the 
configuration file <strong>t3</strong> is read on both <strong>node1</strong> and <strong>node2</strong>. 
Both test nodes write log files to the same directory. (However, the <strong>Common Test</strong> Master 
node uses a different log directory than the test nodes.)</p><p>If the test session is instead started with a call to 
<strong>ct_master:run(TestSpecName, [ct_node@host_z], [ct_node@host_x])</strong>, 
the result is that test <strong>t1</strong> does not run on 
<strong>ct_node@host_x</strong> (or any other node) while test <strong>t3</strong> runs on both
<strong>ct_node@host_y</strong> and <strong>ct_node@host_z</strong>.</p><p>A nice feature is that a test specification that includes node 
information can still be used as input to the regular <strong>Common Test</strong> server 
(as described in section
<a href="./run_test_chapter#test_specifications">Test Specifications</a>). 
The result is that any test specified to run on a node with the same
name as the <strong>Common Test</strong> node in question (typically <strong>ct@somehost</strong> if started
with the <strong>ct_run</strong> program), is performed. Tests without explicit
node association are always performed too, of course.</p><h4>Automatic Startup of Test Target Nodes</h4><a name="ct_slave"></a><p>Initial actions can be started and performed automatically on
test target nodes using test specification term <strong>init</strong>.</p><p>Two subterms are supported, <strong>node_start</strong> and <strong>eval</strong>.</p><p><em>Example:</em></p><pre>
 {node, node1, node1@host1}.
 {node, node2, node1@host2}.
 {node, node3, node2@host2}.
 {node, node4, node1@host3}.
 {init, node1, [{node_start, [{callback_module, my_slave_callback}]}]}.
 {init, [node2, node3], {node_start, [{username, "ct_user"}, {password, "ct_password"}]}}.
 {init, node4, {eval, {module, function, []}}}.</pre><p>This test specification declares that <strong>node1@host1</strong> is to be started using
the user callback function <strong>callback_module:my_slave_callback/0</strong>, and nodes
<strong>node1@host2</strong> and <strong>node2@host2</strong> are to be started with the default callback
module <strong>ct_slave</strong>. The specified username and password are used to log on to remote
host <strong>host2</strong>. Also, function <strong>module:function/0</strong> is evaluated on
<strong>node1@host3</strong>, and the result of this call is printed to the log.</p><p>The default callback module <a href="ct_slave">ct_slave</a>,
has the following features:
</p><ul><li>Starting Erlang target nodes on local or remote hosts (application <strong>SSH</strong> is used for communication). </li><li>Ability to start an Erlang emulator with more flags (any flags supported by <strong>erl</strong> are supported). </li><li>Supervision of a node being started using internal callback functions. Used to prevent hanging nodes. (Configurable.) </li><li>Monitoring of the master node by the slaves. A slave node can be stopped if the master node terminates. (Configurable.) </li><li>Execution of user functions after a slave node is started. Functions can  be specified as a list of <strong>{Module, Function, Arguments}</strong> tuples. </li></ul><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>An <strong>eval</strong> term for the node and
<strong>startup_functions</strong> in the <strong>node_start</strong> options list can be specified. 
In this case, the node is started first, then the <strong>startup_functions</strong> are
executed, and finally functions specified with <strong>eval</strong> are called.
</p></div><a name="event_handling"></a><h4>General</h4><p>The operator of a <strong>Common Test</strong> system can receive
event notifications continuously during a test run. For example, 
<strong>Common Test</strong> reports when a test case starts and stops, 
the current count of successful, failed, and skipped cases, and so on. 
This information can be used for different purposes such as logging progress 
and results in another format than HTML, saving statistics to a database 
for report generation, and test system supervision.</p><p><strong>Common Test</strong> has a framework for event handling based on
the OTP event manager concept and <strong>gen_event</strong> behavior. 
When the <strong>Common Test</strong> server starts, it spawns an event manager. 
During test execution the manager gets a notification from the server 
when something of potential interest happens. Any event handler plugged into 
the event manager can match on events of interest, take action, or
pass the information on. The event handlers are Erlang modules
implemented by the <strong>Common Test</strong> user according to the <strong>gen_event</strong> 
behavior (for details, see module
<a href="./gen_event">stdlib/gen_event</a> and
section
<a href="./events">doc/design_principles/events</a>
in OTP Design Principles in the System Documentation).
</p><p>A <strong>Common Test</strong> server always starts an event manager. 
The server also plugs in a default event handler, which only
purpose is to relay notifications to a globally registered <strong>Common Test</strong> 
Master event manager (if a <strong>Common Test</strong> Master server is running in the system). 
The <strong>Common Test</strong> Master also spawns an event manager at startup.
Event handlers plugged into this manager receives the events from 
all the test nodes, plus information from the <strong>Common Test</strong> Master server.
</p><p>User-specific event handlers can be plugged into a <strong>Common Test</strong> event
manager, either by telling <strong>Common Test</strong> to install them before the test
run (described later), or by adding the handlers dynamically during the test
run using
<a href="../stdlib/gen_event#add_handler-3">stdlib/gen_event#add_handler-3</a> or
<a href="../stdlib/gen_event#add_sup_handler-3">stdlib/gen_event#add_sup_handler-3</a>.
In the latter scenario, the reference of the <strong>Common Test</strong> event manager is
required. To get it, call 
<a href="./ct#get_event_mgr_ref-0">ct#get_event_mgr_ref-0</a> 
or (on the <strong>Common Test</strong> Master node) 
<a href="./ct_master#get_event_mgr_ref-0">ct_master#get_event_mgr_ref-0</a>.</p><a name="usage"></a><h4>Use</h4><p>Event handlers can be installed by an <strong>event_handler</strong> start flag 
(<a href="ct_run">ct_run</a>) or option 
<a href="./ct#run_test-1">ct#run_test-1</a>, where the
argument specifies the names of one or more event handler modules.</p><p><em>Example:</em></p><p><strong>$ ct_run -suite test/my_SUITE -event_handler handlers/my_evh1  handlers/my_evh2 -pa $PWD/handlers</strong></p><p>To pass start arguments to the event handler init function, use option 
<strong>ct_run -event_handler_init</strong>  instead of
<strong>-event_handler</strong>.</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>All event handler modules must have <strong>gen_event</strong> behavior.
These modules must be precompiled and their locations must be
added explicitly to the Erlang code server search path (as in the previous
example).</p></div><p>An event_handler tuple in argument <strong>Opts</strong> has the following definition 
(see <a href="./ct#run_test-1">ct#run_test-1</a>):</p><pre>
 {event_handler,EventHandlers}

 EventHandlers = EH | [EH]
 EH = atom() | {atom(),InitArgs} | {[atom()],InitArgs}
 InitArgs = [term()]</pre><p>In the following example, two event handlers for the <strong>my_SUITE</strong> test are installed:</p><pre>
 1&gt; ct:run_test([{suite,"test/my_SUITE"},{event_handler,[my_evh1,{my_evh2,[node()]}]}]).</pre><p>Event handler <strong>my_evh1</strong> is started with <strong>[]</strong> as argument to the init function. 
Event handler <strong>my_evh2</strong> is started with the name of the current node in the init argument list.</p><p>Event handlers can also be plugged in using one of the following
<a href="./run_test_chapter#test_specifications">test specification</a> 
terms:</p><ul><li><strong>{event_handler, EventHandlers}</strong></li><li><strong>{event_handler, EventHandlers, InitArgs}</strong></li><li><strong>{event_handler, NodeRefs, EventHandlers}</strong></li><li><strong>{event_handler, NodeRefs, EventHandlers, InitArgs}</strong></li></ul><p><strong>EventHandlers</strong> is a list of module names. Before a test 
session starts, the init function of each plugged in event handler 
is called (with the <strong>InitArgs</strong> list as argument or <strong>[]</strong> if
no start arguments are specified).</p><p>To plug in a handler to the <strong>Common Test</strong> Master event manager, specify 
<strong>master</strong> as the node in <strong>NodeRefs</strong>.</p><p>To be able to match on events, the event handler module must
include the header file <strong>ct_event.hrl</strong>. An event is a record with the
following definition:</p><p><strong>#event{name, node, data}</strong></p><dl><dt><strong>name</strong></dt><dd><p>Label (type) of the event.</p></dd><dt><strong>node</strong></dt><dd><p>Name of the node that the event originated from 
(only relevant for <strong>Common Test</strong> Master event handlers).</p></dd><dt><strong>data</strong></dt><dd><p>Specific for the event.</p></dd></dl><a name="events"></a><h4>General Events</h4><p>The general events are as follows:</p><dl><dt><strong>#event{name = start_logging, data = LogDir}</strong></dt><dd> <p><strong>LogDir = string()</strong>, top-level log directory for the test run.</p> <p>This event indicates that the logging process of <strong>Common Test</strong>
has started successfully and is ready to receive I/O
messages.</p></dd><dt><strong>#event{name = stop_logging, data = []}</strong></dt><dd> <p>This event indicates that the logging process of <strong>Common Test</strong>
was shut down at the end of the test run.
</p></dd><dt><strong>#event{name = test_start, data = {StartTime,LogDir}}</strong></dt><dd> <p><strong>StartTime = {date(),time()}</strong>, test run start date and time.</p> <p><strong>LogDir = string()</strong>, top-level log directory for the test run.</p> <p>This event indicates that <strong>Common Test</strong> has finished initial preparations
and begins executing test cases.
</p></dd><dt><strong>#event{name = test_done, data = EndTime}</strong></dt><dd> <p><strong>EndTime = {date(),time()}</strong>, date and time the test run finished.</p> <p>This event indicates that the last test case has been executed and 
<strong>Common Test</strong> is shutting down.	
</p></dd><dt><strong>#event{name = start_info, data = {Tests,Suites,Cases}}</strong></dt><dd> <p><strong>Tests = integer()</strong>, number of tests.</p> <p><strong>Suites = integer()</strong>, total number of suites.</p> <p><strong>Cases = integer() | unknown</strong>, total number of test cases.</p> <p>This event gives initial test run information that can be interpreted as: 
"This test run will execute <strong>Tests</strong> separate tests, in total containing
<strong>Cases</strong> number of test cases, in <strong>Suites</strong> number of suites".
However, if a test case group with a repeat property exists in any test, 
the total number of test cases cannot be calculated (unknown).
</p></dd><dt><strong>#event{name = tc_start, data = {Suite,FuncOrGroup}}</strong></dt><dd> <p><strong>Suite = atom()</strong>, name of the test suite.</p> <p><strong>FuncOrGroup = Func | {Conf,GroupName,GroupProperties}</strong></p> <p><strong>Func = atom()</strong>, name of test case or configuration function.</p> <p><strong>Conf = init_per_group | end_per_group</strong>, group configuration function.</p> <p><strong>GroupName = atom()</strong>, name of the group.</p> <p><strong>GroupProperties = list()</strong>, list of execution properties for the group.</p> <p>This event informs about the start of a test case, or a group configuration
function. The event is sent also for <strong>init_per_suite</strong> and <strong>end_per_suite</strong>,
but not for <strong>init_per_testcase</strong> and <strong>end_per_testcase</strong>. If a group
configuration function starts, the group name and execution properties
are also specified.
</p></dd><dt><strong>#event{name = tc_logfile, data = {{Suite,Func},LogFileName}}</strong></dt><dd> <p><strong>Suite = atom()</strong>, name of the test suite.</p> <p><strong>Func = atom()</strong>, name of test case or configuration function.</p> <p><strong>LogFileName = string()</strong>, full name of the test case log file.</p> <p>This event is sent at the start of each test case (and configuration function
except <strong>init/end_per_testcase</strong>) and carries information about the
full name (that is, the file name including the absolute directory path) of
the current test case log file.
</p></dd><dt><strong>#event{name = tc_done, data = {Suite,FuncOrGroup,Result}}</strong></dt><dd> <a name="tc_done"></a> <p><strong>Suite = atom()</strong>, name of the suite.</p> <p><strong>FuncOrGroup = Func | {Conf,GroupName,GroupProperties}</strong></p> <p><strong>Func = atom()</strong>, name of test case or configuration function.</p> <p><strong>Conf = init_per_group | end_per_group</strong>, group configuration function.</p> <p><strong>GroupName = unknown | atom()</strong>, name of the group 
(unknown if init- or end function times out).</p> <p><strong>GroupProperties = list()</strong>, list of execution properties for the group.</p> <p><strong>Result = ok | {auto_skipped,SkipReason} | {skipped,SkipReason} | {failed,FailReason}</strong>,
the result.</p> <a name="skipreason"></a> <p><strong>SkipReason = {require_failed,RequireInfo} |  {require_failed_in_suite0,RequireInfo} |  {failed,{Suite,init_per_testcase,FailInfo}} |  UserTerm</strong>, 
why the case was skipped.</p> <a name="failreason"></a> <p><strong>FailReason = {error,FailInfo} |  {error,{RunTimeError,StackTrace}} |  {timetrap_timeout,integer()} |  {failed,{Suite,end_per_testcase,FailInfo}}</strong>, reason for failure.</p>	 <p><strong>RequireInfo = {not_available,atom() | tuple()}</strong>, why require failed.</p> <p><strong>FailInfo = {timetrap_timeout,integer()} |  {RunTimeError,StackTrace} |  UserTerm</strong>, 
error details.</p> <p><strong>RunTimeError = term()</strong>, a runtime error, for example, 
<strong>badmatch</strong> or <strong>undef</strong>.</p> <p><strong>StackTrace = list()</strong>, list of function calls preceding a runtime error.</p> <p><strong>UserTerm = term()</strong>, any data specified by user, or <strong>exit/1</strong> information.</p> <p>This event informs about the end of a test case or a configuration function (see event 
<strong>tc_start</strong> for details on element <strong>FuncOrGroup</strong>). With this event 
comes the final result of the function in question. It is possible to determine on the 
top level of <strong>Result</strong> if the function was successful, skipped (by the user), 
or if it failed.</p> <p>It is also possible to dig deeper and, for example, perform pattern matching 
on the various reasons for skipped or failed. Notice that <strong>{'EXIT',Reason}</strong> tuples 
are translated into <strong>{error,Reason}</strong>. 
Notice also that if a <strong>{failed,{Suite,end_per_testcase,FailInfo}</strong>
result is received, the test case was successful, but 
<strong>end_per_testcase</strong> for the case failed.
</p></dd><dt><strong>#event{name = tc_auto_skip, data = {Suite,TestName,Reason}}</strong></dt><dd> <a name="tc_auto_skip"></a> <p><strong>Suite = atom()</strong>, the name of the suite.</p> <p><strong>TestName = init_per_suite | end_per_suite | {init_per_group,GroupName} | {end_per_group,GroupName} | {FuncName,GroupName} | FuncName</strong></p> <p><strong>FuncName = atom()</strong>, the name of the test case or configuration function.</p> <p><strong>GroupName = atom()</strong>, the name of the test case group.</p> <p><strong>Reason = {failed,FailReason} | {require_failed_in_suite0,RequireInfo}</strong>, 
reason for auto-skipping <strong>Func</strong>.</p> <p><strong>FailReason = {Suite,ConfigFunc,FailInfo}} |  {Suite,FailedCaseInSequence}</strong>, reason for failure.</p>	 <p><strong>RequireInfo = {not_available,atom() | tuple()}</strong>, why require failed.</p> <p><strong>ConfigFunc = init_per_suite | init_per_group</strong></p> <p><strong>FailInfo = {timetrap_timeout,integer()} |  {RunTimeError,StackTrace} | bad_return | UserTerm</strong>, 
error details.</p> <p><strong>FailedCaseInSequence = atom()</strong>, the name of a case that failed in a sequence.</p> <p><strong>RunTimeError = term()</strong>, a runtime error, for example <strong>badmatch</strong> or
<strong>undef</strong>.</p> <p><strong>StackTrace = list()</strong>, list of function calls preceeding a runtime error.</p> <p><strong>UserTerm = term()</strong>, any data specified by user, or <strong>exit/1</strong> information.</p> <p>This event is sent for every test case or configuration function that <strong>Common Test</strong>
has skipped automatically because of either a failed <strong>init_per_suite</strong> or 
<strong>init_per_group</strong>, a failed <strong>require</strong> in <strong>suite/0</strong>, or a failed test case
in a sequence. Notice that this event is never received as a result of a test case getting
skipped because of <strong>init_per_testcase</strong> failing, as that information is carried with
event <strong>tc_done</strong>. If a failed test case belongs to a test case group, the second
data element is a tuple <strong>{FuncName,GroupName}</strong>, otherwise only the function name.
</p></dd><dt><strong>#event{name = tc_user_skip, data = {Suite,TestName,Comment}}</strong></dt><dd> <a name="tc_user_skip"></a>   <p><strong>Suite = atom()</strong>, the name of the suite.</p> <p><strong>TestName = init_per_suite | end_per_suite | {init_per_group,GroupName} | {end_per_group,GroupName} | {FuncName,GroupName} | FuncName</strong></p> <p><strong>FuncName = atom()</strong>, the name of the test case or configuration function.</p> <p><strong>GroupName = atom()</strong>, the name of the test case group.</p> <p><strong>Comment = string()</strong>, why the test case was skipped.</p> <p>This event specifies that a test case was skipped by the user. 
It is only received if the skip is declared in a test specification. 
Otherwise, user skip information is received as a <strong>{skipped,SkipReason}</strong> 
result in event <strong>tc_done</strong> for the test case. If a skipped test case belongs
to a test case group, the second data element is a tuple <strong>{FuncName,GroupName}</strong>,
otherwise only the function name.
</p></dd><dt><strong>#event{name = test_stats, data = {Ok,Failed,Skipped}}</strong></dt><dd> <p><strong>Ok = integer()</strong>, current number of successful test cases.</p> <p><strong>Failed = integer()</strong>, current number of failed test cases.</p> <p><strong>Skipped = {UserSkipped,AutoSkipped}</strong></p> <p><strong>UserSkipped = integer()</strong>, current number of user-skipped test cases.</p> <p><strong>AutoSkipped = integer()</strong>, current number of auto-skipped test cases.</p> <p>This is a statistics event with current count of successful, skipped, 
and failed test cases so far. This event is sent after the end of each test case,
immediately following event <strong>tc_done</strong>.
</p></dd></dl><h4>Internal Events</h4><p>The internal events are as follows:</p><dl><dt><strong>#event{name = start_make, data = Dir}</strong></dt><dd> <p><strong>Dir = string()</strong>, running make in this directory.</p> <p>This internal event says that <strong>Common Test</strong> starts compiling
modules in directory <strong>Dir</strong>.
</p></dd><dt><strong>#event{name = finished_make, data = Dir}</strong></dt><dd> <p><strong>Dir = string()</strong>, finished running make in this directory.</p> <p>This internal event says that <strong>Common Test</strong> is finished compiling
modules in directory <strong>Dir</strong>.
</p></dd><dt><strong>#event{name = start_write_file, data = FullNameFile}</strong></dt><dd> <p><strong>FullNameFile = string(), full name of the file.</strong></p> <p>This internal event is used by the <strong>Common Test</strong> Master process to 
synchronize particular file operations.
</p></dd><dt><strong>#event{name = finished_write_file, data = FullNameFile}</strong></dt><dd> <p><strong>FullNameFile = string(), full name of the file.</strong></p> <p>This internal event is used by the <strong>Common Test</strong> Master process to 
synchronize particular file operations.
</p></dd></dl><h4>Notes</h4><p>The events are also documented in <strong>ct_event.erl</strong>. This module
can serve as an example of what an event handler for the <strong>Common Test</strong> event 
manager can look like.</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>To ensure that printouts to <strong>stdout</strong> (or printouts made with
<a href="./ct#log-2">ct#log-2</a> or 
<a href="./ct#pal-2">ct#pal-2</a>) get written to the test case log
file, and not to the <strong>Common Test</strong> framework log, you can synchronize
with the <strong>Common Test</strong> server by matching on evvents <strong>tc_start</strong> and <strong>tc_done</strong>.
In the period between these events, all I/O is directed to the
test case log file. These events are sent synchronously to avoid potential
timing problems (for example, that the test case log file is closed just before
an I/O message from an external process gets through). Knowing this, you
need to be careful that your <strong>handle_event/2</strong> callback function does not
stall the test execution, possibly causing unexpected behavior as a result.</p></div><h4>General</h4><p>When creating test suites, it is strongly recommended to not
create dependencies between test cases, that is, letting test cases
depend on the result of previous test cases. There are various
reasons for this, such as, the following:</p><ul><li>It makes it impossible to run test cases individually.</li><li>It makes it impossible to run test cases in a different order.</li><li>It makes debugging difficult (as a fault can be the result of a problem in a different test case than the one failing).</li><li>There are no good and explicit ways to declare dependencies, so  it can be difficult to see and understand these in test suite  code and in test logs.</li><li>Extending, restructuring, and maintaining test suites with  test case dependencies is difficult.</li></ul><p>There are often sufficient means to work around the need for test 
case dependencies. Generally, the problem is related to the state of 
the System Under Test (SUT). The action of one test case can change the 
system state. For some other test case to run properly, this new state 
must be known.</p><p>Instead of passing data between test cases, it is recommended
that the test cases read the state from the SUT and perform assertions
(that is, let the test case run if the state is as expected, otherwise reset or fail).
It is also recommended to use the state to set variables necessary for the 
test case to execute properly. Common actions can often be implemented as 
library functions for test cases to call to set the SUT in a required state. 
(Such common actions can also be separately tested, if necessary,
to ensure that they work as expected). It is sometimes also possible, 
but not always desirable, to group tests together in one test case, that is,
let a test case perform a "scenario" test (a test consisting of subtests).</p><p>Consider, for example, a server application under test. The following 
functionality is to be tested:</p><ul><li>Starting the server</li><li>Configuring the server</li><li>Connecting a client to the server</li><li>Disconnecting a client from the server</li><li>Stopping the server</li></ul><p>There are obvious dependencies between the listed functions. The server cannot 
be configured if it has not first been started, a client connot be connectd until 
the server is properly configured, and so on. If we want to have one test 
case for each function, we might be tempted to try to always run the
test cases in the stated order and carry possible data (identities, handles,
and so on) between the cases and therefore introduce dependencies between them.</p><p>To avoid this, we can consider starting and stopping the server for every test.
We can thus implement the start and stop action as common functions to be 
called from 
<a href="./common_test#Module:init_per_testcase-2">common_test#Module:init_per_testcase-2</a> and 
<a href="./common_test#Module:end_per_testcase-2">common_test#Module:end_per_testcase-2</a>. 
(Remember to test the start and stop functionality separately.) 
The configuration can also be implemented as a common function, maybe grouped 
with the start function. Finally, the testing of connecting and disconnecting a 
client can be grouped into one test case. The resulting suite can look as
follows:</p><pre>      
 -module(my_server_SUITE).
 -compile(export_all).
 -include_lib("ct.hrl").

 %%% init and end functions...

 suite() -&gt; [{require,my_server_cfg}].

 init_per_testcase(start_and_stop, Config) -&gt;
     Config;

 init_per_testcase(config, Config) -&gt;
     [{server_pid,start_server()} | Config];

 init_per_testcase(_, Config) -&gt;
     ServerPid = start_server(),
     configure_server(),
     [{server_pid,ServerPid} | Config].

 end_per_testcase(start_and_stop, _) -&gt;
     ok;

 end_per_testcase(_, _) -&gt;
     ServerPid = ?config(server_pid),
     stop_server(ServerPid).

 %%% test cases...

 all() -&gt; [start_and_stop, config, connect_and_disconnect].

 %% test that starting and stopping works
 start_and_stop(_) -&gt;
     ServerPid = start_server(),
     stop_server(ServerPid).

 %% configuration test
 config(Config) -&gt;
     ServerPid = ?config(server_pid, Config),
     configure_server(ServerPid).

 %% test connecting and disconnecting client
 connect_and_disconnect(Config) -&gt;
     ServerPid = ?config(server_pid, Config),
     {ok,SessionId} = my_server:connect(ServerPid),
     ok = my_server:disconnect(ServerPid, SessionId).

 %%% common functions...

 start_server() -&gt;
     {ok,ServerPid} = my_server:start(),
     ServerPid.

 stop_server(ServerPid) -&gt;
     ok = my_server:stop(),
     ok.

 configure_server(ServerPid) -&gt;
     ServerCfgData = ct:get_config(my_server_cfg),
     ok = my_server:configure(ServerPid, ServerCfgData),
     ok.</pre><a name="save_config"></a><h4>Saving Configuration Data</h4><p>Sometimes it is impossible, or infeasible, to
implement independent test cases. Maybe it is not possible to read the 
SUT state. Maybe resetting the SUT is impossible and it takes too long time
to restart the system. In situations where test case dependency is necessary,
CT offers a structured way to carry data from one test case to the next. The
same mechanism can also be used to carry data from one test suite to the next.</p><p>The mechanism for passing data is called <strong>save_config</strong>. The idea is that
one test case (or suite) can save the current value of <strong>Config</strong>, or any list of
key-value tuples, so that the next executing test case (or test suite) can read it. 
The configuration data is not saved permanently but can only be passed from one 
case (or suite) to the next.</p><p>To save <strong>Config</strong> data, return tuple <strong>{save_config,ConfigList}</strong>
from <strong>end_per_testcase</strong> or from the main test case function.</p><p>To read data saved by a previous test case, use macro <strong>config</strong> with a 
<strong>saved_config</strong> key as follows:</p><p><strong>{Saver,ConfigList} = ?config(saved_config, Config)</strong></p><p><strong>Saver</strong> (<strong>atom()</strong>) is the name of the previous test case (where the
data was saved). The <strong>config</strong> macro can be used to extract particular data
also from the recalled <strong>ConfigList</strong>. It is strongly recommended that 
<strong>Saver</strong> is always matched to the expected name of the saving test case. 
This way, problems because of restructuring of the test suite can be avoided. 
Also, it makes the dependency more explicit and the test suite easier to read 
and maintain.</p><p>To pass data from one test suite to another, the same mechanism is used. The data
is to be saved by finction
<a href="./common_test#Module:end_per_suite-1">common_test#Module:end_per_suite-1</a> 
and read by function
<a href="./common_test#Module:init_per_suite-1">common_test#Module:init_per_suite-1</a>
in the suite that follows. When passing data between suites, <strong>Saver</strong> carries the 
name of the test suite.</p><p><em>Example:</em></p><pre>
 -module(server_b_SUITE).
 -compile(export_all).
 -include_lib("ct.hrl").

 %%% init and end functions...

 init_per_suite(Config) -&gt;
     %% read config saved by previous test suite
     {server_a_SUITE,OldConfig} = ?config(saved_config, Config),
     %% extract server identity (comes from server_a_SUITE)
     ServerId = ?config(server_id, OldConfig),
     SessionId = connect_to_server(ServerId),
     [{ids,{ServerId,SessionId}} | Config].

 end_per_suite(Config) -&gt;
     %% save config for server_c_SUITE (session_id and server_id)
     {save_config,Config}

 %%% test cases...

 all() -&gt; [allocate, deallocate].

 allocate(Config) -&gt;
     {ServerId,SessionId} = ?config(ids, Config),
     {ok,Handle} = allocate_resource(ServerId, SessionId),
     %% save handle for deallocation test
     NewConfig = [{handle,Handle}],
     {save_config,NewConfig}.

 deallocate(Config) -&gt;
     {ServerId,SessionId} = ?config(ids, Config),
     {allocate,OldConfig} = ?config(saved_config, Config),
     Handle = ?config(handle, OldConfig),
     ok = deallocate_resource(ServerId, SessionId, Handle).</pre><p>To save <strong>Config</strong> data from a test case that is to be
skipped, return tuple 
<strong>{skip_and_save,Reason,ConfigList}</strong>.</p><p>The result is that the test case is skipped with <strong>Reason</strong> printed to
the log file (as described earlier) and <strong>ConfigList</strong> is saved 
for the next test case. <strong>ConfigList</strong> can be read using 
<strong>?config(saved_config, Config)</strong>, as described earlier. <strong>skip_and_save</strong>
can also be returned from <strong>init_per_suite</strong>. In this case, the saved data can
be read by <strong>init_per_suite</strong> in the suite that follows.</p><a name="sequences"></a><h4>Sequences</h4><p>Sometimes test cases depend on each other so that
if one case fails, the following tests are not to be executed.
Typically, if the <strong>save_config</strong> facility is used and a test 
case that is expected to save data crashes, the following 
case cannot run. <strong>Common Test</strong> offers a way to declare such dependencies, 
called sequences.</p><p>A sequence of test cases is defined as a test case group
with a <strong>sequence</strong> property. Test case groups are defined
through function <strong>groups/0</strong> in the test suite (for details, see section
<a href="./write_test_chapter#test_case_groups">Test Case Groups</a>.</p><p>For example, to ensure that if <strong>allocate</strong>
in <strong>server_b_SUITE</strong> crashes, <strong>deallocate</strong> is skipped,
the following sequence can be defined:</p><pre>
 groups() -&gt; [{alloc_and_dealloc, [sequence], [alloc,dealloc]}].</pre><p>Assume that the suite contains the test case <strong>get_resource_status</strong> 
that is independent of the other two cases, then function <strong>all</strong> can 
look as follows:</p><pre>
 all() -&gt; [{group,alloc_and_dealloc}, get_resource_status].</pre><p>If <strong>alloc</strong> succeeds, <strong>dealloc</strong> is also executed. If <strong>alloc</strong> fails
however, <strong>dealloc</strong> is not executed but marked as <strong>SKIPPED</strong> in the HTML log. 
<strong>get_resource_status</strong> runs no matter what happens to the <strong>alloc_and_dealloc</strong>
cases.</p><p>Test cases in a sequence are executed in order until all succeed or 
one fails. If one fails, all following cases in the sequence are skipped.
The cases in the sequence that have succeeded up to that point are reported as 
successful in the log. Any number of sequences can be specified.</p><p><em>Example:</em></p><pre>
 groups() -&gt; [{scenarioA, [sequence], [testA1, testA2]}, 
              {scenarioB, [sequence], [testB1, testB2, testB3]}].

 all() -&gt; [test1, 
           test2, 
           {group,scenarioA}, 
	   test3, 
           {group,scenarioB}, 
           test4].</pre><p>A sequence group can have subgroups. Such subgroups can have 
any property, that is, they are not required to also be sequences. If you want the 
status of the subgroup to affect the sequence on the level above, return 
<strong>{return_group_result,Status}</strong> from 
<a href="./common_test#Module:end_per_group-2">common_test#Module:end_per_group-2</a>, 
as described in section
<a href="./write_test_chapter#repeated_groups">Repeated Groups</a>
in Writing Test Suites.
A failed subgroup (<strong>Status == failed</strong>) causes the execution of a 
sequence to fail in the	same way a test case does.</p><a name="general"></a><h4>General</h4><p>
The <em>Common Test Hook (CTH)</em> framework allows 
extensions of the default behavior of <strong>Common Test</strong> using hooks 
before and after all test suite calls. CTHs allow advanced <strong>Common Test</strong>
users to abstract out behavior that is common to multiple test suites
without littering all test suites with library calls. This can be used
for logging, starting, and monitoring external systems, 
building C files needed by the tests, and so on.</p><p>In brief, CTH allows you to do the following:</p><ul><li>Manipulate the runtime configuration before each suite  configuration call.</li><li>Manipulate the return of all suite configuration calls, and in  extension, the result of the tests themselves.</li></ul><p>The following sections describe how to use CTHs, when they are run,
and how to manipulate the test results in a CTH.</p><div class="alert alert-warning"><h4 class="alert-heading">Warning</h4><p>When executing within a CTH, all timetraps are shut off. So
if your CTH never returns, the entire test run is stalled.</p></div><a name="installing"></a><h4>Installing a CTH</h4><p>A CTH can be installed in multiple ways in your test run. You can do it
for all tests in a run, for specific test suites, and for specific groups 
within a test suite. If you want a CTH to be present in all test suites 
within your test run, there are three ways to accomplish that, as follows:
</p><ul><li>Add <strong>-ct_hooks</strong> as an argument to  <a href="./run_test_chapter#ct_run">ct_run</a>.  To add multiple CTHs using this method, append them to each other using the keyword <strong>and</strong>, that is,  <strong>ct_run -ct_hooks cth1 [{debug,true}] and cth2 ...</strong>.</li><li>Add tag <strong>ct_hooks</strong> to your  <a href="./run_test_chapter#test_specifications"> Test Specification</a>.</li><li>Add tag <strong>ct_hooks</strong> to your call to  <a href="./ct#run_test-1">ct:run_test/1</a>.</li></ul><p>CTHs can also be added within a test suite. This is done by returning
<strong>{ct_hooks,[CTH]}</strong> in the configuration list from 
<a href="./common_test#Module:suite-0">suite/0</a>,
<a href="./common_test#Module:init_per_suite-1"> init_per_suite/1</a>, or
<a href="./common_test#Module:init_per_group-2"> init_per_group/2</a>.</p><p>In this case, <strong>CTH</strong> can either be only the module name of the CTH 
or a tuple with the module name and the initial arguments, and optionally 
the hook priority of the CTH. For example, one of the following:</p><ul><li><strong>{ct_hooks,[my_cth_module]}</strong></li><li><strong>{ct_hooks,[{my_cth_module,[{debug,true}]}]}</strong></li><li><strong>{ct_hooks,[{my_cth_module,[{debug,true}],500}]}</strong></li></ul><h4>Overriding CTHs</h4><p>By default, each installation of a CTH causes a new instance of it
to be activated. This can cause problems if you want to override 
CTHs in test specifications while still having them in the
suite information function. The 
<a href="./ct_hooks#Module:id-1">id/1</a>
callback exists to address this problem. By returning the same
<strong>id</strong> in both places, <strong>Common Test</strong> knows that this CTH
is already installed and does not try to install it again.</p><h4>CTH Execution Order</h4><p>By default, each CTH installed is executed in the order that
they are installed for init calls, and then reversed for end calls.
This is not always desired, so <strong>Common Test</strong> allows
the user to specify a priority for each hook. The priority can either
be specified in the CTH function 
<a href="./ct_hooks#Module:init-2">init/2</a> or when 
installing the hook. The priority specified at installation overrides the 
priority returned by the CTH.</p><a name="scope"></a><h4>CTH Scope</h4><p>Once the CTH is installed into a certain test run it remains there until
its scope is expired. The scope of a CTH depends on when it is 
installed, see the following table.
Function <a href="./ct_hooks#Module:init-2">init/2</a> is 
called at the beginning of the scope and function
<a href="./ct_hooks#Module:terminate-1">terminate/1</a> 
is called when the scope ends.</p><table class="table table-bordered table-hover table-striped"><caption>Scope of a CTH</caption><tbody><tr><td><em>CTH installed in</em></td><td><em>CTH scope begins before</em></td><td><em>CTH scope ends after</em></td></tr><tr><td><a href="./run_test_chapter#ct_run">ct_run</a></td><td>the first test suite is to be run</td><td>the last test suite has been run</td></tr><tr><td><a href="./ct#run_test-1">ct:run_test</a></td><td>the first test suite is run</td><td>the last test suite has been run</td></tr><tr><td><a href="./run_test_chapter#test_specifications"> Test Specification</a></td><td>the first test suite is run</td><td>the last test suite has been run</td></tr><tr><td><a href="./common_test#Module:suite-0">suite/0 </a></td><td><a href="./ct_hooks#Module:pre_init_per_suite-3"> pre_init_per_suite/3</a> is called</td><td><a href="./ct_hooks#Module:post_end_per_suite-4"> post_end_per_suite/4</a> has been called for that test suite</td></tr><tr><td><a href="./common_test#Module:init_per_suite-1"> init_per_suite/1</a></td><td><a href="./ct_hooks#Module:post_init_per_suite-4"> post_init_per_suite/4</a> is called</td><td><a href="./ct_hooks#Module:post_end_per_suite-4"> post_end_per_suite/4</a> has been called for that test suite</td></tr><tr><td><a href="./common_test#Module:init_per_group-2"> init_per_group/2</a></td><td><a href="./ct_hooks#Module:post_init_per_group-5"> post_init_per_group/5</a> is called</td><td><a href="./ct_hooks#Module:post_end_per_group-5"> post_end_per_group/5</a> has been called for that group</td></tr></tbody></table><h4>CTH Processes and Tables</h4><p>CTHs are run with the same process scoping as normal test suites,
that is, a different process executes the <strong>init_per_suite</strong> hooks then the
<strong>init_per_group</strong> or <strong>per_testcase</strong> hooks. So if you want to spawn a 
process in the CTH, you cannot link with the CTH process, as it exits 
after the post hook ends. Also, if you for some reason need an ETS 
table with your CTH, you must spawn a process that handles it.</p><h4>External Configuration Data and Logging</h4><p>Configuration data values in the CTH can be read
by calling 
<a href="./ct#get_config-1">ct#get_config-1</a> 
(as explained in section
<a href="./config_file_chapter#require_config_data">Requiring and Reading Configuration Data</a>).
The configuration variables in question must, as always, first have been
required by a suite-, group-, or test case information function,
or by function <a href="./ct#require-1">ct#require-1</a>.
The latter can also be used in CT hook functions.</p><p>The CT hook functions can call any logging function
in the <strong>ct</strong> interface to print information to the log files, or to
add comments in the suite overview page.
</p><a name="manipulating"></a><h4>Manipulating Tests</h4><p>Through CTHs the results of tests and configuration functions can be manipulated. 
The main purpose to do this with CTHs is to allow common 
patterns to be abstracted out from test suites and applied to
multiple test suites without duplicating any code. All the callback
functions for a CTH follow a common interface described hereafter.</p><p><strong>Common Test</strong> always calls all available hook functions, even pre- 
and post hooks for configuration functions that are not implemented in the suite.
For example, <strong>pre_init_per_suite(x_SUITE, ...)</strong> and
<strong>post_init_per_suite(x_SUITE, ...)</strong> are called for test suite
<strong>x_SUITE</strong>, even if it does not export <strong>init_per_suite/1</strong>. 
With this feature hooks can be used as configuration fallbacks, and all
configuration functions can be replaced with hook functions.</p><a name="pre"></a><h4>Pre Hooks</h4><p>
In a CTH, the behavior can be hooked in before the following functions:</p><ul><li><a href="./common_test#Module:init_per_suite-1">common_test#Module:init_per_suite-1</a></li><li><a href="./common_test#Module:init_per_group-2">common_test#Module:init_per_group-2</a></li><li><a href="./common_test#Module:init_per_testcase-2">common_test#Module:init_per_testcase-2</a></li><li><a href="./common_test#Module:end_per_testcase-2">common_test#Module:end_per_testcase-2</a></li><li><a href="./common_test#Module:end_per_group-2">common_test#Module:end_per_group-2</a></li><li><a href="./common_test#Module:end_per_suite-1">common_test#Module:end_per_suite-1</a></li></ul><p>
This is done in the CTH functions called <strong>pre_&lt;name of function&gt;</strong>.
These functions take the arguments <strong>SuiteName</strong>, <strong>Name</strong> (group or test case name, if applicable), 
<strong>Config</strong>, and <strong>CTHState</strong>. The return value of the CTH function
is always a combination of a result for the suite/group/test and an 
updated <strong>CTHState</strong>.</p><p>To let the test suite continue on executing, return the configuration 
list that you want the test to use as the result.</p><p>All pre hooks, except <strong>pre_end_per_testcase/4</strong>, can
skip or fail the test by returning a tuple with <strong>skip</strong> or
<strong>fail</strong>, and a reason as the result.</p><p><em>Example:</em></p><pre><code class="">
 pre_init_per_suite(SuiteName, Config, CTHState) -&gt;
   case db:connect() of
     {error,_Reason} -&gt;
       {{fail, "Could not connect to DB"}, CTHState};
     {ok, Handle} -&gt;
       {[{db_handle, Handle} | Config], CTHState#state{ handle = Handle }}
   end.</code></pre><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>If you use multiple CTHs, the first part of the return tuple is
used as input for the next CTH. So in the previous example the next CTH can
get <strong>{fail,Reason}</strong> as the second parameter. If you have many CTHs
interacting, do not let each CTH return <strong>fail</strong> or <strong>skip</strong>. 
Instead, return that an action is to be taken through the <strong>Config</strong> 
list and implement a CTH that, at the end, takes the correct action.</p></div><a name="post"></a><h4>Post Hooks</h4><p>In a CTH, behavior can be hooked in after the following functions:</p><ul><li><a href="./common_test#Module:init_per_suite-1">common_test#Module:init_per_suite-1</a></li><li><a href="./common_test#Module:init_per_group-2">common_test#Module:init_per_group-2</a></li><li><a href="./common_test#Module:init_per_testcase-2">common_test#Module:init_per_testcase-2</a></li><li><a href="./common_test#Module:end_per_testcase-2">common_test#Module:end_per_testcase-2</a></li><li><a href="./common_test#Module:end_per_group-2">common_test#Module:end_per_group-2</a></li><li><a href="./common_test#Module:end_per_suite-1">common_test#Module:end_per_suite-1</a></li></ul><p>
This is done in the CTH functions called <strong>post_&lt;name of function&gt;</strong>. 
These functions take the arguments <strong>SuiteName</strong>, <strong>Name</strong> (group or test case name, if applicable),
<strong>Config</strong>, <strong>Return</strong>, and <strong>CTHState</strong>. <strong>Config</strong> in this
case is the same <strong>Config</strong> as the testcase is called with. 
<strong>Return</strong> is the value returned by the testcase. If the testcase 
fails by crashing, <strong>Return</strong> is
<strong>{'EXIT',{{Error,Reason},Stacktrace}}</strong>.</p><p>The return value of the CTH function is always a combination of a
result for the suite/group/test and an updated <strong>CTHState</strong>. If
you do not want the callback to affect the outcome of the test,
return the <strong>Return</strong> data as it is given to the CTH. You can also
modify the test result. By returning the <strong>Config</strong> list
with element <strong>tc_status</strong> removed, you can recover from a test 
failure. As in all the pre hooks, it is also possible to fail/skip
the test case in the post hook.</p><p><em>Example:</em></p><pre><code class="">
 post_end_per_testcase(_Suite, _TC, Config, {'EXIT',{_,_}}, CTHState) -&gt;
   case db:check_consistency() of
     true -&gt;
       %% DB is good, pass the test.
       {proplists:delete(tc_status, Config), CTHState};
     false -&gt;
       %% DB is not good, mark as skipped instead of failing
       {{skip, "DB is inconsisten!"}, CTHState}
   end;
 post_end_per_testcase(_Suite, _TC, Config, Return, CTHState) -&gt;
   %% Do nothing if tc does not crash.
   {Return, CTHState}.</code></pre><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>Do recover from a testcase failure using CTHs only a last resort. 
If used wrongly, it can be very difficult to determine which tests that 
pass or fail in a test run.</p></div><h4>Skip and Fail Hooks</h4><p>
After any post hook has been executed for all installed CTHs, 
<a href="./ct_hooks#Module:on_tc_fail-4">on_tc_fail</a>
or <a href="./ct_hooks#Module:on_tc_skip-4">on_tc_skip</a>
is called if the testcase failed or was skipped, respectively. 
You cannot affect the outcome of the tests any further at this point. 
</p><a name="synchronizing"></a><h4>Synchronizing External User Applications with Common Test</h4><p>CTHs can be used to synchronize test runs with external user applications.
The init function can, for example, start and/or communicate with an application that
has the purpose of preparing the SUT for an upcoming test run, or 
initialize a database for saving test data to during the test run. The
terminate function can similarly order such an application to reset the SUT
after the test run, and/or tell the application to finish active sessions
and terminate.
Any system error- or progress reports generated during the init- or
termination stage are saved in the 
<a href="./run_test_chapter#pre_post_test_io_log">Pre- and Post Test I/O Log</a>. 
(This is also true for any printouts made
with <strong>ct:log/2</strong> and <strong>ct:pal/2</strong>).</p><p>To ensure that <strong>Common Test</strong> does not start executing tests, or
closes its log files and shuts down, before the external application
is ready for it, <strong>Common Test</strong> can be synchronized with the application. 
During startup and shutdown, <strong>Common Test</strong> can be suspended, simply by
having a CTH evaluate a <strong>receive</strong> expression in the init- or terminate
function. The macros <strong>?CT_HOOK_INIT_PROCESS</strong> (the process executing the hook
init function) and <strong>?CT_HOOK_TERMINATE_PROCESS</strong> (the process executing
the hook terminate function) each specifies the name of the correct <strong>Common Test</strong>
process to send a message to. This is done to return from the <strong>receive</strong>.
These macros are defined in <strong>ct.hrl</strong>.
</p><a name="example"></a><h4>Example CTH</h4><p>The following CTH logs information about a test run into a format 
parseable by <a href="../kernel/file#consult-1">file:consult/1</a> 
(in Kernel):
</p><pre><code class="">
 %%% Common Test Example Common Test Hook module.
 -module(example_cth).

 %% Callbacks
 -export([id/1]).
 -export([init/2]).

 -export([pre_init_per_suite/3]).
 -export([post_init_per_suite/4]).
 -export([pre_end_per_suite/3]).
 -export([post_end_per_suite/4]).

 -export([pre_init_per_group/4]).
 -export([post_init_per_group/5]).
 -export([pre_end_per_group/4]).
 -export([post_end_per_group/5]).

 -export([pre_init_per_testcase/4]).
 -export([post_init_per_testcase/5]).
 -export([pre_end_per_testcase/4]).
 -export([post_end_per_testcase/5]).

 -export([on_tc_fail/4]).
 -export([on_tc_skip/4]).

 -export([terminate/1]).

 -record(state, { file_handle, total, suite_total, ts, tcs, data }).

 %% Return a unique id for this CTH.
 id(Opts) -&gt;
   proplists:get_value(filename, Opts, "/tmp/file.log").

 %% Always called before any other callback function. Use this to initiate
 %% any common state. 
 init(Id, Opts) -&gt;
     {ok,D} = file:open(Id,[write]),
     {ok, #state{ file_handle = D, total = 0, data = [] }}.

 %% Called before init_per_suite is called.
 pre_init_per_suite(Suite,Config,State) -&gt;
     {Config, State#state{ suite_total = 0, tcs = [] }}.

 %% Called after init_per_suite.
 post_init_per_suite(Suite,Config,Return,State) -&gt;
     {Return, State}.

 %% Called before end_per_suite.
 pre_end_per_suite(Suite,Config,State) -&gt;
     {Config, State}.

 %% Called after end_per_suite.
 post_end_per_suite(Suite,Config,Return,State) -&gt;
     Data = {suites, Suite, State#state.suite_total, lists:reverse(State#state.tcs)},
     {Return, State#state{ data = [Data | State#state.data] ,
                           total = State#state.total + State#state.suite_total } }.

 %% Called before each init_per_group.
 pre_init_per_group(Suite,Group,Config,State) -&gt;
     {Config, State}.

 %% Called after each init_per_group.
 post_init_per_group(Suite,Group,Config,Return,State) -&gt;
     {Return, State}.

 %% Called before each end_per_group.
 pre_end_per_group(Suite,Group,Config,State) -&gt;
     {Config, State}.

 %% Called after each end_per_group.
 post_end_per_group(Suite,Group,Config,Return,State) -&gt;
     {Return, State}.

 %% Called before each init_per_testcase.
 pre_init_per_testcase(Suite,TC,Config,State) -&gt;
     {Config, State#state{ ts = now(), total = State#state.suite_total + 1 } }.

 %% Called after each init_per_testcase (immediately before the test case).
 post_init_per_testcase(Suite,TC,Config,Return,State) -&gt;
     {Return, State}

%% Called before each end_per_testcase (immediately after the test case).
 pre_end_per_testcase(Suite,TC,Config,State) -&gt;
     {Config, State}.

 %% Called after each end_per_testcase.
 post_end_per_testcase(Suite,TC,Config,Return,State) -&gt;
     TCInfo = {testcase, Suite, TC, Return, timer:now_diff(now(), State#state.ts)},
     {Return, State#state{ ts = undefined, tcs = [TCInfo | State#state.tcs] } }.

 %% Called after post_init_per_suite, post_end_per_suite, post_init_per_group,
 %% post_end_per_group and post_end_per_testcase if the suite, group or test case failed.
 on_tc_fail(Suite, TC, Reason, State) -&gt;
     State.

 %% Called when a test case is skipped by either user action
 %% or due to an init function failing.  
 on_tc_skip(Suite, TC, Reason, State) -&gt;
     State.

 %% Called when the scope of the CTH is done
 terminate(State) -&gt;
     io:format(State#state.file_handle, "~p.~n",
                [{test_run, State#state.total, State#state.data}]),
     file:close(State#state.file_handle),
     ok.</code></pre><a name="builtin_cths"></a><h4>Built-In CTHs</h4><p><strong>Common Test</strong> is delivered with some general-purpose CTHs that
can be enabled by the user to provide generic testing functionality.
Some of these CTHs are enabled by default when <strong>common_test</strong> is started to run.
They can be disabled by setting <strong>enable_builtin_hooks</strong> to
<strong>false</strong> on the command line or in the test specification. The following
two CTHs are delivered with <strong>Common Test</strong>:</p><dl><dt><strong>cth_log_redirect</strong></dt><dd> <p>Built-in</p> <p>Captures all log events that would normally be printed by the default
logger handler, and prints them to the current test case log.
If an event cannot be associated with a test case, it is printed in
the <strong>Common Test</strong> framework log.
This happens for test cases running in parallel and events occuring
in-between test cases. You can configure the level of
<a href="./sasl_app">SASL</a> reports
using the normal SASL mechanisms.</p> </dd><dt><strong>cth_surefire</strong></dt><dd> <p>Not built-in</p> <p>Captures all test results and outputs them as surefire
XML into a file. The created file is by default
called <strong>junit_report.xml</strong>. The file name can be changed by
setting option <strong>path</strong> for this hook, for example:</p> <p><strong>-ct_hooks cth_surefire [{path,"/tmp/report.xml"}]</strong></p> <p>If option <strong>url_base</strong> is set, an extra
attribute named <strong>url</strong> is added to each
<strong>testsuite</strong> and <strong>testcase</strong> XML element. The value
is constructed from <strong>url_base</strong> and a relative path
to the test suite or test case log, respectively, for example:</p> <p><strong>-ct_hooks cth_surefire [{url_base, "http://myserver.com/"}]</strong></p> <p>gives an URL attribute value similar to</p> <p><strong>"http://myserver.com/ct_run.ct@myhost.2012-12-12_11.19.39/ x86_64-unknown-linux-gnu.my_test.logs/run.2012-12-12_11.19.39/suite.log.html"</strong></p> <p>Surefire XML can, for example, be used by Jenkins to display test
results.</p> </dd></dl><h4>Goals</h4><p>It is not possible to prove that a program is correct by
testing. On the contrary, it has been formally proven that it is
impossible to prove programs in general by testing. Theoretical
program proofs or plain examination of code can be viable options
for those wishing to certify that a program is correct. The test
server, as it is based on testing, cannot be used for
certification. Its intended use is instead to (cost effectively)
<em>find bugs</em>. A successful test suite is one that reveals a
bug. If a test suite results in OK, then we know very little that
we did not know before.</p><h4>What to Test</h4><p>
There are many kinds of test suites. Some concentrate on
calling every function or command (in the documented way) in 
a certain interface.
Some others do the same, but use all kinds of illegal
parameters, and verify that the server stays alive and rejects
the requests with reasonable error codes. Some test suites
simulate an application (typically consisting of a few modules of
an application), some try to do tricky requests in general, and some
test suites even test internal functions with help of special
Load Modules on target.</p><p>Another interesting category of test suites is the one
checking that fixed bugs do not reoccur. When a bugfix is introduced,
a test case that checks for that specific bug is written
and submitted to the affected test suites.</p><p>Aim for finding bugs. Write whatever test that has the highest
probability of finding a bug, now or in the future. Concentrate
more on the critical parts. Bugs in critical subsystems are much
more expensive than others.</p><p>Aim for functionality testing rather than implementation
details. Implementation details change quite often, and the test
suites are to be long lived. Implementation details often differ
on different platforms and versions. If implementation details
must be tested, try to factor them out into separate test
cases. These test cases can later be rewritten or skipped.</p><p>Also, aim for testing everything once, no less, no more. It is
not effective having every test case fail only because one
function in the interface changed.</p><h3>common_test</h3><p>A framework for automated testing of any target nodes.</p><p>The <strong>Common Test</strong> framework is an environment for
implementing and performing automatic and semi-automatic execution of
test cases.In brief, <strong>Common Test</strong> supports:<ul><li>Automated execution of test suites (sets of test cases)</li><li>Logging of events during execution</li><li>HTML presentation of test suite results</li><li>HTML presentation of test suite code</li><li>Support functions for test suite authors</li><li>Step-by-step execution of test cases</li></ul>The following section describes the mandatory and optional test suite
functions that <strong>Common Test</strong> calls during test execution.
For more details, see section
<a href="write_test_chapter">Writing Test Suites</a>
in the User's Guide.</p><h4>Test Case Callback Functions</h4><p>The following functions define the callback interface
for a test suite.</p><h3>Functions</h3><h4>Module:all() -&gt; Tests | {skip,Reason}</h4><p>Returns the list of all test case groups and test cases in the module.</p><ul><li><span class="v">Tests = [TestCase | {testcase,TestCase,TCRepeatProps} | {group,GroupName} | {group,GroupName,Properties} | {group,GroupName,Properties,SubGroups}]</span></li><li><span class="v">TestCase = atom()</span></li><li><span class="v">TCRepeatProps = [{repeat,N} | {repeat_until_ok,N} | {repeat_until_fail,N}]</span></li><li><span class="v">GroupName = atom()</span></li><li><span class="v">Properties = [parallel | sequence | Shuffle | {GroupRepeatType,N}] | default</span></li><li><span class="v">SubGroups = [{GroupName,Properties} | {GroupName,Properties,SubGroups}]</span></li><li><span class="v">Shuffle = shuffle | {shuffle,Seed}</span></li><li><span class="v">Seed = {integer(),integer(),integer()}</span></li><li><span class="v">GroupRepeatType = repeat | repeat_until_all_ok | repeat_until_all_fail | repeat_until_any_ok | repeat_until_any_fail</span></li><li><span class="v">N = integer() | forever</span></li><li><span class="v">Reason = term()</span></li></ul><p>MANDATORY</p><p>Returns the list of all test cases and test case groups in the
test suite module to be executed. This list also specifies the
order the cases and groups are executed by <strong>Common Test</strong>.
A test case is represented by an atom,
the name of the test case function, or a <strong>testcase</strong> tuple
indicating that the test case shall be repeated. A test case group is
represented by a <strong>group</strong> tuple, where <strong>GroupName</strong>,
an atom, is the name of the group (defined in
<a href="#Module:groups-0">Module:groups-0</a>).
Execution properties for groups can also be specified, both
for a top-level group and for any of its subgroups.
Group execution properties specified here override
properties in the group definition (see
<a href="#Module:groups-0">Module:groups-0</a>).
(With value <strong>default</strong>, the group definition properties
are used).</p><p>If <strong>{skip,Reason}</strong> is returned, all test cases
in the module are skipped and <strong>Reason</strong>
is printed on the HTML result page.</p><p>For details on groups, see section
<a href="./write_test_chapter#test_case_groups">Test Case Groups</a> in the User's Guide.</p><h4>Module:groups() -&gt; GroupDefs</h4><p>Returns a list of test case group definitions.</p><ul><li><span class="v">GroupDefs = [Group]</span></li><li><span class="v">Group = {GroupName,Properties,GroupsAndTestCases}</span></li><li><span class="v">GroupName = atom()</span></li><li><span class="v">Properties = [parallel | sequence | Shuffle | {GroupRepeatType,N}]</span></li><li><span class="v">GroupsAndTestCases = [Group | {group,GroupName} | TestCase | {testcase,TestCase,TCRepeatProps}]</span></li><li><span class="v">TestCase = atom()</span></li><li><span class="v">TCRepeatProps = [{repeat,N} | {repeat_until_ok,N} | {repeat_until_fail,N}]</span></li><li><span class="v">Shuffle = shuffle | {shuffle,Seed}</span></li><li><span class="v">Seed = {integer(),integer(),integer()}</span></li><li><span class="v">GroupRepeatType = repeat | repeat_until_all_ok | repeat_until_all_fail | repeat_until_any_ok | repeat_until_any_fail</span></li><li><span class="v">N = integer() | forever</span></li></ul><p>OPTIONAL</p><p>Defines test case groups. For details, see section
<a href="./write_test_chapter#test_case_groups">Test Case  Groups</a> in the User's Guide.</p><h4>Module:suite() -&gt; [Info]</h4><p>Test suite info function (providing default data for the suite).</p><ul><li><span class="v">Info = {timetrap,Time} | {require,Required} | {require,Name,Required} | {userdata,UserData} | {silent_connections,Conns} | {stylesheet,CSSFile} | {ct_hooks, CTHs}</span></li><li><span class="v">Time = TimeVal | TimeFunc</span></li><li><span class="v">TimeVal = MilliSec | {seconds,integer()} | {minutes,integer()} | {hours,integer()}</span></li><li><span class="v">TimeFunc = {Mod,Func,Args} | Fun</span></li><li><span class="v">MilliSec = integer()</span></li><li><span class="v">Mod = atom()</span></li><li><span class="v">Func = atom()</span></li><li><span class="v">Args = list()</span></li><li><span class="v">Fun = fun()</span></li><li><span class="v">Required = Key | {Key,SubKeys} | {Key,SubKey} | {Key,SubKey,SubKeys}</span></li><li><span class="v">Key = atom()</span></li><li><span class="v">SubKeys = SubKey | [SubKey]</span></li><li><span class="v">SubKey = atom()</span></li><li><span class="v">Name = atom()</span></li><li><span class="v">UserData = term()</span></li><li><span class="v">Conns = [atom()]</span></li><li><span class="v">CSSFile = string()</span></li><li><span class="v">CTHs = [CTHModule |</span></li><li><span class="v">Â Â Â Â Â Â Â Â {CTHModule, CTHInitArgs} |</span></li><li><span class="v">Â Â Â Â Â Â Â Â {CTHModule, CTHInitArgs, CTHPriority}]</span></li><li><span class="v">CTHModule = atom()</span></li><li><span class="v">CTHInitArgs = term()</span></li></ul><p>OPTIONAL</p><p>The test suite information function.	Returns a list of tagged
tuples specifying various properties related to the execution of
this test suite (common for all test cases in the suite).</p><p>Tag <strong>timetrap</strong> sets the maximum time that each
test case is allowed to execute (including
<a href="#Module:init_per_testcase-2">Module:init_per_testcase-2</a>
and
<a href="#Module:end_per_testcase-2">Module:end_per_testcase-2</a>).
If the timetrap time is exceeded, the test case fails with reason
<strong>timetrap_timeout</strong>. A <strong>TimeFunc</strong> function can be used to
set a new timetrap by returning a <strong>TimeVal</strong>. It can also be
used to trigger a timetrap time-out by, at some point, returning a
value other than a <strong>TimeVal</strong>. For details, see section
<a href="./write_test_chapter#timetraps">Timetrap Time-Outs</a>
in the User's Guide.</p><p>Tag <strong>require</strong> specifies configuration variables
required by test cases (or configuration functions)
in the suite. If the required configuration variables are not found
in any of the configuration files, all test cases are skipped.
For details about the <strong>require</strong> functionality, see funtion
<a href="./ct#require-1">ct#require-1</a>.</p><p>With <strong>userdata</strong>, the user can
specify any test suite-related information, which can be
read by calling
<a href="./ct#userdata-2">ct#userdata-2</a>.</p><p>Tag <strong>ct_hooks</strong> specifies the
<a href="ct_hooks_chapter">Common Test Hooks</a>
to be run with this suite.</p><p>Other tuples than the ones defined are ignored.</p><p>For details about the test suite information function, see section
<a href="./write_test_chapter#suite">Test Suite Information Function</a> in the User's Guide.</p><h4>Module:init_per_suite(Config) -&gt; NewConfig | {skip,Reason} |
	  {skip_and_save,Reason,SaveConfig}</h4><p>Test suite initializations.</p><ul><li><span class="v">Config = NewConfig = SaveConfig = [{Key,Value}]</span></li><li><span class="v">Key = atom()</span></li><li><span class="v">Value = term()</span></li><li><span class="v">Reason = term()</span></li></ul><p>OPTIONAL; if this function is defined, then <a href="#Module:end_per_suite-1">Module:end_per_suite-1</a>
must also be defined.</p><p>This configuration function is called as the first function in the
suite. It typically contains initializations that are common for
all test cases in the suite, and that must only be done
once. Parameter <strong>Config</strong> is the configuration data
that can be modified. Whatever is returned from this
function is specified as <strong>Config</strong> to all configuration functions
and test cases in the suite.</p><p>If <strong>{skip,Reason}</strong>
is returned, all test cases in the suite are skipped
and <strong>Reason</strong> is printed in the overview log for the suite.</p><p>For information on <strong>save_config</strong> and <strong>skip_and_save</strong>,
see section
<a href="./dependencies_chapter#save_config">Saving Configuration Data</a> in the User's Guide.</p><h4>Module:end_per_suite(Config) -&gt; term() | 
	{save_config,SaveConfig}</h4><p>Test suite finalization.</p><ul><li><span class="v">Config = SaveConfig = [{Key,Value}]</span></li><li><span class="v">Key = atom()</span></li><li><span class="v">Value = term()</span></li></ul><p>OPTIONAL; if this function is defined, then <a href="#Module:init_per_suite-1">Module:init_per_suite-1</a>
must also be defined.</p><p>This function is called as the last test case in the
suite. It is meant to be used for cleaning up after
<a href="#Module:init_per_suite-1">Module:init_per_suite-1</a>.</p><p>For information on <strong>save_config</strong>, see section
<a href="./dependencies_chapter#save_config">Saving Configuration Data</a> in the User's Guide.</p><h4>Module:group(GroupName) -&gt; [Info]</h4><p>Test case group information function (providing default data for a test case group, that is, its test cases and subgroups).</p><ul><li><span class="v">Info = {timetrap,Time} | {require,Required} | {require,Name,Required} | {userdata,UserData} | {silent_connections,Conns} | {stylesheet,CSSFile} | {ct_hooks, CTHs}</span></li><li><span class="v">Time = TimeVal | TimeFunc</span></li><li><span class="v">TimeVal = MilliSec | {seconds,integer()} | {minutes,integer()} | {hours,integer()}</span></li><li><span class="v">TimeFunc = {Mod,Func,Args} | Fun</span></li><li><span class="v">MilliSec = integer()</span></li><li><span class="v">Mod = atom()</span></li><li><span class="v">Func = atom()</span></li><li><span class="v">Args = list()</span></li><li><span class="v">Fun = fun()</span></li><li><span class="v">Required = Key | {Key,SubKeys} | {Key,Subkey} | {Key,Subkey,SubKeys}</span></li><li><span class="v">Key = atom()</span></li><li><span class="v">SubKeys = SubKey | [SubKey]</span></li><li><span class="v">SubKey = atom()</span></li><li><span class="v">Name = atom()</span></li><li><span class="v">UserData = term()</span></li><li><span class="v">Conns = [atom()]</span></li><li><span class="v">CSSFile = string()</span></li><li><span class="v">CTHs = [CTHModule |</span></li><li><span class="v">Â Â Â Â Â Â Â Â {CTHModule, CTHInitArgs} |</span></li><li><span class="v">Â Â Â Â Â Â Â Â {CTHModule, CTHInitArgs, CTHPriority}]</span></li><li><span class="v">CTHModule = atom()</span></li><li><span class="v">CTHInitArgs = term()</span></li></ul><p>OPTIONAL</p><p>The test case group information function. It is supposed to
return a list of tagged tuples that specify various properties
related to the execution of a test case group (that is, its test
cases and subgroups). Properties set by
<a href="#Module:group-1">Module:group-1</a> override
properties with the same key that have been set previously by
<a href="#Module:suite-0">Module:suite-0</a>.</p><p>Tag <strong>timetrap</strong> sets the maximum time that each
test case is allowed to execute (including
<a href="#Module:init_per_testcase-2">Module:init_per_testcase-2</a>
and
<a href="#Module:end_per_testcase-2">Module:end_per_testcase-2</a>).
If the timetrap time is
exceeded, the test case fails with reason
<strong>timetrap_timeout</strong>. A <strong>TimeFunc</strong> function can be used to
set a new timetrap by returning a <strong>TimeVal</strong>. It can also be
used to trigger a timetrap time-out by, at some point, returning a
value other than a <strong>TimeVal</strong>. For details, see section
<a href="./write_test_chapter#timetraps">Timetrap Time-Outs</a> in the User's Guide.</p><p>Tag <strong>require</strong> specifies configuration variables
required by test cases (or configuration functions)
in the suite. If the required configuration variables are not found
in any of the configuration files, all test cases in this group are
skipped. For details about the <strong>require</strong> functionality, see
function
<a href="./ct#require-1">ct#require-1</a>.</p><p>With <strong>userdata</strong>, the user can
specify any test case group related information that can be 
read by calling
<a href="./ct#userdata-2">ct#userdata-2</a>.</p><p>Tag <strong>ct_hooks</strong> specifies the
<a href="ct_hooks_chapter">Common Test Hooks</a>
to be run with this suite.</p><p>Other tuples than the ones defined are ignored.</p><p>For details about the test case group information function,
see section <a href="./write_test_chapter#group_info">Group Information Function</a> in the User's Guide.</p><h4>Module:init_per_group(GroupName, Config) -&gt; NewConfig |
	  {skip,Reason}</h4><p>Test case group initializations.</p><ul><li><span class="v">GroupName = atom()</span></li><li><span class="v">Config = NewConfig = [{Key,Value}]</span></li><li><span class="v">Key = atom()</span></li><li><span class="v">Value = term()</span></li><li><span class="v">Reason = term()</span></li></ul><p>OPTIONAL; if this function is defined, then <a href="#Module:end_per_group-2">Module:end_per_group-2</a>
must also be defined.</p><p>This configuration function is called before execution of a
test case group. It typically contains initializations that are 
common for all test cases and subgroups in the group, and that
must only be performed once. <strong>GroupName</strong> is the name of the
group, as specified in the group definition (see
<a href="#Module:groups-0">Module:groups-0</a>).
Parameter <strong>Config</strong> is the configuration data that can be
modified.
The return value of this function is given as <strong>Config</strong>
to all test cases and subgroups in the group.</p><p>If <strong>{skip,Reason}</strong>
is returned, all test cases in the group are skipped and
<strong>Reason</strong> is printed in the overview log for the group.</p><p>For information about test case groups, see section 
<a href="./write_test_chapter#test_case_groups">Test Case  Groups</a> in the User's Guide.</p><h4>Module:end_per_group(GroupName, Config) -&gt; term() |
	  {return_group_result,Status}</h4><p>Test case group finalization.</p><ul><li><span class="v">GroupName = atom()</span></li><li><span class="v">Config = [{Key,Value}]</span></li><li><span class="v">Key = atom()</span></li><li><span class="v">Value = term()</span></li><li><span class="v">Status = ok | skipped | failed</span></li></ul><p>OPTIONAL; if this function is defined, then <a href="#Module:init_per_group-2">Module:init_per_group-2</a>
must also be defined.</p><p>This function is called after the execution of a test case group
is finished. It is meant to be used for cleaning up after
<a href="#Module:init_per_group-2">Module:init_per_group-2</a>.
A status value for a nested subgroup can be returned with
<strong>{return_group_result,Status}</strong>. The status can be retrieved in
<a href="#Module:end_per_group-2">Module:end_per_group-2</a>
for the group on the level above. The status is also used by
<strong>Common Test</strong> for deciding if execution of a group is to
proceed if property <strong>sequence</strong> or <strong>repeat_until_*</strong>
is set.</p><p>For details about test case groups, see section 
<a href="./write_test_chapter#test_case_groups">Test Case Groups</a> in the User's Guide.</p><h4>Module:init_per_testcase(TestCase, Config) -&gt; NewConfig | {fail,Reason} | {skip,Reason}</h4><p>Test case initializations.</p><ul><li><span class="v"> TestCase = atom()</span></li><li><span class="v"> Config = NewConfig = [{Key,Value}]</span></li><li><span class="v"> Key = atom()</span></li><li><span class="v"> Value = term()</span></li><li><span class="v"> Reason = term()</span></li></ul><p>OPTIONAL; if this function is defined,
then <a href="#Module:end_per_testcase-2">Module:end_per_testcase-2</a> must also be
defined.</p><p>This function is called before each test case. Argument
<strong>TestCase</strong> is the test case name, and
<strong>Config</strong> (list of key-value tuples) is the configuration
data that can be modified. The <strong>NewConfig</strong> list returned
from this function is given as <strong>Config</strong> to the test case.
If <strong>{fail,Reason}</strong> is returned, the test case is
marked as failed without being executed.</p><p>If <strong>{skip,Reason}</strong> is returned, the test case is skipped
and <strong>Reason</strong> is printed in the overview log for the suite.</p><h4>Module:end_per_testcase(TestCase, Config) -&gt; term() | {fail,Reason} | {save_config,SaveConfig}</h4><p>Test case finalization.</p><ul><li><span class="v">TestCase = atom()</span></li><li><span class="v">Config = SaveConfig = [{Key,Value}]</span></li><li><span class="v">Key = atom()</span></li><li><span class="v">Value = term()</span></li><li><span class="v">Reason = term()</span></li></ul><p>OPTIONAL; if this function is defined,
then <a href="#Module:init_per_testcase-2">Module:init_per_testcase-2</a> must also be
defined.</p><p>This function is called after each test case, and can be used
to clean up after
<a href="#Module:init_per_testcase-2">Module:init_per_testcase-2</a>
and the test case. Any return value (besides <strong>{fail,Reason}</strong>
and <strong>{save_config,SaveConfig}</strong>) is ignored. By returning
<strong>{fail,Reason}</strong>, <strong>TestCase</strong> is marked as faulty (even
though it was successful in the sense that it returned
a value instead of terminating).</p><p>For information on <strong>save_config</strong>, see section
<a href="./dependencies_chapter#save_config">Saving Configuration Data</a> in the User's Guide.</p><h4>Module:Testcase() -&gt; [Info]</h4><p>Test case information function.</p><ul><li><span class="v">Info = {timetrap,Time} | {require,Required} | {require,Name,Required} | {userdata,UserData} | {silent_connections,Conns}</span></li><li><span class="v">Time = TimeVal | TimeFunc</span></li><li><span class="v">TimeVal = MilliSec | {seconds,integer()} | {minutes,integer()} | {hours,integer()}</span></li><li><span class="v">TimeFunc = {Mod,Func,Args} | Fun</span></li><li><span class="v">MilliSec = integer()</span></li><li><span class="v">Mod = atom()</span></li><li><span class="v">Func = atom()</span></li><li><span class="v">Args = list()</span></li><li><span class="v">Fun = fun()</span></li><li><span class="v">Required = Key | {Key,SubKeys} | {Key,Subkey} | {Key,Subkey,SubKeys}</span></li><li><span class="v">Key = atom()</span></li><li><span class="v">SubKeys = SubKey | [SubKey]</span></li><li><span class="v">SubKey = atom()</span></li><li><span class="v">Name = atom()</span></li><li><span class="v">UserData = term()</span></li><li><span class="v">Conns = [atom()]</span></li></ul><p>OPTIONAL</p><p>The test case information function. It is supposed to
return a list of tagged tuples that specify various properties
related to the execution of this particular test case.
Properties set by
<a href="#Module:Testcase-0">Module:Testcase-0</a>
override properties set previously for the test case by
<a href="#Module:group-1">Module:group-1</a> or
<a href="#Module:suite-0">Module:suite-0</a>.</p><p>Tag <strong>timetrap</strong> sets the maximum time that the
test case is allowed to execute. If the timetrap time is
exceeded, the test case fails with reason <strong>timetrap_timeout</strong>.
<a href="#Module:init_per_testcase-2">Module:init_per_testcase-2</a>
and
<a href="#Module:end_per_testcase-2">Module:end_per_testcase-2</a>
are included in the timetrap time.
A <strong>TimeFunc</strong> function can be used to
set a new timetrap by returning a <strong>TimeVal</strong>. It can also be
used to trigger a timetrap time-out by, at some point, returning a
value other than a <strong>TimeVal</strong>. For details, see section
<a href="./write_test_chapter#timetraps">Timetrap Time-Outs</a> in the User's Guide.</p><p>Tag <strong>require</strong> specifies configuration variables
that are required by the test case (or <strong>init_per_testcase/2</strong>
or <strong>end_per_testcase/2</strong>).
If the required configuration variables are not found in any of the
configuration files, the test case is skipped. For details about
the <strong>require</strong> functionality, see function
<a href="./ct#require-1">ct#require-1</a>.</p><p>If <strong>timetrap</strong> or <strong>require</strong> is not set, the
default values specified by
<a href="#Module:suite-0">Module:suite-0</a> (or
<a href="#Module:group-1">Module:group-1</a>) are used.</p><p>With <strong>userdata</strong>, the user can specify any test case-related
information that can be read by calling
<a href="./ct#userdata-3">ct#userdata-3</a>.</p><p>Other tuples than the ones defined are ignored.</p><p>For details about the test case information function, see section
<a href="./write_test_chapter#info_function">Test Case Information Function</a> in the User's Guide.</p><h4>Module:Testcase(Config) -&gt;  term() | {skip,Reason} | {comment,Comment} | {save_config,SaveConfig} | {skip_and_save,Reason,SaveConfig} | exit()</h4><p>A test case.</p><ul><li><span class="v">Config = SaveConfig = [{Key,Value}]</span></li><li><span class="v">Key = atom()</span></li><li><span class="v">Value = term()</span></li><li><span class="v">Reason = term()</span></li><li><span class="v">Comment = string()</span></li></ul><p>MANDATORY</p><p>The implementation of a test case. Call the functions to test and
check the result. If something fails, ensure the
function causes a runtime error or call
<a href="./ct#fail-1">ct#fail-1</a>
(which also causes the test case process to terminate).</p><p>Elements from the <strong>Config</strong> list can, for example, be read
with <strong>proplists:get_value/2</strong> in STDLIB
(or the macro <strong>?config</strong> defined in <strong>ct.hrl</strong>).</p><p>If you decide not to run the test case after all, return
<strong>{skip,Reason}</strong>. <strong>Reason</strong> is then
printed in field <strong>Comment</strong> on the HTML result page.</p><p>To print some information in field <strong>Comment</strong> on the HTML
result page, return <strong>{comment,Comment}</strong>.</p><p>If the function returns anything else, the test case is
considered successful. The return value always gets printed
in the test case log file.</p><p>For details about test case implementation, see section
<a href="./write_test_chapter#test_cases">Test Cases</a>
in the User's Guide.</p><p>For information on <strong>save_config</strong> and <strong>skip_and_save</strong>,
see section
<a href="./dependencies_chapter#save_config">Saving Configuration Data</a> in the User's Guide.</p><h3>ct_run</h3><p>Program used for starting Common Test from the
    OS command line.</p><p>The <strong>ct_run</strong> program is automatically installed with Erlang/OTP
and the <strong>Common Test</strong> application (for more information, see
section <a href="install_chapter">Installation</a>
in the User's Guide). The program accepts different start flags.
Some flags trigger <strong>ct_run</strong> to start <strong>Common Test</strong> and
pass on data to it. Some flags start an Erlang node prepared for
running <strong>Common Test</strong> in a particular mode.The interface function
<a href="./ct#run_test-1">ct#run_test-1</a>,
corresponding to the <strong>ct_run</strong> program, is used for starting
<strong>Common Test</strong> from the Erlang shell (or an Erlang program).
For details, see the <a href="ct">ct</a>
manual page.<strong>ct_run</strong> also accepts Erlang emulator flags. These are used
when <strong>ct_run</strong> calls <strong>erl</strong> to start the Erlang node (this
makes it possible to add directories to the code server path,
change the cookie on the node, start more applications, and so on).With the optional flag <strong>-erl_args</strong>, options on the <strong>ct_run</strong>
command line can be divided into two groups:<ul><li>One group that <strong>Common Test</strong> is to process (those preceding <strong>-erl_args</strong>).</li><li>One group that <strong>Common Test</strong> is to ignore and pass on directly to the emulator (those following <strong>-erl_args</strong>).</li></ul>Options preceding <strong>-erl_args</strong> that <strong>Common Test</strong>
does not recognize are also passed on to the emulator untouched.
By <strong>-erl_args</strong> the user can specify flags with the same name, but
with different destinations, on the <strong>ct_run</strong> command line.If flags <strong>-pa</strong> or <strong>-pz</strong> are specified in the
<strong>Common Test</strong> group of options (preceding <strong>-erl_args</strong>),
relative directories are converted to absolute and reinserted into
the code path by <strong>Common Test</strong>. This is to avoid problems
loading user modules when <strong>Common Test</strong> changes working directory
during test runs. However, <strong>Common Test</strong> ignores flags <strong>-pa</strong>
and <strong>-pz</strong> following <strong>-erl_args</strong> on the command line. These
directories are added to the code path normally (that is, on specified
form).Exit status is set before the program ends. Value <strong>0</strong> indicates
a successful test result, <strong>1</strong> indicates one or more failed or
auto-skipped test cases, and <strong>2</strong> indicates test execution failure.If <strong>ct_run</strong> is called with option <strong>-help</strong>, it prints all
valid start flags to <strong>stdout</strong>.</p><a name="ct_run"></a><h4>Run Tests from Command Line</h4><pre>
 ct_run -dir TestDir1 TestDir2 .. TestDirN |
  [-dir TestDir] -suite Suite1 Suite2 .. SuiteN
  [-group Groups1 Groups2 .. GroupsN] [-case Case1 Case2 .. CaseN]
  [-step [config | keep_inactive]]
  [-config ConfigFile1 ConfigFile2 .. ConfigFileN]
  [-userconfig CallbackModule1 ConfigString1 and CallbackModule2
   ConfigString2 and .. CallbackModuleN ConfigStringN]
  [-decrypt_key Key] | [-decrypt_file KeyFile]
  [-label Label]
  [-logdir LogDir]
  [-logopts LogOpts]
  [-verbosity GenVLevel | [Category1 VLevel1 and
   Category2 VLevel2 and .. CategoryN VLevelN]]
  [-silent_connections [ConnType1 ConnType2 .. ConnTypeN]]
  [-stylesheet CSSFile]
  [-cover CoverCfgFile]
  [-cover_stop Bool]
  [-event_handler EvHandler1 EvHandler2 .. EvHandlerN] |
  [-event_handler_init EvHandler1 InitArg1 and
   EvHandler2 InitArg2 and .. EvHandlerN InitArgN]
  [-include InclDir1 InclDir2 .. InclDirN]
  [-no_auto_compile]
  [-abort_if_missing_suites]
  [-muliply_timetraps Multiplier]
  [-scale_timetraps]
  [-create_priv_dir auto_per_run | auto_per_tc | manual_per_tc]
  [-repeat N] |
  [-duration HHMMSS [-force_stop [skip_rest]]] |
  [-until [YYMoMoDD]HHMMSS [-force_stop [skip_rest]]]
  [-basic_html]
  [-no_esc_chars]
  [-keep_logs all | NLogs]
  [-ct_hooks CTHModule1 CTHOpts1 and CTHModule2 CTHOpts2 and ..
   CTHModuleN CTHOptsN]
  [-exit_status ignore_config]
  [-help]</pre><h4>Run Tests using Test Specification</h4><pre>
 ct_run -spec TestSpec1 TestSpec2 .. TestSpecN
  [-join_specs]
  [-config ConfigFile1 ConfigFile2 .. ConfigFileN]
  [-userconfig CallbackModule1 ConfigString1 and CallbackModule2
   ConfigString2 and .. and CallbackModuleN ConfigStringN]
  [-decrypt_key Key] | [-decrypt_file KeyFile]
  [-label Label]
  [-logdir LogDir]
  [-logopts LogOpts]
  [-verbosity GenVLevel | [Category1 VLevel1 and
   Category2 VLevel2 and .. CategoryN VLevelN]]
  [-allow_user_terms]
  [-silent_connections [ConnType1 ConnType2 .. ConnTypeN]]
  [-stylesheet CSSFile]
  [-cover CoverCfgFile]
  [-cover_stop Bool]
  [-event_handler EvHandler1 EvHandler2 .. EvHandlerN] |
  [-event_handler_init EvHandler1 InitArg1 and
   EvHandler2 InitArg2 and .. EvHandlerN InitArgN]
  [-include InclDir1 InclDir2 .. InclDirN]
  [-no_auto_compile]
  [-abort_if_missing_suites]
  [-muliply_timetraps Multiplier]
  [-scale_timetraps]
  [-create_priv_dir auto_per_run | auto_per_tc | manual_per_tc]
  [-repeat N] |
  [-duration HHMMSS [-force_stop [skip_rest]]] |
  [-until [YYMoMoDD]HHMMSS [-force_stop [skip_rest]]]
  [-basic_html]
  [-no_esc_chars]
  [-keep_logs all | NLogs]
  [-ct_hooks CTHModule1 CTHOpts1 and CTHModule2 CTHOpts2 and ..
   CTHModuleN CTHOptsN]
  [-exit_status ignore_config]</pre><h4>Run Tests in Web-Based GUI</h4><pre>
 ct_run -vts [-browser Browser]
  [-dir TestDir1 TestDir2 .. TestDirN] |
  [[dir TestDir] -suite Suite [[-group Group] [-case Case]]]
  [-config ConfigFile1 ConfigFile2 .. ConfigFileN]
  [-userconfig CallbackModule1 ConfigString1 and CallbackModule2
    ConfigString2 and .. and CallbackModuleN ConfigStringN]
  [-logopts LogOpts]
  [-verbosity GenVLevel | [Category1 VLevel1 and
   Category2 VLevel2 and .. CategoryN VLevelN]]
  [-decrypt_key Key] | [-decrypt_file KeyFile]
  [-include InclDir1 InclDir2 .. InclDirN]
  [-no_auto_compile]
  [-abort_if_missing_suites]
  [-muliply_timetraps Multiplier]
  [-scale_timetraps]
  [-create_priv_dir auto_per_run | auto_per_tc | manual_per_tc]
  [-basic_html]
  [-no_esc_chars]
  [-keep_logs all | NLogs]</pre><h4>Refresh HTML Index Files</h4><pre>
 ct_run -refresh_logs [-logdir LogDir] [-basic_html]
  [-keep_logs all | NLogs]</pre><h4>Run Common Test in Interactive Mode</h4><pre>
 ct_run -shell
  [-config ConfigFile1 ConfigFile2 ... ConfigFileN]
  [-userconfig CallbackModule1 ConfigString1 and CallbackModule2
   ConfigString2 and .. and CallbackModuleN ConfigStringN]
  [-decrypt_key Key] | [-decrypt_file KeyFile]</pre><h4>Start a Common Test Master Node</h4><pre>
 ct_run -ctmaster</pre><h4>See Also</h4><p>For information about the start flags, see section
<a href="run_test_chapter">Running Tests and Analyzing Results</a> in the User's Guide.</p><h3>ct</h3><p>Main user interface for the Common Test framework.</p><p>Main user interface for the <strong>Common Test</strong> framework.This module implements the command-line interface for running
tests and basic functions for <strong>Common Test</strong> case issues, such as
configuration and logging.<em>Test Suite Support Macros</em>The <strong>config</strong> macro is defined in <strong>ct.hrl</strong>. This macro is
to be used to retrieve information from the <strong>Config</strong> variable sent
to all test cases. It is used with two arguments; the first is the name
of the configuration variable to retrieve, the second is the
<strong>Config</strong> variable supplied to the test case.Possible configuration variables include:<ul><li><p><strong>data_dir</strong> - Data file directory</p></li><li><p><strong>priv_dir</strong> - Scratch file directory</p></li><li><p>Whatever added by
<a href="./common_test#Module:init_per_suite-1">common_test#Module:init_per_suite-1</a>
or
<a href="./common_test#Module:init_per_testcase-2">common_test#Module:init_per_testcase-2</a>
in the test suite.</p></li></ul></p><h4>Data Types</h4><a name="types"></a><dl><dt> <a name="type-handle"></a> <strong>handle() = pid()</strong> </dt><dd> <p>The identity (handle) of a connection.</p> </dd><dt> <a name="type-config_key"></a> <strong>config_key() = atom()</strong> </dt><dd> <p>A configuration key which exists in a configuration file</p> </dd><dt> <a name="type-target_name"></a><strong>target_name() = atom()</strong> </dt><dd> <p>A name and association to configuration data introduced
through a require statement, or a call to
<a href="#require-2">require-2</a>,
for example,
<strong>ct:require(mynodename,{node,[telnet]})</strong>.</p> </dd><dt> <a name="type-key_or_name"></a> <strong>key_or_name() = config_key() | target_name()</strong> </dt><dd></dd><dt> <a name="type-conn_log_options"></a> <strong>conn_log_options() = [conn_log_option()]</strong> </dt><dd> <p>Options that can be given to the <strong>cth_conn_log</strong> hook,
which is used for logging of NETCONF and Telnet
connections. See
<a href="./ct_netconfc#Logging">ct_netconfc</a>
or <a href="./ct_telnet#Logging">ct_telnet</a>
for description and examples of how to use this hook.</p> </dd><dt> <a name="type-conn_log_option"></a> <strong>conn_log_option() = {log_type,conn_log_type()} | {hosts,[key_or_name()]}</strong> </dt><dd></dd><dt> <a name="type-conn_log_type"></a> <strong>conn_log_type() = raw | pretty | html | silent</strong> </dt><dd></dd><dt> <a name="type-conn_log_mod"></a> <strong>conn_log_mod() = ct_netconfc | ct_telnet</strong> </dt><dd></dd></dl><h3>Functions</h3><h4>abort_current_testcase(Reason) -&gt; ok | {error, ErrorReason}</h4><p>Aborts the currently executing test case.</p><ul><li><span class="v">Reason = term()</span></li><li><span class="v">ErrorReason = no_testcase_running | parallel_group</span></li></ul><a name="abort_current_testcase-1"></a><p>Aborts the currently executing test case. The user must know with
certainty which test case is currently executing. The function is
therefore only safe to call from a function that has been called
(or synchronously invoked) by the test case.</p><p><strong>Reason</strong>, the reason for aborting the test case, is printed
in the test case log.</p><h4>add_config(Callback, Config) -&gt; ok | {error, Reason}</h4><p>Loads configuration variables using the specified callback module and configuration string.</p><ul><li><span class="v">Callback = atom()</span></li><li><span class="v">Config = string()</span></li><li><span class="v">Reason = term()</span></li></ul><a name="add_config-2"></a><p>Loads configuration variables using the specified callback module and
configuration string. The callback module is to be either loaded or
present in the code part. Loaded configuration variables can later
be removed using function
<a href="#remove_config-2">remove_config-2</a>.
</p><h4>break(Comment) -&gt; ok | {error, Reason}</h4><p>Cancels any active timetrap and pause the execution of the current test case until the user calls function continue/0.</p><ul><li><span class="v">Comment = string()</span></li><li><span class="v">Reason = {multiple_cases_running, TestCases} | 'enable break with release_shell option'</span></li><li><span class="v">TestCases = [atom()]</span></li></ul><a name="break-1"></a><p>Cancels any active timetrap and pauses the execution of the
current test case until the user calls function <strong>continue/0</strong>.
The user can then interact with the Erlang node running the tests,
for example, for debugging purposes or for manually executing a
part of the test case. If a parallel group is executing,
<a href="#break-2">break-2</a> is to be
called instead.</p><p>A cancelled timetrap is not automatically reactivated after the
break, but must be started exlicitly with
<a href="#timetrap-1">timetrap-1</a>.</p><p>In order for the break/continue functionality to work, <strong>Common Test</strong> must release the shell process controlling <strong>stdin</strong>.
This is done by setting start option <strong>release_shell</strong>
to <strong>true</strong>. For details, see section
<a href="./run_test_chapter#erlang_shell_or_program">Running Tests from the Erlang Shell or from an Erlang Program</a>
in the User's Guide.</p><h4>break(TestCase, Comment) -&gt; ok | {error, Reason}</h4><p>Works the same way as break/1, only argument TestCase makes it possible to pause a test case executing in a parallel group.</p><ul><li><span class="v">TestCase = atom()</span></li><li><span class="v">Comment = string()</span></li><li><span class="v">Reason = 'test case not running' | 'enable break with release_shell option'</span></li></ul><a name="break-2"></a><p>Works the same way as
<a href="#break-1">break-1</a>, only
argument <strong>TestCase</strong> makes it possible to pause a test case
executing in a parallel group. Function
<a href="#continue-1">continue-1</a> is to
be used to resume execution of <strong>TestCase</strong>.</p><p>For details, see
<a href="#break/1">break/1</a>.</p><h4>capture_get() -&gt; ListOfStrings</h4><p>Equivalent to capture_get([default]).</p><ul><li><span class="v">ListOfStrings = [string()]</span></li></ul><a name="capture_get-0"></a><p>Equivalent to
<a href="#capture_get-1">ct:capture_get([default])</a>.</p><h4>capture_get(ExclCategories) -&gt; ListOfStrings</h4><p>Returns and purges the list of text strings buffered during the latest session of capturing printouts to stdout.</p><ul><li><span class="v">ExclCategories = [atom()]</span></li><li><span class="v">ListOfStrings = [string()]</span></li></ul><a name="capture_get-1"></a><p>Returns and purges the list of text strings buffered during the
latest session of capturing printouts to <strong>stdout</strong>. Log
categories that are to be ignored in <strong>ListOfStrings</strong> can be
specified with <strong>ExclCategories</strong>.
If <strong>ExclCategories = []</strong>, no filtering takes place.</p><p>See also
<a href="#capture_start-0">capture_start-0</a>,
<a href="#capture_stop-0">capture_stop-0</a>,
<a href="#log-3">log-3</a>.</p><h4>capture_start() -&gt; ok</h4><p>Starts capturing all text strings printed to stdout during execution of the test case.</p><a name="capture_start-0"></a><p>Starts capturing all text strings printed to <strong>stdout</strong>
during execution of the test case.</p><p>See also
<a href="#capture_get-1">capture_get-1</a>,
<a href="#capture_stop-0">capture_stop-0</a>.</p><h4>capture_stop() -&gt; ok</h4><p>Stops capturing text strings (a session started with capture_start/0).</p><a name="capture_stop-0"></a><p>Stops capturing text strings (a session started with
<strong>capture_start/0</strong>).</p><p>See also
<a href="#capture_get-1">capture_get-1</a>,
<a href="#capture_start-0">capture_start-0</a>.</p><h4>comment(Comment) -&gt; ok</h4><p>Prints the specified Comment in the comment field in the table on the test suite result page.</p><ul><li><span class="v">Comment = term()</span></li></ul><a name="comment-1"></a><p>Prints the specified <strong>Comment</strong> in the comment field in the
table on the test suite result page.</p><p>If called several times, only the last comment is printed. The
test case return value <strong>{comment,Comment}</strong> overwrites the
string set by this function.</p><h4>comment(Format, Args) -&gt; ok</h4><p>Prints the formatted string in the comment field in the table on the test suite result page.</p><ul><li><span class="v">Format = string()</span></li><li><span class="v">Args = list()</span></li></ul><a name="comment-2"></a><p>Prints the formatted string in the comment field in the table
on the test suite result page.</p><p>Arguments <strong>Format</strong> and <strong>Args</strong> are used in a call to
<strong>io_lib:format/2</strong> to create the comment string. The behavior
of <strong>comment/2</strong> is otherwise the same as function
<a href="#comment-1">comment-1</a>.</p><h4>continue() -&gt; ok</h4><p>This function must be called to continue after a test case (not executing in a parallel group) has called break/1.</p><a name="continue-0"></a><p>This function must be called to continue after a test case
(not executing in a parallel group) has called function
<a href="#break-1">break-1</a>.</p><h4>continue(TestCase) -&gt; ok</h4><p>This function must be called to continue after a test case has called break/2.</p><ul><li><span class="v">TestCase = atom()</span></li></ul><a name="continue-1"></a><p>This function must be called to continue after a test case has
called <a href="#break-2">break-2</a>.
If the paused test case, <strong>TestCase</strong>, executes in a parallel
group, this function, rather than <strong>continue/0</strong>, must be used
to let the test case proceed.</p><h4>decrypt_config_file(EncryptFileName, TargetFileName) -&gt; ok | {error, Reason}</h4><p>Decrypts EncryptFileName, previously generated with encrypt_config_file/2,3.</p><ul><li><span class="v">EncryptFileName = string()</span></li><li><span class="v">TargetFileName = string()</span></li><li><span class="v">Reason = term()</span></li></ul><a name="decrypt_config_file-2"></a><p>Decrypts <strong>EncryptFileName</strong>, previously generated with
<a href="#encrypt_config_file-2">encrypt_config_file-2</a>.
The original file contents is saved in the target file. The
encryption key, a string, must be available in a text file named
<strong>.ct_config.crypt</strong>, either in the current directory, or the
home directory of the user (it is searched for in that order).</p><h4>decrypt_config_file(EncryptFileName, TargetFileName, KeyOrFile) -&gt; ok | {error, Reason}</h4><p>Decrypts EncryptFileName, previously generated with encrypt_config_file/2,3.</p><ul><li><span class="v">EncryptFileName = string()</span></li><li><span class="v">TargetFileName = string()</span></li><li><span class="v">KeyOrFile = {key, string()} | {file, string()}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="decrypt_config_file-3"></a><p>Decrypts <strong>EncryptFileName</strong>, previously generated with
<a href="#encrypt_config_file-2">encrypt_config_file-2</a>.
The original file contents is saved in the target file. The key
must have the same value as that used for encryption.</p><h4>encrypt_config_file(SrcFileName, EncryptFileName) -&gt; ok | {error, Reason}</h4><p>Encrypts the source configuration file with DES3 and saves the result in file EncryptFileName.</p><ul><li><span class="v">SrcFileName = string()</span></li><li><span class="v">EncryptFileName = string()</span></li><li><span class="v">Reason = term()</span></li></ul><a name="encrypt_config_file-2"></a><p>Encrypts the source configuration file with DES3 and saves the result
in file <strong>EncryptFileName</strong>. The key, a string, must be
available in a text file named <strong>.ct_config.crypt</strong>, either
in the current directory, or the home directory of the user (it
is searched for in that order).</p><p>For information about using encrypted configuration files when
running tests, see section
<a href="./config_file_chapter#encrypted_config_files">Encrypted Configuration Files</a> in the User's Guide.</p><p>For details on DES3 encryption/decryption, see application
<a href="./index">crypto/index</a>.</p><h4>encrypt_config_file(SrcFileName, EncryptFileName, KeyOrFile) -&gt; ok | {error, Reason}</h4><p>Encrypts the source configuration file with DES3 and saves the result in the target file EncryptFileName.</p><ul><li><span class="v">SrcFileName = string()</span></li><li><span class="v">EncryptFileName = string()</span></li><li><span class="v">KeyOrFile = {key, string()} | {file, string()}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="encrypt_config_file-3"></a><p>Encrypts the source configuration file with DES3 and saves the result
in the target file <strong>EncryptFileName</strong>. The encryption key
to use is either the value in <strong>{key,Key}</strong> or the value
stored in the file specified by <strong>{file,File}</strong>.</p><p>For information about using encrypted configuration files when
running tests, see section
<a href="./config_file_chapter#encrypted_config_files">Encrypted Configuration Files</a> in the User's Guide.</p><p>For details on DES3 encryption/decryption, see application
<a href="./index">crypto/index</a>.</p><h4>fail(Reason) -&gt; ok</h4><p>Terminates a test case with the specified error Reason.</p><ul><li><span class="v">Reason = term()</span></li></ul><a name="fail-1"></a><p>Terminates a test case with the specified error <strong>Reason</strong>.</p><h4>fail(Format, Args) -&gt; ok</h4><p>Terminates a test case with an error message specified by a format string and a list of values (used as arguments to io_lib:format/2).</p><ul><li><span class="v">Format = string()</span></li><li><span class="v">Args = list()</span></li></ul><a name="fail-2"></a><p>Terminates a test case with an error message specified by a
format string and a list of values (used as arguments to
<strong>io_lib:format/2</strong>).</p><h4>get_config(Required) -&gt; Value</h4><p>Equivalent to get_config(Required, undefined, []).</p><a name="get_config-1"></a><p>Equivalent to <a href="#get_config-3">get_config-3</a>.</p><h4>get_config(Required, Default) -&gt; Value</h4><p>Equivalent to get_config(Required, Default, []).</p><a name="get_config-2"></a><p>Equivalent to <a href="#get_config-3">get_config-3</a>.</p><h4>get_config(Required, Default, Opts) -&gt; ValueOrElement</h4><p>Reads configuration data values.</p><ul><li><span class="v">Required = KeyOrName | {KeyOrName, SubKey} | {KeyOrName, SubKey, SubKey}</span></li><li><span class="v">KeyOrName = atom()</span></li><li><span class="v">SubKey = atom()</span></li><li><span class="v">Default = term()</span></li><li><span class="v">Opts = [Opt] | []</span></li><li><span class="v">Opt = element | all</span></li><li><span class="v">ValueOrElement = term() | Default</span></li></ul><a name="get_config-3"></a><p>Reads configuration data values.</p><p>Returns the matching values or configuration elements, given a
configuration variable key or its associated name (if one has been
specified with
<a href="#require-2">require-2</a>
or a <strong>require</strong> statement).</p><p><em>Example:</em></p><p>Given the following configuration file:</p><pre>
 {unix,[{telnet,IpAddr},
        {user,[{username,Username},
               {password,Password}]}]}.</pre><p>Then:</p><pre>
 ct:get_config(unix,Default) -&gt; [{telnet,IpAddr}, 
  {user, [{username,Username}, {password,Password}]}]
 ct:get_config({unix,telnet},Default) -&gt; IpAddr
 ct:get_config({unix,user,username},Default) -&gt; Username
 ct:get_config({unix,ftp},Default) -&gt; Default
 ct:get_config(unknownkey,Default) -&gt; Default</pre><p>If a configuration variable key has been associated with a name (by
<a href="#require-2">require-2</a>
or a <strong>require</strong> statement), the name can be used instead
of the key to read the value:</p><pre>
 ct:require(myuser,{unix,user}) -&gt; ok.
 ct:get_config(myuser,Default) -&gt; [{username,Username}, {password,Password}]</pre><p>If a configuration variable is defined in multiple files, use option
<strong>all</strong> to access all possible values. The values are returned
in a list. The order of the elements corresponds to the order
that the configuration files were specified at startup.</p><p>If configuration elements (key-value tuples) are to be returned as
result instead of values, use option <strong>element</strong>. The
returned elements are then on the form <strong>{Required,Value}</strong>.</p><p>See also
<a href="#get_config-1">get_config-1</a>,
<a href="#get_config-2">get_config-2</a>,
<a href="#require-1">require-1</a>,
<a href="#require-2">require-2</a>.</p><h4>get_event_mgr_ref() -&gt; EvMgrRef</h4><p>Gets a reference to the <strong>Common Test</strong> event manager.</p><ul><li><span class="v">EvMgrRef = atom()</span></li></ul><a name="get_event_mgr_ref-0"></a><p>Gets a reference to the <strong>Common Test</strong> event manager.
The reference can be used to, for example, add a user-specific
event handler while tests are running.</p><p><em>Example:</em></p><pre>
 gen_event:add_handler(ct:get_event_mgr_ref(), my_ev_h, [])</pre><h4>get_progname() -&gt; string()</h4><p>Returns the command used to start this Erlang instance.</p><a name="get_progname-0"></a><p>Returns the command used to start this Erlang instance.
If this information could not be found, the string
<strong>"no_prog_name"</strong> is returned.</p><h4>get_status() -&gt; TestStatus | {error, Reason} | no_tests_running</h4><p>Returns status of ongoing test.</p><ul><li><span class="v">TestStatus = [StatusElem]</span></li><li><span class="v">StatusElem = {current, TestCaseInfo} | {successful, Successful} | {failed, Failed} | {skipped, Skipped} | {total, Total}</span></li><li><span class="v">TestCaseInfo = {Suite, TestCase} | [{Suite, TestCase}]</span></li><li><span class="v">Suite = atom()</span></li><li><span class="v">TestCase = atom()</span></li><li><span class="v">Successful = integer()</span></li><li><span class="v">Failed = integer()</span></li><li><span class="v">Skipped = {UserSkipped, AutoSkipped}</span></li><li><span class="v">UserSkipped = integer()</span></li><li><span class="v">AutoSkipped = integer()</span></li><li><span class="v">Total = integer()</span></li><li><span class="v">Reason = term()</span></li></ul><a name="get_status-0"></a><p>Returns status of ongoing test. The returned list contains
information about which test case is executing (a list of cases
when a parallel test case group is executing), as well as
counters for successful, failed, skipped, and total test cases
so far.</p><h4>get_target_name(Handle) -&gt; {ok, TargetName} | {error, Reason}</h4><p>Returns the name of the target that the specified connection belongs to.</p><ul><li><span class="v">Handle = handle()</span></li><li><span class="v">TargetName = target_name()</span></li></ul><a name="get_target_name-1"></a><p>Returns the name of the target that the specified connection
belongs to.</p><h4>get_testspec_terms() -&gt; TestSpecTerms | undefined</h4><p>Gets a list of all test specification terms used to configure and run this test.</p><ul><li><span class="v">TestSpecTerms = [{Tag, Value}]</span></li><li><span class="v">Value = [term()]</span></li></ul><a name="get_testspec_terms-0"></a><p>Gets a list of all test specification terms used to configure
and run this test.</p><h4>get_testspec_terms(Tags) -&gt; TestSpecTerms | undefined</h4><p>Reads one or more terms from the test specification used to configure and run this test.</p><ul><li><span class="v">Tags = [Tag] | Tag</span></li><li><span class="v">Tag = atom()</span></li><li><span class="v">TestSpecTerms = [{Tag, Value}] | {Tag, Value}</span></li><li><span class="v">Value = [{Node, term()}] | [term()]</span></li><li><span class="v">Node = atom()</span></li></ul><a name="get_testspec_terms-1"></a><p>Reads one or more terms from the test specification used to
configure and run this test. <strong>Tag</strong> is any valid test
specification tag, for example, <strong>label</strong>, <strong>config</strong>, or
<strong>logdir</strong>. User-specific terms are also available to read if
option <strong>allow_user_terms</strong> is set.</p><p>All value tuples returned, except user terms, have the node
name as first element.</p><p>To read test terms, use <strong>Tag = tests</strong> (rather than
<strong>suites</strong>, <strong>groups</strong>, or <strong>cases</strong>). <strong>Value</strong> is
then the list of <em>all</em> tests on the form
<strong>[{Node,Dir,[{TestSpec,GroupsAndCases1},...]},...]</strong>, where
<strong>GroupsAndCases = [{Group,[Case]}] | [Case]</strong>.</p><h4>get_timetrap_info() -&gt; {Time, {Scaling,ScaleVal}}</h4><p>Reads information about the timetrap set for the current test case.</p><ul><li><span class="v">Time = integer() | infinity</span></li><li><span class="v">Scaling = true | false</span></li><li><span class="v">ScaleVal = integer()</span></li></ul><a name="get_timetrap_info-0"></a><p>Reads information about the timetrap set for the current test
case. <strong>Scaling</strong> indicates if <strong>Common Test</strong> will attempt
to compensate timetraps automatically for runtime delays
introduced by, for example, tools like cover. <strong>ScaleVal</strong> is
the value of the current scaling multipler (always 1 if scaling is
disabled). Note the <strong>Time</strong> is not the scaled result.</p><h4>get_verbosity(Category) -&gt; Level | undefined</h4><p>Read the verbosity level for a logging category.</p><ul><li><span class="v">Category = default | atom()</span></li><li><span class="v">Level = integer()</span></li></ul><a name="get_verbosity-1"></a><p>This function returns the verbosity level for the specified logging
category. See the <a href="./write_test_chapter#logging"> User's Guide</a> for details. Use the value <strong>default</strong> to read
the general verbosity level.</p><h4>install(Opts) -&gt; ok | {error, Reason}</h4><p>Installs configuration files and event handlers.</p><ul><li><span class="v">Opts = [Opt]</span></li><li><span class="v">Opt = {config, ConfigFiles} | {event_handler, Modules} | {decrypt, KeyOrFile}</span></li><li><span class="v">ConfigFiles = [ConfigFile]</span></li><li><span class="v">ConfigFile = string()</span></li><li><span class="v">Modules = [atom()]</span></li><li><span class="v">KeyOrFile = {key, Key} | {file, KeyFile}</span></li><li><span class="v">Key = string()</span></li><li><span class="v">KeyFile = string()</span></li></ul><a name="install-1"></a><p>Installs configuration files and event handlers.</p><p>Run this function once before the first test.</p><p><em>Example:</em></p><pre>
 install([{config,["config_node.ctc","config_user.ctc"]}])</pre><p>This function is automatically run by program <strong>ct_run</strong>.</p><h4>listenv(Telnet) -&gt; [Env]</h4><p>Performs command listenv on the specified Telnet connection and returns the result as a list of key-value pairs.</p><ul><li><span class="v">Telnet = term()</span></li><li><span class="v">Env = {Key, Value}</span></li><li><span class="v">Key = string()</span></li><li><span class="v">Value = string()</span></li></ul><a name="listenv-1"></a><p>Performs command <strong>listenv</strong> on the specified Telnet connection
and returns the result as a list of key-value pairs.</p><h4>log(Format) -&gt; ok</h4><p>Equivalent to log(default, 50, Format, [], []).</p><a name="log-1"></a><p>Equivalent to
<a href="#log-5">log-5</a>.</p><h4>log(X1, X2) -&gt; ok</h4><p>Equivalent to log(Category, Importance, Format, FormatArgs, []).</p><ul><li><span class="v">X1 = Category | Importance | Format</span></li><li><span class="v">X2 = Format | FormatArgs</span></li></ul><a name="log-2"></a><p>Equivalent to <a href="#log-5">log-5</a>.</p><h4>log(X1, X2, X3) -&gt; ok</h4><p>Equivalent to log(Category, Importance, Format, FormatArgs, Opts).</p><ul><li><span class="v">X1 = Category | Importance</span></li><li><span class="v">X2 = Importance | Format</span></li><li><span class="v">X3 = Format | FormatArgs | Opts</span></li></ul><a name="log-3"></a><p>Equivalent to <a href="#log-5">log-5</a>.</p><h4>log(X1, X2, X3, X4) -&gt; ok</h4><p>Equivalent to log(Category, Importance, Format, FormatArgs, Opts).</p><ul><li><span class="v">X1 = Category | Importance</span></li><li><span class="v">X2 = Importance | Format</span></li><li><span class="v">X3 = Format | FormatArgs</span></li><li><span class="v">X4 = FormatArgs | Opts</span></li></ul><a name="log-4"></a><p>Equivalent to <a href="#log-5">log-5</a>.</p><h4>log(Category, Importance, Format, FormatArgs, Opts) -&gt; ok</h4><p>Prints from a test case to the log file.</p><ul><li><span class="v">Category = atom()</span></li><li><span class="v">Importance = integer()</span></li><li><span class="v">Format = string()</span></li><li><span class="v">FormatArgs = list()</span></li><li><span class="v">Opts = [Opt]</span></li><li><span class="v">Opt = {heading,string()} | no_css | esc_chars</span></li></ul><a name="log-5"></a><p>Prints from a test case to the log file.</p><p>This function is meant for printing a string directly from a
test case to the test case log file.</p><p>Default <strong>Category</strong> is <strong>default</strong>,
default <strong>Importance</strong> is <strong>?STD_IMPORTANCE</strong>,
and default value for <strong>FormatArgs</strong> is <strong>[]</strong>.</p><p>For details on <strong>Category</strong>, <strong>Importance</strong> and the <strong>no_css</strong>
option, see section <a href="./write_test_chapter#logging"> Logging - Categories and Verbosity Levels</a> in the User's Guide.</p><p>Common Test will not escape special HTML characters (&lt;, &gt; and &amp;)
in the text printed with this function, unless the <strong>esc_chars</strong>
option is used.</p><h4>make_priv_dir() -&gt; ok | {error, Reason}</h4><p>If the test has been started with option create_priv_dir set to manual_per_tc, in order for the test case to use the private directory, it must first create it by calling this function.</p><ul><li><span class="v">Reason = term()</span></li></ul><a name="make_priv_dir-0"></a><p>If the test is started with option <strong>create_priv_dir</strong>
set to <strong>manual_per_tc</strong>, in order for the test case to use
the private directory, it must first create it by calling this
function.</p><h4>notify(Name, Data) -&gt; ok</h4><p>Sends an asynchronous notification of type Name with Data to the <strong>Common Test</strong> event manager.</p><ul><li><span class="v">Name = atom()</span></li><li><span class="v">Data = term()</span></li></ul><a name="notify-2"></a><p>Sends an asynchronous notification of type <strong>Name</strong> with
<strong>Data</strong>to the Common Test event manager. This can later be
caught by any installed event manager.</p><p>See also
<a href="./gen_event">stdlib/gen_event</a>.</p><h4>pal(Format) -&gt; ok</h4><p>Equivalent to pal(default, 50, Format, [], []).</p><a name="pal-1"></a><p>Equivalent to
<a href="#pal-5">pal-5</a>.</p><h4>pal(X1, X2) -&gt; ok</h4><p>Equivalent to pal(Category, Importance, Format, FormatArgs, []).</p><ul><li><span class="v">X1 = Category | Importance | Format</span></li><li><span class="v">X2 = Format | FormatArgs</span></li></ul><a name="pal-2"></a><p>Equivalent to <a href="#pal-5">pal-5</a>.</p><h4>pal(X1, X2, X3) -&gt; ok</h4><p>Equivalent to pal(Category, Importance, Format, FormatArgs, Opts).</p><ul><li><span class="v">X1 = Category | Importance</span></li><li><span class="v">X2 = Importance | Format</span></li><li><span class="v">X3 = Format | FormatArgs | Opts</span></li></ul><a name="pal-3"></a><p>Equivalent to <a href="#pal-5">pal-5</a>.</p><h4>pal(X1, X2, X3, X4) -&gt; ok</h4><p>Equivalent to pal(Category, Importance, Format, FormatArgs, Opts).</p><ul><li><span class="v">X1 = Category | Importance</span></li><li><span class="v">X2 = Importance | Format</span></li><li><span class="v">X3 = Format | FormatArgs</span></li><li><span class="v">X4 = FormatArgs | Opts</span></li></ul><a name="pal-4"></a><p>Equivalent to <a href="#pal-5">pal-5</a>.</p><h4>pal(Category, Importance, Format, FormatArgs, Opts) -&gt; ok</h4><p>Prints and logs from a test case.</p><ul><li><span class="v">Category = atom()</span></li><li><span class="v">Importance = integer()</span></li><li><span class="v">Format = string()</span></li><li><span class="v">FormatArgs = list()</span></li><li><span class="v">Opts = [Opt]</span></li><li><span class="v">Opt = {heading,string()} | no_css</span></li></ul><a name="pal-5"></a><p>Prints and logs from a test case.</p><p>This function is meant for printing a string from a test case,
both to the test case log file and to the console.</p><p>Default <strong>Category</strong> is <strong>default</strong>,
default <strong>Importance</strong> is <strong>?STD_IMPORTANCE</strong>,
and default value for <strong>FormatArgs</strong> is <strong>[]</strong>.</p><p>For details on <strong>Category</strong> and <strong>Importance</strong>, see section
<a href="./write_test_chapter#logging">Logging - Categories and Verbosity Levels</a> in the User's Guide.</p><p>Note that special characters in the text (&lt;, &gt; and &amp;) will
be escaped by Common Test before the text is printed to the log
file.</p><h4>parse_table(Data) -&gt; {Heading, Table}</h4><p>Parses the printout from an SQL table and returns a list of tuples.</p><ul><li><span class="v">Data = [string()]</span></li><li><span class="v">Heading = tuple()</span></li><li><span class="v">Table = [tuple()]</span></li></ul><a name="parse_table-1"></a><p>Parses the printout from an SQL table and returns a list of
tuples.</p><p>The printout to parse is typically the result of a <strong>select</strong>
command in SQL. The returned <strong>Table</strong> is a list of tuples,
where each tuple is a row in the table.</p><p><strong>Heading</strong> is a tuple of strings representing the headings
of each column in the table.</p><h4>print(Format) -&gt; ok</h4><p>Equivalent to print(default, 50, Format, [], []).</p><a name="print-1"></a><p>Equivalent to <a href="#print-5">print-5</a>.</p><h4>print(X1, X2) -&gt; ok</h4><p>Equivalent to print(Category, Importance, Format, FormatArgs, []).</p><ul><li><span class="v">X1 = Category | Importance | Format</span></li><li><span class="v">X2 = Format | FormatArgs</span></li></ul><a name="print-2"></a><p>Equivalent to <a href="#print-5">print-5</a>.</p><h4>print(X1, X2, X3) -&gt; ok</h4><p>Equivalent to print(Category, Importance, Format, FormatArgs, Opts).</p><ul><li><span class="v">X1 = Category | Importance</span></li><li><span class="v">X2 = Importance | Format</span></li><li><span class="v">X3 = Format | FormatArgs | Opts</span></li></ul><a name="print-3"></a><p>Equivalent to <a href="#print-5">print-5</a>.</p><h4>print(X1, X2, X3, X4) -&gt; ok</h4><p>Equivalent to print(Category, Importance, Format, FormatArgs, Opts).</p><ul><li><span class="v">X1 = Category | Importance</span></li><li><span class="v">X2 = Importance | Format</span></li><li><span class="v">X3 = Format | FormatArgs</span></li><li><span class="v">X4 = FormatArgs | Opts</span></li></ul><a name="print-4"></a><p>Equivalent to <a href="#print-5">print-5</a>.</p><h4>print(Category, Importance, Format, FormatArgs, Opts) -&gt; ok</h4><p>Prints from a test case to the console.</p><ul><li><span class="v">Category = atom()</span></li><li><span class="v">Importance = integer()</span></li><li><span class="v">Format = string()</span></li><li><span class="v">FormatArgs = list()</span></li><li><span class="v">Opts = [Opt]</span></li><li><span class="v">Opt = {heading,string()}</span></li></ul><a name="print-5"></a><p>Prints from a test case to the console.</p><p>This function is meant for printing a string from a test case to
the console.</p><p>Default <strong>Category</strong> is <strong>default</strong>,
default <strong>Importance</strong> is <strong>?STD_IMPORTANCE</strong>,
and default value for <strong>FormatArgs</strong> is <strong>[]</strong>.</p><p>For details on <strong>Category</strong> and <strong>Importance</strong>, see section
<a href="./write_test_chapter#logging">Logging - Categories and Verbosity Levels</a> in the User's Guide.</p><h4>reload_config(Required) -&gt; ValueOrElement | {error, Reason}</h4><p>Reloads configuration file containing specified configuration key.</p><ul><li><span class="v">Required = KeyOrName | {KeyOrName, SubKey} | {KeyOrName, SubKey, SubKey}</span></li><li><span class="v">KeyOrName = atom()</span></li><li><span class="v">SubKey = atom()</span></li><li><span class="v">ValueOrElement = term()</span></li></ul><a name="reload_config-1"></a><p>Reloads configuration file containing specified configuration key.</p><p>This function updates the configuration data from which the
specified configuration variable was read, and returns the (possibly)
new value of this variable.</p><p>If some variables were present in the configuration, but are
not loaded using this function, they are removed from the
configuration table together with their aliases.</p><h4>remaining_test_procs() -&gt; {TestProcs,SharedGL,OtherGLs}</h4><p>&gt;This function will return the identity of test- and group leader processes that are still running at the time of this call.</p><ul><li><span class="v">TestProcs = [{pid(),GL}]</span></li><li><span class="v">GL = pid()</span></li><li><span class="v">SharedGL = pid()</span></li><li><span class="v">OtherGLs = [pid()]</span></li></ul><a name="remaining_test_procs-0"></a><p>This function will return the identity of test- and group
leader processes that are still running at the time of this call.
<strong>TestProcs</strong> are processes in the system that have a Common Test IO
process as group leader. <strong>SharedGL</strong> is the central Common Test
IO process, responsible for printing to log files for configuration
functions and sequentially executing test cases. <strong>OtherGLs</strong> are
Common Test IO processes that print to log files for test cases
in parallel test case groups.</p><p>The process information returned by this function may be
used to locate and terminate remaining processes after tests have
finished executing. The function would typically by called from
Common Test Hook functions.</p><p>Note that processes that execute configuration functions or
test cases are never included in <strong>TestProcs</strong>. It is therefore safe
to use post configuration hook functions (such as post_end_per_suite,
post_end_per_group, post_end_per_testcase) to terminate all processes
in <strong>TestProcs</strong> that have the current group leader process as its group
leader.</p><p>Note also that the shared group leader (<strong>SharedGL</strong>) must never be
terminated by the user, only by Common Test. Group leader processes
for parallel test case groups (<strong>OtherGLs</strong>) may however be terminated
in post_end_per_group hook functions.</p><h4>remove_config(Callback, Config) -&gt; ok</h4><p>Removes configuration variables (together with their aliases) that were loaded with specified callback module and configuration string.</p><ul><li><span class="v">Callback = atom()</span></li><li><span class="v">Config = string()</span></li><li><span class="v">Reason = term()</span></li></ul><a name="remove_config-2"></a><p>Removes configuration variables (together wih their aliases)
that were loaded with specified callback module and configuration
string.</p><h4>require(Required) -&gt; ok | {error, Reason}</h4><p>Checks if the required configuration is available.</p><ul><li><span class="v">Required = Key | {Key, SubKeys} | {Key, SubKey, SubKeys}</span></li><li><span class="v">Key = atom()</span></li><li><span class="v">SubKeys = SubKey | [SubKey]</span></li><li><span class="v">SubKey = atom()</span></li></ul><a name="require-1"></a><p>Checks if the required configuration is available. Arbitrarily
deep tuples can be specified as <strong>Required</strong>. Only the last
element of the tuple can be a list of <strong>SubKey</strong>s.</p><p><em>Example 1.</em> Require the variable <strong>myvar</strong>:</p><pre>
 ok = ct:require(myvar).</pre><p>In this case the configuration file must at least contain:</p><pre>
 {myvar,Value}.</pre><p><em>Example 2.</em> Require key <strong>myvar</strong> with subkeys
<strong>sub1</strong> and <strong>sub2</strong>:</p><pre>
 ok = ct:require({myvar,[sub1,sub2]}).</pre><p>In this case the configuration file must at least contain:</p><pre>
 {myvar,[{sub1,Value},{sub2,Value}]}.</pre><p><em>Example 3.</em> Require key <strong>myvar</strong> with subkey
<strong>sub1</strong> with <strong>subsub1</strong>:</p><pre>
 ok = ct:require({myvar,sub1,sub2}).</pre><p>In this case the configuration file must at least contain:</p><pre>
 {myvar,[{sub1,[{sub2,Value}]}]}.</pre><p>See also
<a href="#get_config-1">get_config-1</a>,
<a href="#get_config-2">get_config-2</a>,
<a href="#get_config-3">get_config-3</a>,
<a href="#require-2">require-2</a>.</p><h4>require(Name, Required) -&gt; ok | {error, Reason}</h4><p>Checks if the required configuration is available and gives it a name.</p><ul><li><span class="v">Name = atom()</span></li><li><span class="v">Required = Key | {Key, SubKey} | {Key, SubKey, SubKey}</span></li><li><span class="v">SubKey = Key</span></li><li><span class="v">Key = atom()</span></li></ul><a name="require-2"></a><p>Checks if the required configuration is available and gives it a
name. The semantics for <strong>Required</strong> is the same as in
<a href="#require-1">require-1</a> except
that a list of <strong>SubKey</strong>s cannot be specified.</p><p>If the requested data is available, the subentry is associated
with <strong>Name</strong> so that the value of the element can be read with
<a href="#get_config-1">get_config-1</a>
provided <strong>Name</strong> is used instead of the whole <strong>Required</strong>
term.</p><p><em>Example:</em></p><p>Require one node with a Telnet connection and an FTP connection.
Name the node <strong>a</strong>:</p><pre>
 ok = ct:require(a,{machine,node}).</pre><p>All references to this node can then use the node name. For
example, a file over FTP is fetched like follows:</p><pre>
 ok = ct:ftp_get(a,RemoteFile,LocalFile).</pre><p>For this to work, the configuration file must at least contain:</p><pre>
 {machine,[{node,[{telnet,IpAddr},{ftp,IpAddr}]}]}.</pre><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>The behavior of this function changed radically in
<strong>Common Test</strong> 1.6.2. To keep some backwards compatability,
it is still possible to do:<br/>
<strong>ct:require(a,{node,[telnet,ftp]}).</strong><br/>
This associates the name <strong>a</strong> with the top-level <strong>node</strong>
entry. For this to work, the configuration file must at least
contain:<br/>
<strong>{node,[{telnet,IpAddr},{ftp,IpAddr}]}.</strong></p></div><p>See also
<a href="#get_config-1">get_config-1</a>,
<a href="#get_config-2">get_config-2</a>,
<a href="#get_config-3">get_config-3</a>,
<a href="#require-1">require-1</a>.</p><h4>run(TestDirs) -&gt; Result</h4><p>Runs all test cases in all suites in the specified directories.</p><ul><li><span class="v">TestDirs = TestDir | [TestDir]</span></li></ul><a name="run-1"></a><p>Runs all test cases in all suites in the specified directories.</p><p>See also <a href="#run-3">run-3</a>.</p><h4>run(TestDir, Suite) -&gt; Result</h4><p>Runs all test cases in the specified suite.</p><a name="run-2"></a><p>Runs all test cases in the specified suite.</p><p>See also <a href="#run-3">run-3</a>.</p><h4>run(TestDir, Suite, Cases) -&gt; Result</h4><p>Runs the specified test cases.</p><ul><li><span class="v">TestDir = string()</span></li><li><span class="v">Suite = atom()</span></li><li><span class="v">Cases = atom() | [atom()]</span></li><li><span class="v">Result = [TestResult] | {error, Reason}</span></li></ul><a name="run-3"></a><p>Runs the specified test cases.</p><p>Requires that
<a href="#install-1">install-1</a> has been
run first.</p><p>Suites (<strong>*_SUITE.erl</strong>) files must be stored in <strong>TestDir</strong>
or <strong>TestDir/test</strong>. All suites are compiled when the test is
run.</p><h4>run_test(Opts) -&gt; Result</h4><p>Runs tests as specified by the combination of options in Opts.</p><ul><li><span class="v">Opts = [OptTuples]</span></li><li><span class="v">OptTuples = {dir, TestDirs} | {suite, Suites} | {group, Groups} | {testcase, Cases} | {spec, TestSpecs} | {join_specs, Bool} | {label, Label} | {config, CfgFiles} | {userconfig, UserConfig} | {allow_user_terms, Bool} | {logdir, LogDir} | {silent_connections, Conns} | {stylesheet, CSSFile} | {cover, CoverSpecFile} | {cover_stop, Bool} | {step, StepOpts} | {event_handler, EventHandlers} | {include, InclDirs} | {auto_compile, Bool} | {abort_if_missing_suites, Bool} | {create_priv_dir, CreatePrivDir} | {multiply_timetraps, M} | {scale_timetraps, Bool} | {repeat, N} | {duration, DurTime} | {until, StopTime} | {force_stop, ForceStop} | {decrypt, DecryptKeyOrFile} | {refresh_logs, LogDir} | {logopts, LogOpts} | {verbosity, VLevels} | {basic_html, Bool} | {esc_chars, Bool} | {keep_logs,KeepSpec} | {ct_hooks, CTHs} | {enable_builtin_hooks, Bool} | {release_shell, Bool}</span></li><li><span class="v">TestDirs = [string()] | string()</span></li><li><span class="v">Suites = [string()] | [atom()] | string() | atom()</span></li><li><span class="v">Cases = [atom()] | atom()</span></li><li><span class="v">Groups = GroupNameOrPath | [GroupNameOrPath]</span></li><li><span class="v">GroupNameOrPath = [atom()] | atom() | all</span></li><li><span class="v">TestSpecs = [string()] | string()</span></li><li><span class="v">Label = string() | atom()</span></li><li><span class="v">CfgFiles = [string()] | string()</span></li><li><span class="v">UserConfig = [{CallbackMod, CfgStrings}] | {CallbackMod, CfgStrings}</span></li><li><span class="v">CallbackMod = atom()</span></li><li><span class="v">CfgStrings = [string()] | string()</span></li><li><span class="v">LogDir = string()</span></li><li><span class="v">Conns = all | [atom()]</span></li><li><span class="v">CSSFile = string()</span></li><li><span class="v">CoverSpecFile = string()</span></li><li><span class="v">StepOpts = [StepOpt] | []</span></li><li><span class="v">StepOpt = config | keep_inactive</span></li><li><span class="v">EventHandlers = EH | [EH]</span></li><li><span class="v">EH = atom() | {atom(), InitArgs} | {[atom()], InitArgs}</span></li><li><span class="v">InitArgs = [term()]</span></li><li><span class="v">InclDirs = [string()] | string()</span></li><li><span class="v">CreatePrivDir = auto_per_run | auto_per_tc | manual_per_tc</span></li><li><span class="v">M = integer()</span></li><li><span class="v">N = integer()</span></li><li><span class="v">DurTime = string(HHMMSS)</span></li><li><span class="v">StopTime = string(YYMoMoDDHHMMSS) | string(HHMMSS)</span></li><li><span class="v">ForceStop = skip_rest | Bool</span></li><li><span class="v">DecryptKeyOrFile = {key, DecryptKey} | {file, DecryptFile}</span></li><li><span class="v">DecryptKey = string()</span></li><li><span class="v">DecryptFile = string()</span></li><li><span class="v">LogOpts = [LogOpt]</span></li><li><span class="v">LogOpt = no_nl | no_src</span></li><li><span class="v">VLevels = VLevel | [{Category, VLevel}]</span></li><li><span class="v">VLevel = integer()</span></li><li><span class="v">Category = atom()</span></li><li><span class="v">KeepSpec = all | pos_integer()</span></li><li><span class="v">CTHs = [CTHModule | {CTHModule, CTHInitArgs}]</span></li><li><span class="v">CTHModule = atom()</span></li><li><span class="v">CTHInitArgs = term()</span></li><li><span class="v">Result = {Ok, Failed, {UserSkipped, AutoSkipped}} | TestRunnerPid | {error, Reason}</span></li><li><span class="v">Ok = integer()</span></li><li><span class="v">Failed = integer()</span></li><li><span class="v">UserSkipped = integer()</span></li><li><span class="v">AutoSkipped = integer()</span></li><li><span class="v">TestRunnerPid = pid()</span></li><li><span class="v">Reason = term()</span></li></ul><a name="run_test-1"></a><p>Runs tests as specified by the combination of options in
<strong>Opts</strong>. The options are the same as those used with program
<strong>ct_run</strong>, see <a href="./ct_run#ct_run">Run Tests from Command Line</a> in the <strong>ct_run</strong> manual page.</p><p>Here a <strong>TestDir</strong> can be used to point out the path to a
<strong>Suite</strong>. Option <strong>testcase</strong> corresponds to option
<strong>-case</strong> in program <strong>ct_run</strong>. Configuration files
specified in <strong>Opts</strong> are installed automatically at startup.</p><p><strong>TestRunnerPid</strong> is returned if <strong>release_shell == true</strong>.
For details, see
<a href="#break-1">break-1</a>.</p><p><strong>Reason</strong> indicates the type of error encountered.</p><h4>run_testspec(TestSpec) -&gt; Result</h4><p>Runs a test specified by TestSpec.</p><ul><li><span class="v">TestSpec = [term()]</span></li><li><span class="v">Result = {Ok, Failed, {UserSkipped, AutoSkipped}} | {error, Reason}</span></li><li><span class="v">Ok = integer()</span></li><li><span class="v">Failed = integer()</span></li><li><span class="v">UserSkipped = integer()</span></li><li><span class="v">AutoSkipped = integer()</span></li><li><span class="v">Reason = term()</span></li></ul><a name="run_testspec-1"></a><p>Runs a test specified by <strong>TestSpec</strong>. The same terms are used
as in test specification files.</p><p><strong>Reason</strong> indicates the type of error encountered.</p><h4>set_verbosity(Category, Level) -&gt; ok</h4><p>Set the verbosity level for a logging category.</p><ul><li><span class="v">Category = default | atom()</span></li><li><span class="v">Level = integer()</span></li></ul><a name="set_verbosity-2"></a><p>Use this function to set, or modify, the verbosity level for a logging
category. See the <a href="./write_test_chapter#logging"> User's Guide</a> for details. Use the value <strong>default</strong> to set the
general verbosity level.</p><h4>sleep(Time) -&gt; ok</h4><p>This function, similar to timer:sleep/1, suspends the test case for a specified time.</p><ul><li><span class="v">Time = {hours, Hours} | {minutes, Mins} | {seconds, Secs} | Millisecs | infinity</span></li><li><span class="v">Hours = integer()</span></li><li><span class="v">Mins = integer()</span></li><li><span class="v">Secs = integer()</span></li><li><span class="v">Millisecs = integer() | float()</span></li></ul><a name="sleep-1"></a><p>This function, similar to <strong>timer:sleep/1</strong> in STDLIB,
suspends the test case for a specified time.
However, this function also multiplies <strong>Time</strong> with the
<strong>multiply_timetraps</strong> value (if set) and under certain
circumstances also scales up the time automatically if
<strong>scale_timetraps</strong> is set to <strong>true</strong> (default is
<strong>false</strong>).</p><h4>start_interactive() -&gt; ok</h4><p>Starts <strong>Common Test</strong> in interactive mode.</p><a name="start_interactive-0"></a><p>Starts <strong>Common Test</strong> in interactive mode.</p><p>From this mode, all test case support functions can be executed
directly from the Erlang shell. The interactive mode can also be
started from the OS command line with <strong>ct_run -shell [-config File...]</strong>.</p><p>If any functions (for example, Telnet or FTP) using
"required configuration data" are to be called from the Erlang shell,
configuration data must first be required with
<a href="#require-2">require-2</a>.</p><p><em>Example:</em></p><pre>
 &gt; ct:require(unix_telnet, unix).
 ok
 &gt; ct_telnet:open(unix_telnet).
 {ok,&lt;0.105.0&gt;}
 &gt; ct_telnet:cmd(unix_telnet, "ls .").
 {ok,["ls","file1  ...",...]}</pre><h4>step(TestDir, Suite, Case) -&gt; Result</h4><p>Steps through a test case with the debugger.</p><ul><li><span class="v">Case = atom()</span></li></ul><a name="step-3"></a><p>Steps through a test case with the debugger.</p><p>See also <a href="#run-3">run-3</a>.</p><h4>step(TestDir, Suite, Case, Opts) -&gt; Result</h4><p>Steps through a test case with the debugger.</p><ul><li><span class="v">Case = atom()</span></li><li><span class="v">Opts = [Opt] | []</span></li><li><span class="v">Opt = config | keep_inactive</span></li></ul><a name="step-4"></a><p>Steps through a test case with the debugger. If option
<strong>config</strong> has been specifed, breakpoints are also set on
the configuration functions in <strong>Suite</strong>.</p><p>See also <a href="#run-3">run-3</a>.</p><h4>stop_interactive() -&gt; ok</h4><p>Exits the interactive mode.</p><a name="stop_interactive-0"></a><p>Exits the interactive mode.</p><p>See also
<a href="#start_interactive-0">start_interactive-0</a>.
</p><h4>sync_notify(Name, Data) -&gt; ok</h4><p>Sends a synchronous notification of type Name with Data to the <strong>Common Test</strong> event manager.</p><ul><li><span class="v">Name = atom()</span></li><li><span class="v">Data = term()</span></li></ul><a name="sync_notify-2"></a><p>Sends a synchronous notification of type <strong>Name</strong> with
<strong>Data</strong> to the <strong>Common Test</strong> event manager. This can later be
caught by any installed event manager.</p><p>See also
<a href="./gen_event">stdlib/gen_event</a>.
</p><h4>testcases(TestDir, Suite) -&gt; Testcases | {error, Reason}</h4><p>Returns all test cases in the specified suite.</p><ul><li><span class="v">TestDir = string()</span></li><li><span class="v">Suite = atom()</span></li><li><span class="v">Testcases = list()</span></li><li><span class="v">Reason = term()</span></li></ul><a name="testcases-2"></a><p>Returns all test cases in the specified suite.</p><h4>timetrap(Time) -&gt; ok</h4><p>Sets a new timetrap for the running test case.</p><ul><li><span class="v">Time = {hours, Hours} | {minutes, Mins} | {seconds, Secs} | Millisecs | infinity | Func</span></li><li><span class="v">Hours = integer()</span></li><li><span class="v">Mins = integer()</span></li><li><span class="v">Secs = integer()</span></li><li><span class="v">Millisecs = integer()</span></li><li><span class="v">Func = {M, F, A} | function()</span></li><li><span class="v">M = atom()</span></li><li><span class="v">F = atom()</span></li><li><span class="v">A = list()</span></li></ul><a name="timetrap-1"></a><p>Sets a new timetrap for the running test case.</p><p>If the argument is <strong>Func</strong>, the timetrap is triggered when
this function returns. <strong>Func</strong> can also return a new
<strong>Time</strong> value, which in that case is the value for the new
timetrap.</p><h4>userdata(TestDir, Suite) -&gt; SuiteUserData | {error, Reason}</h4><p>Returns any data specified with tag userdata in the list of tuples returned from Suite:suite/0.</p><ul><li><span class="v">TestDir = string()</span></li><li><span class="v">Suite = atom()</span></li><li><span class="v">SuiteUserData = [term()]</span></li><li><span class="v">Reason = term()</span></li></ul><a name="userdata-2"></a><p>Returns any data specified with tag <strong>userdata</strong> in the list
of tuples returned from
<a href="./common_test#Module:suite-0">common_test#Module:suite-0</a>.</p><h4>userdata(TestDir, Suite, Case::GroupOrCase) -&gt; TCUserData | {error, Reason}</h4><p>Returns any data specified with tag userdata in the list of tuples returned from Suite:group(GroupName) or Suite:Case().</p><ul><li><span class="v">TestDir = string()</span></li><li><span class="v">Suite = atom()</span></li><li><span class="v">GroupOrCase = {group, GroupName} | atom()</span></li><li><span class="v">GroupName = atom()</span></li><li><span class="v">TCUserData = [term()]</span></li><li><span class="v">Reason = term()</span></li></ul><a name="userdata-3"></a><p>Returns any data specified with tag <strong>userdata</strong> in the list
of tuples returned from <strong>Suite:group(GroupName)</strong> or
<strong>Suite:Case()</strong>.</p><h3>ct_master</h3><p>Distributed test execution control for Common Test.</p><p>Distributed test execution control for <strong>Common Test</strong>.This module exports functions for running <strong>Common Test</strong> nodes on
multiple hosts in parallel.</p><h3>Functions</h3><h4>abort() -&gt; ok</h4><p>Stops all running tests.</p><a name="abort-0"></a><p>Stops all running tests.</p><h4>abort(Nodes) -&gt; ok</h4><p>Stops tests on specified nodes.</p><ul><li><span class="v">Nodes = atom() | [atom()]</span></li></ul><a name="abort-1"></a><p>Stops tests on specified nodes.</p><h4>basic_html(Bool) -&gt; ok</h4><p>If set to true, the ct_master logs are written on a primitive HTML format, not using the <strong>Common Test</strong> CSS style sheet.</p><ul><li><span class="v">Bool = true | false</span></li></ul><a name="basic_html-1"></a><p>If set to <strong>true</strong>, the <strong>ct_master logs</strong> are written on a
primitive HTML format, not using the <strong>Common Test</strong> CSS style
sheet.</p><h4>get_event_mgr_ref() -&gt; MasterEvMgrRef</h4><p>Gets a reference to the <strong>Common Test</strong> master event manager.</p><ul><li><span class="v">MasterEvMgrRef = atom()</span></li></ul><a name="get_event_mgr_ref-0"></a><p>Gets a reference to the <strong>Common Test</strong> master event manager.
The reference can be used to, for example, add a user-specific
event handler while tests are running.</p><p><em>Example:</em></p><pre>
 gen_event:add_handler(ct_master:get_event_mgr_ref(), my_ev_h, [])</pre><h4>progress() -&gt; [{Node, Status}]</h4><p>Returns test progress.</p><ul><li><span class="v">Node = atom()</span></li><li><span class="v">Status = finished_ok | ongoing | aborted | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="progress-0"></a><p>Returns test progress. If <strong>Status</strong> is <strong>ongoing</strong>, tests
are running on the node and are not yet finished.</p><h4>run(TestSpecs) -&gt; ok</h4><p>Equivalent to run(TestSpecs, false, [], []).</p><ul><li><span class="v">TestSpecs = string() | [SeparateOrMerged]</span></li></ul><a name="run-1"></a><p>Equivalent to <a href="#run-4">run-4</a>.</p><h4>run(TestSpecs, InclNodes, ExclNodes) -&gt; ok</h4><p>Equivalent to run(TestSpecs, false, InclNodes, ExclNodes). </p><ul><li><span class="v">TestSpecs = string() | [SeparateOrMerged]</span></li><li><span class="v">SeparateOrMerged = string() | [string()]</span></li><li><span class="v">InclNodes = [atom()]</span></li><li><span class="v">ExclNodes = [atom()]</span></li></ul><a name="run-3"></a><p>Equivalent to <a href="#run-4">run-4</a>.</p><h4>run(TestSpecs, AllowUserTerms, InclNodes, ExclNodes) -&gt; ok</h4><p>Tests are spawned on the nodes as specified in TestSpecs. </p><ul><li><span class="v">TestSpecs = string() | [SeparateOrMerged]</span></li><li><span class="v">SeparateOrMerged = string() | [string()]</span></li><li><span class="v">AllowUserTerms = bool()</span></li><li><span class="v">InclNodes = [atom()]</span></li><li><span class="v">ExclNodes = [atom()]</span></li></ul><a name="run-4"></a><p>Tests are spawned on the nodes as specified in <strong>TestSpecs</strong>.
Each specification in <strong>TestSpec</strong> is handled separately.
However, it is also possible to specify a list of specifications to
be merged into one specification before the tests are executed. Any
test without a particular node specification is also executed on
the nodes in <strong>InclNodes</strong>. Nodes in the <strong>ExclNodes</strong> list
are excluded from the test.</p><h4>run_on_node(TestSpecs, Node) -&gt; ok</h4><p>Equivalent to run_on_node(TestSpecs, false, Node).</p><ul><li><span class="v">TestSpecs = string() | [SeparateOrMerged]</span></li><li><span class="v">SeparateOrMerged = string() | [string()]</span></li><li><span class="v">Node = atom()</span></li></ul><a name="run_on_node-2"></a><p>Equivalent to
<a href="#run_on_node-3">run_on_node-3</a>.</p><h4>run_on_node(TestSpecs, AllowUserTerms, Node) -&gt; ok</h4><p>Tests are spawned on Node according to TestSpecs.</p><ul><li><span class="v">TestSpecs = string() | [SeparateOrMerged]</span></li><li><span class="v">SeparateOrMerged = string() | [string()]</span></li><li><span class="v">AllowUserTerms = bool()</span></li><li><span class="v">Node = atom()</span></li></ul><a name="run_on_node-3"></a><p>Tests are spawned on <strong>Node</strong> according to <strong>TestSpecs</strong>.</p><h4>run_test(Node, Opts) -&gt; ok</h4><p>Tests are spawned on Node using ct:run_test/1.</p><ul><li><span class="v">Node = atom()</span></li><li><span class="v">Opts = [OptTuples]</span></li><li><span class="v">OptTuples = {config, CfgFiles} | {dir, TestDirs} | {suite, Suites} | {testcase, Cases} | {spec, TestSpecs} | {allow_user_terms, Bool} | {logdir, LogDir} | {event_handler, EventHandlers} | {silent_connections, Conns} | {cover, CoverSpecFile} | {cover_stop, Bool} | {userconfig, UserCfgFiles}</span></li><li><span class="v">CfgFiles = string() | [string()]</span></li><li><span class="v">TestDirs = string() | [string()]</span></li><li><span class="v">Suites = atom() | [atom()]</span></li><li><span class="v">Cases = atom() | [atom()]</span></li><li><span class="v">TestSpecs = string() | [string()]</span></li><li><span class="v">LogDir = string()</span></li><li><span class="v">EventHandlers = EH | [EH]</span></li><li><span class="v">EH = atom() | {atom(), InitArgs} | {[atom()], InitArgs}</span></li><li><span class="v">InitArgs = [term()]</span></li><li><span class="v">Conns = all | [atom()]</span></li></ul><a name="run_test-2"></a><p>Tests are spawned on <strong>Node</strong> using
<a href="./ct#run_test-1">ct#run_test-1</a></p><h3>ct_cover</h3><p>Common Test framework code coverage support module.
</p><p><strong>Common Test</strong> framework code coverage support module.This module exports help functions for performing code coverage
analysis.</p><h3>Functions</h3><h4>add_nodes(Nodes) -&gt; {ok, StartedNodes} | {error, Reason}</h4><p>Adds nodes to current cover test (only works if cover support is active).</p><ul><li><span class="v">Nodes = [atom()]</span></li><li><span class="v">StartedNodes = [atom()]</span></li><li><span class="v">Reason = cover_not_running | not_main_node</span></li></ul><a name="add_nodes-1"></a><p>Adds nodes to current cover test. Notice that this only works if
cover support is active.</p><p>To have effect, this function is to be called from
<strong>init_per_suite/1</strong> (see
<a href="common_test">common_test</a>)
before any tests are performed.</p><h4>cross_cover_analyse(Level, Tests) -&gt; ok</h4><p>Accumulates cover results over multiple tests.</p><ul><li><span class="v">Level = overview | details</span></li><li><span class="v">Tests = [{Tag, Dir}]</span></li><li><span class="v">Tag = atom()</span></li><li><span class="v">Dir = string()</span></li></ul><a name="cross_cover_analyse-2"></a><p>Accumulates cover results over multiple tests. See section
<a href="./cover_chapter#cross_cover">Cross Cover Analysis</a> in the Users's Guide.</p><h4>remove_nodes(Nodes) -&gt; ok | {error, Reason}</h4><p>Removes nodes from the current cover test.</p><ul><li><span class="v">Nodes = [atom()]</span></li><li><span class="v">Reason = cover_not_running | not_main_node</span></li></ul><a name="remove_nodes-1"></a><p>Removes nodes from the current cover test.</p><p>Call this function to stop cover test on nodes previously
added with
<a href="#add_nodes-1">add_nodes-1</a>.
Results on the remote node are transferred to the <strong>Common Test</strong>
node.</p><h3>ct_ftp</h3><p>FTP client module (based on the FTP application).</p><p>FTP client module (based on the <strong>ftp</strong> application).</p><h4>Data Types</h4><a name="types"></a><dl><dt><strong>connection() = handle() | target_name()</strong></dt><dd><a name="type-connection"></a> <p>For <strong>target_name</strong>, see module
<a href="ct">ct</a>.</p></dd><dt><strong>handle() = handle()</strong></dt><dd><a name="type-handle"></a> <p>Handle for a specific FTP connection, see module
<a href="ct">ct</a>.</p></dd></dl><h3>Functions</h3><h4>cd(Connection, Dir) -&gt; ok | {error, Reason}</h4><p>Changes directory on remote host.</p><ul><li><span class="v">Connection = connection()</span></li><li><span class="v">Dir = string()</span></li></ul><a name="cd-2"></a><p>Changes directory on remote host.</p><h4>close(Connection) -&gt; ok | {error, Reason}</h4><p>Closes the FTP connection.</p><ul><li><span class="v">Connection = connection()</span></li></ul><a name="close-1"></a><p>Closes the FTP connection.</p><h4>delete(Connection, File) -&gt; ok | {error, Reason}</h4><p>Deletes a file on remote host.</p><ul><li><span class="v">Connection = connection()</span></li><li><span class="v">File = string()</span></li></ul><a name="delete-2"></a><p>Deletes a file on remote host.</p><h4>get(KeyOrName, RemoteFile, LocalFile) -&gt; ok | {error, Reason}</h4><p>Opens an FTP connection and fetches a file from the remote host.</p><ul><li><span class="v">KeyOrName = Key | Name</span></li><li><span class="v">Key = atom()</span></li><li><span class="v">Name = target_name()</span></li><li><span class="v">RemoteFile = string()</span></li><li><span class="v">LocalFile = string()</span></li></ul><a name="get-3"></a><p>Opens an FTP connection and fetches a file from the remote
host.</p><p><strong>RemoteFile</strong> and <strong>LocalFile</strong> must be absolute paths.</p><p>The configuration file must be as for
<a href="#put-3">put-3</a>.</p><p>For <strong>target_name</strong>, see module
<a href="ct">ct</a>.</p><p>See also
<a href="./ct#require-2">ct#require-2</a>.</p><h4>ls(Connection, Dir) -&gt; {ok, Listing} | {error, Reason}</h4><p>Lists directory Dir.</p><ul><li><span class="v">Connection = connection()</span></li><li><span class="v">Dir = string()</span></li><li><span class="v">Listing = string()</span></li></ul><a name="ls-2"></a><p>Lists directory <strong>Dir</strong>.</p><h4>open(KeyOrName) -&gt; {ok, Handle} | {error, Reason}</h4><p>Opens an FTP connection to the specified node.</p><ul><li><span class="v">KeyOrName = Key | Name</span></li><li><span class="v">Key = atom()</span></li><li><span class="v">Name = target_name()</span></li><li><span class="v">Handle = handle()</span></li></ul><a name="open-1"></a><p>Opens an FTP connection to the specified node.</p><p>You can open a connection for a particular <strong>Name</strong> and use the
same name as reference for all following subsequent operations.
If you want
the connection to be associated with <strong>Handle</strong> instead (if you,
for example, need to open multiple connections to a host), use
<strong>Key</strong>, the configuration variable name, to specify the target.
A connection without an associated target name can only be closed
with the handle value.</p><p>For information on how to create a new <strong>Name</strong>, see
<a href="./ct#require-2">ct#require-2</a>.</p><p>For <strong>target_name</strong>, see module
<a href="ct">ct</a>.</p><h4>put(KeyOrName, LocalFile, RemoteFile) -&gt; ok | {error, Reason}</h4><p>Opens an FTP connection and sends a file to the remote host.</p><ul><li><span class="v">KeyOrName = Key | Name</span></li><li><span class="v">Key = atom()</span></li><li><span class="v">Name = target_name()</span></li><li><span class="v">LocalFile = string()</span></li><li><span class="v">RemoteFile = string()</span></li></ul><a name="put-3"></a><p>Opens an FTP connection and sends a file to the remote host.</p><p><strong>LocalFile</strong> and <strong>RemoteFile</strong> must be absolute paths.</p><p>For <strong>target_name</strong>, see module
<a href="ct">ct</a>.</p><p>If the target host is a "special" node, the FTP address must be
specified in the configuration file as follows:</p><pre>
 {node,[{ftp,IpAddr}]}.</pre><p>If the target host is something else, for example, a UNIX host,
the configuration file must also include the username and password
(both strings):</p><pre>
 {unix,[{ftp,IpAddr},
        {username,Username},
        {password,Password}]}.</pre><p>See also
<a href="./ct#require-2">ct#require-2</a>.</p><h4>recv(Connection, RemoteFile) -&gt; ok | {error, Reason}</h4><p>Fetches a file over FTP.</p><a name="recv-2"></a><p>Fetches a file over FTP.</p><p>The file gets the same name on the local host.</p><p>See also <a href="#recv-3">recv-3</a>.</p><h4>recv(Connection, RemoteFile, LocalFile) -&gt; ok | {error, Reason}</h4><p>Fetches a file over FTP.</p><ul><li><span class="v">Connection = connection()</span></li><li><span class="v">RemoteFile = string()</span></li><li><span class="v">LocalFile = string()</span></li></ul><a name="recv-3"></a><p>Fetches a file over FTP.</p><p>The file is named <strong>LocalFile</strong> on the local host.</p><h4>send(Connection, LocalFile) -&gt; ok | {error, Reason}</h4><p>Sends a file over FTP.</p><a name="send-2"></a><p>Sends a file over FTP.</p><p>The file gets the same name on the remote host.</p><p>See also
<a href="#send-3">send-3</a>.</p><h4>send(Connection, LocalFile, RemoteFile) -&gt; ok | {error, Reason}</h4><p>Sends a file over FTP.</p><ul><li><span class="v">Connection = connection()</span></li><li><span class="v">LocalFile = string()</span></li><li><span class="v">RemoteFile = string()</span></li></ul><a name="send-3"></a><p>Sends a file over FTP.</p><p>The file is named <strong>RemoteFile</strong> on the remote host.</p><h4>type(Connection, Type) -&gt; ok | {error, Reason}</h4><p>Changes the file transfer type.</p><ul><li><span class="v">Connection = connection()</span></li><li><span class="v">Type = ascii | binary</span></li></ul><a name="type-2"></a><p>Changes the file transfer type.</p><h3>ct_ssh</h3><p>SSH/SFTP client module.</p><p>SSH/SFTP client module.This module uses application <strong>SSH</strong>, which provides detailed
information about, for example, functions, types, and options.Argument <strong>Server</strong> in the SFTP functions is only to be used for
SFTP sessions that have been started on existing SSH connections
(that is, when the original connection type is <strong>ssh</strong>). Whenever
the connection type is <strong>sftp</strong>, use the SSH connection reference
only.The following options are valid for specifying an SSH/SFTP
connection (that is, can be used as configuration elements):<pre>
 [{ConnType, Addr},
  {port, Port},
  {user, UserName}
  {password, Pwd}
  {user_dir, String}
  {public_key_alg, PubKeyAlg}
  {connect_timeout, Timeout}
  {key_cb, KeyCallbackMod}]</pre><strong>ConnType = ssh | sftp</strong>.For other types, see
<a href="./ssh">ssh/ssh</a>.All time-out parameters in <strong>ct_ssh</strong> functions are values in
milliseconds.</p><h4>Data Types</h4><a name="types"></a><dl><dt><strong>connection() = handle() | target_name()</strong></dt><dd><a name="type-connection"></a> <p>For <strong>target_name</strong>, see module
<a href="ct">ct</a>.</p></dd><dt><strong>handle() = handle()</strong></dt><dd><a name="type-handle"></a> <p>Handle for a specific SSH/SFTP connection, see module
<a href="ct">ct</a>.</p></dd><dt><strong>ssh_sftp_return() = term()</strong></dt><dd><a name="type-ssh_sftp_return"></a> <p>Return value from an
<a href="./ssh_sftp">ssh/ssh_sftp</a>
function.</p></dd></dl><h3>Functions</h3><h4>apread(SSH, Handle, Position, Length) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="apread-4"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>apread(SSH, Server, Handle, Position, Length) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="apread-5"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>apwrite(SSH, Handle, Position, Data) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="apwrite-4"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>apwrite(SSH, Server, Handle, Position, Data) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="apwrite-5"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>aread(SSH, Handle, Len) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="aread-3"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>aread(SSH, Server, Handle, Len) -&gt; Result</h4><p>For inforamtion and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="aread-4"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>awrite(SSH, Handle, Data) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="awrite-3"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>awrite(SSH, Server, Handle, Data) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="awrite-4"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>close(SSH, Handle) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="close-2"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>close(SSH, Server, Handle) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="close-3"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>connect(KeyOrName) -&gt; {ok, Handle} | {error, Reason}</h4><p>Equivalent to connect(KeyOrName, host, []).</p><a name="connect-1"></a><p>Equivalent to
<a href="#connect-3">connect-3</a>.</p><h4>connect(KeyOrName, ConnType) -&gt; {ok, Handle} | {error, Reason}</h4><p>Equivalent to connect(KeyOrName, ConnType, []).</p><a name="connect-2"></a><p>Equivalent to
<a href="#connect-3">connect-3</a>.</p><h4>connect(KeyOrName, ConnType, ExtraOpts) -&gt; {ok, Handle} | {error, Reason}</h4><p>Opens an SSH or SFTP connection using the information associated with KeyOrName.</p><ul><li><span class="v">KeyOrName = Key | Name</span></li><li><span class="v">Key = atom()</span></li><li><span class="v">Name = target_name()</span></li><li><span class="v">ConnType = ssh | sftp | host</span></li><li><span class="v">ExtraOpts = ssh_connect_options()</span></li><li><span class="v">Handle = handle()</span></li><li><span class="v">Reason = term()</span></li></ul><a name="connect-3"></a><p>Opens an SSH or SFTP connection using the information associated
with <strong>KeyOrName</strong>.</p><p>If <strong>Name</strong> (an alias name for <strong>Key</strong>) is used to identify
the connection, this name can be used as connection reference for
subsequent calls. Only one open connection at a time associated
with <strong>Name</strong> is possible. If <strong>Key</strong> is used, the returned
handle must be used for subsequent calls (multiple connections can
be opened using the configuration data specified by <strong>Key</strong>).</p><p>For information on how to create a new <strong>Name</strong>, see
<a href="./ct#require-2">ct#require-2</a>.</p><p>For <strong>target_name</strong>, see module
<a href="ct">ct</a>.</p><p><strong>ConnType</strong> always overrides the type specified in the
address tuple in the configuration data (and in <strong>ExtraOpts</strong>).
So it is possible to, for example, open an SFTP connection
directly using data originally specifying an SSH connection. Value
<strong>host</strong> means that the connection type specified by the host
option (either in the configuration data or in <strong>ExtraOpts</strong>)
is used.</p><p><strong>ExtraOpts</strong> (optional) are extra SSH options to be added to
the configuration data for <strong>KeyOrName</strong>. The extra options
override any existing options with the same key in the
configuration data. For details on valid SSH options, see
application <a href="./index">ssh/index</a>.</p><h4>del_dir(SSH, Name) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="del_dir-2"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>del_dir(SSH, Server, Name) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="del_dir-3"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>delete(SSH, Name) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="delete-2"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>delete(SSH, Server, Name) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="delete-3"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>disconnect(SSH) -&gt; ok | {error, Reason}</h4><p>Closes an SSH/SFTP connection.</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Reason = term()</span></li></ul><a name="disconnect-1"></a><p>Closes an SSH/SFTP connection.</p><h4>exec(SSH, Command) -&gt; {ok, Data} | {error, Reason}</h4><p>Equivalent to exec(SSH, Command, DefaultTimeout).</p><a name="exec-2"></a><p>Equivalent to
<a href="#exec-3">exec-3</a>.</p><h4>exec(SSH, Command, Timeout) -&gt; {ok, Data} | {error, Reason}</h4><p>Requests server to perform Command.</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Command = string()</span></li><li><span class="v">Timeout = integer()</span></li><li><span class="v">Data = list()</span></li><li><span class="v">Reason = term()</span></li></ul><a name="exec-3"></a><p>Requests server to perform <strong>Command</strong>. A session channel is
opened automatically for the request. <strong>Data</strong> is received from
the server as a result of the command.</p><h4>exec(SSH, ChannelId, Command, Timeout) -&gt; {ok, Data} | {error, Reason}</h4><p>Requests server to perform Command.</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">ChannelId = integer()</span></li><li><span class="v">Command = string()</span></li><li><span class="v">Timeout = integer()</span></li><li><span class="v">Data = list()</span></li><li><span class="v">Reason = term()</span></li></ul><a name="exec-4"></a><p>Requests server to perform <strong>Command</strong>. A previously opened
session channel is used for the request. <strong>Data</strong> is received
from the server as a result of the command.</p><h4>get_file_info(SSH, Handle) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="get_file_info-2"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>get_file_info(SSH, Server, Handle) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="get_file_info-3"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>list_dir(SSH, Path) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="list_dir-2"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>list_dir(SSH, Server, Path) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="list_dir-3"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>make_dir(SSH, Name) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="make_dir-2"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>make_dir(SSH, Server, Name) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="make_dir-3"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>make_symlink(SSH, Name, Target) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="make_symlink-3"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>make_symlink(SSH, Server, Name, Target) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="make_symlink-4"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>open(SSH, File, Mode) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="open-3"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>open(SSH, Server, File, Mode) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="open-4"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>opendir(SSH, Path) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="opendir-2"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>opendir(SSH, Server, Path) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="opendir-3"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>position(SSH, Handle, Location) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="position-3"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>position(SSH, Server, Handle, Location) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="position-4"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>pread(SSH, Handle, Position, Length) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="pread-4"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>pread(SSH, Server, Handle, Position, Length) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="pread-5"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>pwrite(SSH, Handle, Position, Data) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="pwrite-4"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>pwrite(SSH, Server, Handle, Position, Data) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="pwrite-5"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>read(SSH, Handle, Len) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="read-3"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>read(SSH, Server, Handle, Len) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="read-4"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>read_file(SSH, File) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="read_file-2"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>read_file(SSH, Server, File) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="read_file-3"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>read_file_info(SSH, Name) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="read_file_info-2"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>read_file_info(SSH, Server, Name) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="read_file_info-3"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>read_link(SSH, Name) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="read_link-2"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>read_link(SSH, Server, Name) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="read_link-3"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>read_link_info(SSH, Name) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="read_link_info-2"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>read_link_info(SSH, Server, Name) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="read_link_info-3"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>receive_response(SSH, ChannelId) -&gt; {ok, Data} | {error, Reason}</h4><p>Equivalent to receive_response(SSH, ChannelId, close).</p><a name="receive_response-2"></a><p>Equivalent to
<a href="#receive_response-3">receive_response-3</a>.</p><h4>receive_response(SSH, ChannelId, End) -&gt; {ok, Data} | {error, Reason}</h4><p>Equivalent to receive_response(SSH, ChannelId, End, DefaultTimeout).</p><a name="receive_response-3"></a><p>Equivalent to
<a href="#receive_response-4">receive_response-4</a>.</p><h4>receive_response(SSH, ChannelId, End, Timeout) -&gt; {ok, Data} | {timeout, Data} | {error, Reason}</h4><p>Receives expected data from server on the specified session channel.</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">ChannelId = integer()</span></li><li><span class="v">End = Fun | close | timeout</span></li><li><span class="v">Timeout = integer()</span></li><li><span class="v">Data = list()</span></li><li><span class="v">Reason = term()</span></li></ul><a name="receive_response-4"></a><p>Receives expected data from server on the specified session
channel.</p><p>If <strong>End == close</strong>, data is returned to the caller when the
channel is closed by the server. If a time-out occurs before this
happens, the function returns <strong>{timeout,Data}</strong> (where
<strong>Data</strong> is the data received so far).</p><p>If <strong>End == timeout</strong>, a time-out is expected and
<strong>{ok,Data}</strong> is returned both in the case of a time-out and
when the channel is closed.</p><p>If <strong>End</strong> is a fun, this fun is called with one argument, the
data value in a received <strong>ssh_cm</strong> message (see
<a href="./ssh_connection">ssh/ssh_connection</a>.
The fun is to return either <strong>true</strong> to end the receiving
operation (and have the so far collected data returned) or
<strong>false</strong> to wait for more data from the server. Even if a fun
is supplied, the function returns immediately if the server closes
the channel).</p><h4>rename(SSH, OldName, NewName) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="rename-3"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>rename(SSH, Server, OldName, NewName) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="rename-4"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>send(SSH, ChannelId, Data) -&gt; ok | {error, Reason}</h4><p>Equivalent to send(SSH, ChannelId, 0, Data, DefaultTimeout).</p><a name="send-3"></a><p>Equivalent to <a href="#send-5">send-5</a>.</p><h4>send(SSH, ChannelId, Data, Timeout) -&gt; ok | {error, Reason}</h4><p>Equivalent to send(SSH, ChannelId, 0, Data, Timeout).</p><a name="send-4"></a><p>Equivalent to <a href="#send-5">send-5</a>.</p><h4>send(SSH, ChannelId, Type, Data, Timeout) -&gt; ok | {error, Reason}</h4><p>Sends data to server on specified session channel.</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">ChannelId = integer()</span></li><li><span class="v">Type = integer()</span></li><li><span class="v">Data = list()</span></li><li><span class="v">Timeout = integer()</span></li><li><span class="v">Reason = term()</span></li></ul><a name="send-5"></a><p>Sends data to server on specified session channel.</p><h4>send_and_receive(SSH, ChannelId, Data) -&gt; {ok, Data} | {error, Reason}</h4><p>Equivalent to send_and_receive(SSH, ChannelId, Data, close).</p><a name="send_and_receive-3"></a><p>Equivalent to
<a href="#send_and_receive-4">send_and_receive-4</a>.</p><h4>send_and_receive(SSH, ChannelId, Data, End) -&gt; {ok, Data} | {error, Reason}</h4><p>Equivalent to send_and_receive(SSH, ChannelId, 0, Data, End, DefaultTimeout).</p><a name="send_and_receive-4"></a><p>Equivalent to
<a href="#send_and_receive-6">send_and_receive-6</a>.</p><h4>send_and_receive(SSH, ChannelId, Data, End, Timeout) -&gt; {ok, Data} | {error, Reason}</h4><p>Equivalent to send_and_receive(SSH, ChannelId, 0, Data, End, Timeout).</p><a name="send_and_receive-5"></a><p>Equivalent to
<a href="#send_and_receive-6">send_and_receive-6</a>.</p><h4>send_and_receive(SSH, ChannelId, Type, Data, End, Timeout) -&gt; {ok, Data} | {error, Reason}</h4><p>Sends data to server on specified session channel and waits to receive the server response.</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">ChannelId = integer()</span></li><li><span class="v">Type = integer()</span></li><li><span class="v">Data = list()</span></li><li><span class="v">End = Fun | close | timeout</span></li><li><span class="v">Timeout = integer()</span></li><li><span class="v">Reason = term()</span></li></ul><a name="send_and_receive-6"></a><p>Sends data to server on specified session channel and waits to
receive the server response.</p><p>For details on argument <strong>End</strong>, see
<a href="#receive_response-4">receive_response-4</a>.</p><h4>session_close(SSH, ChannelId) -&gt; ok | {error, Reason}</h4><p>Closes an SSH session channel.</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">ChannelId = integer()</span></li><li><span class="v">Reason = term()</span></li></ul><a name="session_close-2"></a><p>Closes an SSH session channel.</p><h4>session_open(SSH) -&gt; {ok, ChannelId} | {error, Reason}</h4><p>Equivalent to session_open(SSH, DefaultTimeout).</p><a name="session_open-1"></a><p>Equivalent to
<a href="#session_open-2">session_open-2</a>.</p><h4>session_open(SSH, Timeout) -&gt; {ok, ChannelId} | {error, Reason}</h4><p>Opens a channel for an SSH session.</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Timeout = integer()</span></li><li><span class="v">ChannelId = integer()</span></li><li><span class="v">Reason = term()</span></li></ul><a name="session_open-2"></a><p>Opens a channel for an SSH session.</p><h4>sftp_connect(SSH) -&gt; {ok, Server} | {error, Reason}</h4><p>Starts an SFTP session on an already existing SSH connection.</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Server = pid()</span></li><li><span class="v">Reason = term()</span></li></ul><a name="sftp_connect-1"></a><p>Starts an SFTP session on an already existing SSH connection.
<strong>Server</strong> identifies the new session and must be specified
whenever SFTP requests are to be sent.</p><h4>shell(SSH, ChannelId) -&gt; ok | {error, Reason}</h4><p>Equivalent to shell(SSH, ChannelId, DefaultTimeout).</p><a name="shell-2"></a><p>Equivalent to
<a href="#shell-3">shell-3</a>.</p><h4>shell(SSH, ChannelId, Timeout) -&gt; ok | {error, Reason}</h4><p>Requests that the user default shell is executed at the server end.</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">ChannelId = integer()</span></li><li><span class="v">Timeout = integer()</span></li><li><span class="v">Reason = term()</span></li></ul><a name="shell-3"></a><p>Requests that the user default shell (typically defined in
<strong>/etc/passwd</strong> in Unix systems) is executed at the
server end.</p><h4>subsystem(SSH, ChannelId, Subsystem) -&gt; Status | {error, Reason}</h4><p>Equivalent to subsystem(SSH, ChannelId, Subsystem, DefaultTimeout).</p><a name="subsystem-3"></a><p>Equivalent to
<a href="#subsystem-4">subsystem-4</a>.</p><h4>subsystem(SSH, ChannelId, Subsystem, Timeout) -&gt; Status | {error, Reason}</h4><p>Sends a request to execute a predefined subsystem.</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">ChannelId = integer()</span></li><li><span class="v">Subsystem = string()</span></li><li><span class="v">Timeout = integer()</span></li><li><span class="v">Status = success | failure</span></li><li><span class="v">Reason = term()</span></li></ul><a name="subsystem-4"></a><p>Sends a request to execute a predefined subsystem.</p><h4>write(SSH, Handle, Data) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="write-3"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>write(SSH, Server, Handle, Data) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="write-4"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>write_file(SSH, File, Iolist) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="write_file-3"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>write_file(SSH, Server, File, Iolist) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="write_file-4"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>write_file_info(SSH, Name, Info) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="write_file_info-3"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h4>write_file_info(SSH, Server, Name, Info) -&gt; Result</h4><p>For information and other types, see ssh_sftp(3).</p><ul><li><span class="v">SSH = connection()</span></li><li><span class="v">Result = ssh_sftp_return() | {error, Reason}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="write_file_info-4"></a><p>For information and other types, see
<a href="./ssh_sftp">ssh/ssh_sftp</a>.</p><h3>ct_netconfc</h3><p>NETCONF client module.</p><p>NETCONF client module.The NETCONF client is compliant with RFC 4741 NETCONF Configuration
Protocol and RFC 4742 Using the NETCONF Configuration Protocol over
Secure SHell (SSH).<a name="Connecting"></a><em>Connecting to a NETCONF server</em>NETCONF sessions can either be opened by a single call
to <a href="#open-1">open-1</a> or by a call
to <a href="#connect-1">connect-1</a> followed
by one or more calls to
<a href="#session-1">session-1</a>.The properties of the sessions will be exactly the same, except
that when
using <a href="#connect-1">connect-1</a>, you
may start multiple sessions over the same SSH connection. Each
session is implemented as an SSH channel.<a href="#open-1">open-1</a> will establish one
SSH connection with one SSH channel implementing one NETCONF
session. You may start mutiple sessions by
calling <a href="#open-1">open-1</a> multiple
times, but then a new SSH connection will be established for each
session.For each server to test against, the following entry can be added to a
configuration file:<pre>
 {server_id(),options()}.</pre>The <a href="#type-server_id">type-server_id</a>
or an associated
<a href="./ct#type-target_name">ct#type-target_name</a>
must then be used in calls to
<a href="#connect-2">connect-2</a>
or <a href="#open-2">open-2</a>.If no configuration exists for a server,
use <a href="#connect-1">connect-1</a>
or <a href="#open-1">open-1</a> instead,
and specify all necessary options in the <strong>Options</strong> parameter.<a name="Logging"></a><em>Logging</em>The NETCONF server uses <strong>error_logger</strong> for logging of NETCONF
traffic. A special purpose error handler is implemented in
<strong>ct_conn_log_h</strong>. To use this error handler, add the
<strong>cth_conn_log</strong> hook in the test suite, for example:<pre>
 suite() -&gt;
    [{ct_hooks, [{cth_conn_log, [{,}]}]}].</pre><strong>conn_log_mod()</strong> is the name of the <strong>Common Test</strong> module
implementing the connection protocol, for example, <strong>ct_netconfc</strong>.Hook option <strong>log_type</strong> specifies the type of logging:<dl><dt><strong>raw</strong></dt><dd><p>The sent and received NETCONF data is logged to a separate
text file "as is" without any formatting. A link to the file is
added to the test case HTML log.</p></dd><dt><strong>pretty</strong></dt><dd><p>The sent and received NETCONF data is logged to a separate
text file with XML data nicely indented. A link to the file is
added to the test case HTML log.</p></dd><dt><strong>html (default)</strong></dt><dd><p>The sent and received NETCONF traffic is pretty printed
directly in the test case HTML log.</p></dd><dt><strong>silent</strong></dt><dd><p>NETCONF traffic is not logged.</p></dd></dl>By default, all NETCONF traffic is logged in one single log file.
However, different connections can be logged in separate files.
To do this, use hook option <strong>hosts</strong> and list the names of the
servers/connections to be used in the suite. The connections
must be named for this to work, that is, they must be opened with
<a href="#open-2">open-2</a>.Option <strong>hosts</strong> has no effect if <strong>log_type</strong> is set to
<strong>html</strong> or <strong>silent</strong>.The hook options can also be specified in a configuration file with
configuration variable <strong>ct_conn_log</strong>:<pre>
 {ct_conn_log,[{,}]}.</pre>For example:<pre>
 {ct_conn_log,[{ct_netconfc,[{log_type,pretty},
                             {hosts,[]}]}]}</pre><em>Logging Example 1:</em><a name="Logging_example_1"></a>The following <strong>ct_hooks</strong> statement causes pretty printing of
NETCONF traffic to separate logs for the connections named
<strong>nc_server1</strong> and <strong>nc_server2</strong>. Any other connections are
logged to default NETCONF log.<pre>
 suite() -&gt;
    [{ct_hooks, [{cth_conn_log, [{ct_netconfc,[{log_type,pretty}},
                                               {hosts,[nc_server1,nc_server2]}]}
                                ]}]}].</pre>Connections must be opened as follows:<pre>
 open(nc_server1,[...]),
 open(nc_server2,[...]).</pre><em>Logging Example 2:</em><a name="Logging_example_2"></a>The following configuration file causes raw logging of all NETCONF 
traffic in to one single text file:<pre>
 {ct_conn_log,[{ct_netconfc,[{log_type,raw}]}]}.</pre>The <strong>ct_hooks</strong> statement must look as follows:<pre>
 suite() -&gt;
    [{ct_hooks, [{cth_conn_log, []}]}].</pre>The same <strong>ct_hooks</strong> statement without the configuration file
would cause HTML logging of all NETCONF connections in to the test
case HTML log.<a name="Notifications"></a><em>Notifications</em>The NETCONF client is also compliant with RFC 5277 NETCONF Event
Notifications, which defines a mechanism for an asynchronous message 
notification delivery service for the NETCONF protocol.Specific functions to support this are
<a href="#create_subscription-1">create_subscription-1</a>
and
<a href="#get_event_streams-1">get_event_streams-1</a>.<a name="Default_timeout"></a><em>Default Timeout</em>Most of the functions in this module have one variant with
a <strong>Timeout</strong> parameter, and one without. If nothing else is
specified, the default value <strong>infinity</strong> is used when
the <strong>Timeout</strong> parameter is not given.</p><h3>Data Types</h3><span class="name">client</span><span class="name">error_reason</span><span class="name">event_time</span><span class="name">handle</span><p>Opaque reference for a connection to a NETCONF server or a
NETCONF session.</p><span class="name">host</span><span class="name">netconf_db</span><span class="name">notification</span><span class="name">notification_content</span><span class="name">option</span><p><strong>SshConnectOption</strong> is any valid option to
<a href="../ssh/ssh#connect-3">ssh/ssh#connect-3</a>.
Common options used are <strong>user</strong>, <strong>password</strong>
and <strong>user_dir</strong>. The <strong>SshConnectOptions</strong> are
verfied by the SSH application.</p><span class="name">options</span><p>Options used for setting up an SSH connection to a NETCONF
server.</p><span class="name">server_id</span><p>The identity of a server, specified in a configuration
file.</p><span class="name">simple_xml</span><p>This type is further described in application
<a href="./index">xmerl/index</a>.</p><span class="name">stream_data</span><p>For details about the data format for the string values, see
"XML Schema for Event Notifications" in RFC 5277.</p><span class="name">stream_name</span><span class="name">streams</span><span class="name">xml_attribute_tag</span><span class="name">xml_attribute_value</span><span class="name">xml_attributes</span><span class="name">xml_content</span><span class="name">xml_tag</span><span class="name">xpath</span><span class="name">xs_datetime</span><p>This date and time identifier has the same format as the XML type
<strong>dateTime</strong> and is compliant with RFC 3339 Date and Time on
the Internet Timestamps. The format is as follows:</p><pre>
 [-]CCYY-MM-DDThh:mm:ss[.s][Z|(+|-)hh:mm]</pre><h3>Functions</h3><h4>action/2</h4><h4>action/3</h4><p>Executes an action.</p><p>Executes an action. If the return type is void, <strong>ok</strong> is
returned instead of <strong>{ok,[simple_xml()]}</strong>.</p><h4>close_session/1</h4><h4>close_session/2</h4><p>Requests graceful termination of the session associated with the client.</p><p>Requests graceful termination of the session associated with the
client.</p><p>When a NETCONF server receives a <strong>close-session</strong> request, it
gracefully closes the session.  The server releases any locks and
resources associated with the session and gracefully closes any
associated connections. Any NETCONF requests received after a
<strong>close-session</strong> request are ignored.</p><h4>connect/1</h4><p>Opens an SSH connection to a NETCONF server.</p><p>Opens an SSH connection to a NETCONF server.</p><p>If the server options are specified in a configuration file, use
<a href="#connect-2">connect-2</a>
instead.</p><p>The opaque <a href="#type-handle">type-handle</a>
reference returned from this
function is required as connection identifier when opening
sessions over this connection, see
<a href="#session-1">session-1</a>.</p><p>Option <strong>timeout</strong> (milliseconds) is used when setting up the
SSH connection. It is not used for any other purposes during the
lifetime of the connection.</p><h4>connect/2</h4><p>Opens an SSH connection to a named NETCONF server.</p><p>Open an SSH connection to a named NETCONF server.</p><p>If <strong><span class="anno">KeyOrName</span></strong> is a
configured <strong>server_id()</strong> or a
<strong>target_name()</strong> associated with such an Id, then the options
for this server are fetched from the configuration file.</p><p>Argument <strong><span class="anno">ExtraOptions</span></strong> is added to the
options found in the configuration file. If the same options
are specified, the values from the configuration file
overwrite <strong><span class="anno">ExtraOptions</span></strong>.</p><p>If the server is not specified in a configuration file, use
<a href="#connect-1">connect-1</a>
instead.</p><p>The opaque <a href="#type-handle">type-handle</a>
reference returned from this
function can be used as connection identifier when opening
sessions over this connection, see
<a href="#session-1">session-1</a>.
However, if <strong><span class="anno">KeyOrName</span></strong> is a
<strong>target_name()</strong>, that is, if the server is named through a
call to <a href="./ct#require-2">ct#require-2</a>
or a <strong>require</strong> statement in the test suite, then this name can
be used instead of
<a href="#type-handle">type-handle</a>.</p><p>Option <strong>timeout</strong> (milliseconds) is used when setting up the
SSH connection. It is not used for any other purposes during the
lifetime of the connection.</p><h4>copy_config/3</h4><h4>copy_config/4</h4><p>Copies configuration data.</p><p>Copies configuration data.</p><p>Which source and target options that can be issued depends on the
capabilities supported by the server. That is, <strong>:candidate</strong>
and/or <strong>:startup</strong> are required.</p><h4>create_subscription(Client) -&gt; Result</h4><h4>create_subscription(Client, Stream) -&gt; Result</h4><h4>create_subscription(Client, Stream, Filter) -&gt; Result</h4><h4>create_subscription(Client, Stream, Filter, Timeout) -&gt; Result</h4><h4>create_subscription/5</h4><h4>create_subscription/6</h4><p>Creates a subscription for event notifications.</p><p>Creates a subscription for event notifications.</p><p>This function sets up a subscription for NETCONF event
notifications of the specified stream type, matching the specified
filter. The calling process receives notifications as messages of
type <a href="#type-notification">type-notification</a>.</p><p>Only a subset of the function clauses are show above. The
full set of valid combinations of input parameters is as
follows:</p><pre>create_subscription(Client)

create_subscription(Client, Timeout)
create_subscription(Client, Stream)
create_subscription(Client, Filter)

create_subscription(Client, Stream, Timeout)
create_subscription(Client, Filter, Timeout)
create_subscription(Client, Stream, Filter)
create_subscription(Client, StartTime, StopTime)

create_subscription(Client, Stream, Filter, Timeout)
create_subscription(Client, StartTime, StopTime, Timeout)
create_subscription(Client, Stream, StartTime, StopTime)
create_subscription(Client, Filter, StartTime, StopTime)

create_subscription(Client, Stream, StartTime, StopTime, Timeout)
create_subscription(Client, Stream, Filter, StartTime, StopTime)
create_subscription(Client, Stream, Filter, StartTime, StopTime, Timeout)</pre><dl><dt><strong><span class="anno">Stream</span></strong></dt><dd><p>Optional parameter that indicates which stream of event
is of interest. If not present, events in the default NETCONF
stream are sent.</p></dd><dt><strong><span class="anno">Filter</span></strong></dt><dd><p>Optional parameter that indicates which subset of all
possible events is of interest. The parameter format is the
same as that of the filter parameter in the NETCONF protocol
operations. If not present, all events not precluded by other
parameters are sent.</p></dd><dt><strong><span class="anno">StartTime</span></strong></dt><dd><p>Optional parameter used to trigger the replay feature and
indicate that the replay is to start at the time specified.
If <strong><span class="anno">StartTime</span></strong> is not present, this is not a
replay subscription.</p> <p>It is not valid to specify start times that are later than
the current time. If <strong><span class="anno">StartTime</span></strong> is specified
earlier than the log can support, the replay begins with the
earliest available notification.</p> <p>This parameter is of type <strong>dateTime</strong> and compliant to
RFC 3339. Implementations must support time zones.</p></dd><dt><strong><span class="anno">StopTime</span></strong></dt><dd><p>Optional parameter used with the optional replay feature
to indicate the newest notifications of interest. If
<strong><span class="anno">StopTime</span></strong> is not present, the notifications
continues until the subscription is terminated.</p> <p>Must be used with and be later than <strong>StartTime</strong>. Values
of <strong><span class="anno">StopTime</span></strong> in the future are valid. This
parameter is of type <strong>dateTime</strong> and compliant to RFC 3339.
Implementations must support time zones.</p></dd></dl><p>For more details about the event notification mechanism, see
RFC 5277.</p><h4>delete_config/2</h4><h4>delete_config/3</h4><p>Deletes configuration data.</p><p>Deletes configuration data.</p><p>The running configuration cannot be deleted and <strong>:candidate</strong>
or <strong>:startup</strong> must be advertised by the server.</p><h4>disconnect/1</h4><p>Closes the given SSH connection.</p><p>Closes the given SSH connection.</p><p>If there are open NETCONF sessions on the connection, these
will be brutally aborted. To avoid this, close each session
with <a href="#close_session-1">close_session-1</a></p><h4>edit_config/3</h4><h4>edit_config/4</h4><h4>edit_config/4</h4><h4>edit_config/5</h4><p>Edits configuration data.</p><p>Edits configuration data.</p><p>By default only the running target is available, unless the server
includes <strong>:candidate</strong> or <strong>:startup</strong> in its list of
capabilities.</p><p><strong>OptParams</strong> can be used for specifying optional parameters
(<strong>default-operation</strong>, <strong>test-option</strong>, or
<strong>error-option</strong>) to be added to the <strong>edit-config</strong>
request. The value must be a list containing valid simple XML,
for example:</p><pre>
 [{'default-operation', ["none"]},
  {'error-option', ["rollback-on-error"]}]</pre><p>If <strong><span class="anno">OptParams</span></strong> is not given, the default
value <strong>[]</strong> is used.</p><h4>get/2</h4><h4>get/3</h4><p>Gets data.</p><p>Gets data.</p><p>This operation returns both configuration and state data from the
server.</p><p>Filter type <strong>xpath</strong> can be used only if the server supports
<strong>:xpath</strong>.</p><h4>get_capabilities/1</h4><h4>get_capabilities/2</h4><p>Returns the server side capabilities.</p><p>Returns the server side capabilities.</p><p>The following capability identifiers, defined in RFC 4741 NETCONF
Configuration Protocol, can be returned:</p><ul><li><p><strong>"urn:ietf:params:netconf:base:1.0"</strong></p></li><li><p><strong>"urn:ietf:params:netconf:capability:writable-running:1.0"</strong></p></li><li><p><strong>"urn:ietf:params:netconf:capability:candidate:1.0"</strong></p></li><li><p><strong>"urn:ietf:params:netconf:capability:confirmed-commit:1.0"</strong></p></li><li><p><strong>"urn:ietf:params:netconf:capability:rollback-on-error:1.0"</strong></p></li><li><p><strong>"urn:ietf:params:netconf:capability:startup:1.0"</strong></p></li><li><p><strong>"urn:ietf:params:netconf:capability:url:1.0"</strong></p></li><li><p><strong>"urn:ietf:params:netconf:capability:xpath:1.0"</strong></p></li></ul><p>More identifiers can exist, for example, server-side namespace.</p><h4>get_config/3</h4><h4>get_config/4</h4><p>Gets configuration data.</p><p>Gets configuration data.</p><p>To be able to access another source than <strong>running</strong>, the
server must advertise <strong>:candidate</strong> and/or <strong>:startup</strong>.</p><p>Filter type <strong>xpath</strong> can be used only if the server supports
<strong>:xpath</strong>.</p><h4>get_event_streams/1</h4><h4>get_event_streams/2</h4><h4>get_event_streams/2</h4><h4>get_event_streams/3</h4><p>Sends a request to get the specified event streams.</p><p>Sends a request to get the specified event streams.</p><p><strong>Streams</strong> is a list of stream names. The following filter is
sent to the NETCONF server in a <strong>get</strong> request:</p><pre>
 &lt;netconf xmlns="urn:ietf:params:xml:ns:netmod:notification"&gt;
   &lt;streams&gt;
     &lt;stream&gt;
       &lt;name&gt;StreamName1&lt;/name&gt;
     &lt;/stream&gt;
     &lt;stream&gt;
       &lt;name&gt;StreamName2&lt;/name&gt;
     &lt;/stream&gt;
     ...
   &lt;/streams&gt;
 &lt;/netconf&gt;</pre><p>If <strong>Streams</strong> is an empty list, <em>all</em> streams are
requested by sending the following filter:</p><pre>
 &lt;netconf xmlns="urn:ietf:params:xml:ns:netmod:notification"&gt;
   &lt;streams/&gt;
 &lt;/netconf&gt;</pre><p>If more complex filtering is needed, use
<a href="#get-2">get-2</a> and
specify the exact filter according to "XML Schema for Event
Notifications" in RFC 5277.</p><h4>get_session_id/1</h4><h4>get_session_id/2</h4><p>Returns the session Id associated with the specified client.</p><p>Returns the session Id associated with the specified client.</p><h4>hello/1</h4><h4>hello/2</h4><h4>hello/3</h4><p>Exchanges hello messages with the server.</p><p>Exchanges <strong>hello</strong> messages with the server.</p><p>Adds optional capabilities and sends a <strong>hello</strong> message to the
server and waits for the return.</p><h4>kill_session/2</h4><h4>kill_session/3</h4><p>Forces termination of the session associated with the supplied session Id.</p><p>Forces termination of the session associated with the supplied
session Id.</p><p>The server side must abort any ongoing operations, release any
locks and resources associated with the session, and close any
associated connections.</p><p>Only if the server is in the confirmed commit phase, the
configuration is restored to its state before entering the confirmed
commit phase. Otherwise, no configuration rollback is performed.</p><p>If the specified <strong>SessionId</strong> is equal to the current session
Id, an error is returned.</p><h4>lock/2</h4><h4>lock/3</h4><p>Locks the configuration target.</p><p>Locks the configuration target.</p><p>Which target parameters that can be used depends on if
<strong>:candidate</strong> and/or <strong>:startup</strong> are supported by the
server. If successfull, the configuration system of the device is
unavailable to other clients (NETCONF, CORBA, SNMP, and so on).
Locks are intended to be short-lived.</p><p>Operation
<a href="#kill_session-2">kill_session-2</a>
can be used to force the release of a lock owned by another NETCONF
session. How this is achieved by the server side is
implementation-specific.</p><h4>only_open/1</h4><p>Opens a NETCONF session, but does not send hello.</p><p>Opens a NETCONF session, but does not send <strong>hello</strong>.</p><p>As <a href="#open-1">open-1</a>, but
does not send a <strong>hello</strong> message.</p><h4>only_open/2</h4><p>Opens a named NETCONF session, but does not send hello.</p><p>Opens a named NETCONF session, but does not send <strong>hello</strong>.</p><p>As <a href="#open-2">open-2</a>, but
does not send a <strong>hello</strong> message.</p><h4>open/1</h4><p>Opens a NETCONF session and exchanges hello messages.</p><p>Opens a NETCONF session and exchanges <strong>hello</strong> messages.</p><p>If the server options are specified in a configuration file,
or if a named client is needed for logging purposes (see section
<a href="#Logging">Logging</a> in this module), use
<a href="#open-2">open-2</a>
instead.</p><p>The opaque <a href="#type-handle">type-handle</a>
reference returned from this
function is required as client identifier when calling any other
function in this module.</p><p>Option <strong>timeout</strong> (milliseconds) is used when setting up the
SSH connection and when waiting for the <strong>hello</strong> message from
the server. It is not used for any other purposes during the
lifetime of the connection.</p><h4>open/2</h4><p>Opens a named NETCONF session and exchanges hello messages.</p><p>Opens a named NETCONF session and exchanges <strong>hello</strong>
messages.</p><p>If <strong><span class="anno">KeyOrName</span></strong> is a
configured <strong>server_id()</strong> or a
<strong>target_name()</strong> associated with such an Id, then the options
for this server are fetched from the configuration file.</p><p>Argument <strong><span class="anno">ExtraOptions</span></strong> is added to the
options found in the configuration file. If the same
options are specified, the values from the configuration
file overwrite <strong><span class="anno">ExtraOptions</span></strong>.</p><p>If the server is not specified in a configuration file, use
<a href="#open-1">open-1</a>
instead.</p><p>The opaque <a href="#type-handle">type-handle</a>
reference returned from this
function can be used as client identifier when calling any other
function in this module. However, if <strong><span class="anno">KeyOrName</span></strong> is a
<strong>target_name()</strong>, that is, if the server is named through a
call to <a href="./ct#require-2">ct#require-2</a>
or a <strong>require</strong> statement in the test suite, then this name can
be used instead of
<a href="#type-handle">type-handle</a>.</p><p>Option <strong>timeout</strong> (milliseconds) is used when setting up the
SSH connection and when waiting for the <strong>hello</strong> message from
the server. It is not used for any other purposes during the
lifetime of the connection.</p><p>See also
<a href="./ct#require-2">ct#require-2</a>.</p><h4>send/2</h4><h4>send/3</h4><p>Sends an XML document to the server.</p><p>Sends an XML document to the server.</p><p>The specified XML document is sent "as is" to the server. This
function can be used for sending XML documents that cannot be
expressed by other interface functions in this module.</p><h4>send_rpc/2</h4><h4>send_rpc/3</h4><p>Sends a NETCONF rpc request to the server.</p><p>Sends a NETCONF <strong>rpc</strong> request to the server.</p><p>The specified XML document is wrapped in a valid NETCONF <strong>rpc</strong>
request and sent to the server. The <strong>message-id</strong> and namespace
attributes are added to element <strong>rpc</strong>.</p><p>This function can be used for sending <strong>rpc</strong> requests that
cannot be expressed by other interface functions in this module.</p><h4>session/1</h4><h4>session/2</h4><h4>session/2</h4><h4>session/3</h4><p>Opens a NETCONF session as a channel on the given SSH connection, and exchanges hello messages with the server.</p><ul><li>session_options</li></ul><ul><li>session_option</li></ul><p>Opens a NETCONF session as a channel on the given SSH
connection, and exchanges hello messages with the server.</p><p>The opaque <a href="#type-handle">type-handle</a>
reference returned from this
function can be used as client identifier when calling any
other function in this module. However, if <strong><span class="anno">KeyOrName</span></strong>
is used and it is a <strong>target_name()</strong>, that is, if the
server is named through a call
to <a href="./ct#require-2">ct#require-2</a>
or a <strong>require</strong> statement in the test suite, then this
name can be used instead of
<a href="#type-handle">type-handle</a>.</p><h4>unlock/2</h4><h4>unlock/3</h4><p>Unlocks the configuration target.</p><p>Unlocks the configuration target.</p><p>If the client earlier has acquired a lock through
<a href="#lock-2">lock-2</a>, this
operation releases the associated lock. To access another target
than <strong>running</strong>, the server must support <strong>:candidate</strong>
and/or <strong>:startup</strong>.</p><h3>ct_rpc</h3><p>Common Test specific layer on Erlang/OTP rpc.</p><p><strong>Common Test</strong> specific layer on Erlang/OTP <strong>rpc</strong>.</p><h3>Functions</h3><h4>app_node(App, Candidates) -&gt; NodeName</h4><p>From a set of candidate nodes determines which of them is running the application App.</p><ul><li><span class="v">App = atom()</span></li><li><span class="v">Candidates = [NodeName]</span></li><li><span class="v">NodeName = atom()</span></li></ul><a name="app_node-2"></a><p>From a set of candidate nodes determines which of them is running
the application <strong>App</strong>. If none of the candidate nodes is
running <strong>App</strong>, the function makes the test case calling
this function to fail. This function is the same as calling
<strong>app_node(App, Candidates, true)</strong>.</p><h4>app_node(App, Candidates, FailOnBadRPC) -&gt; NodeName</h4><p>Same as app_node/2, except that argument FailOnBadRPC determines if the search for a candidate node is to stop if badrpc is received at some point.</p><ul><li><span class="v">App = atom()</span></li><li><span class="v">Candidates = [NodeName]</span></li><li><span class="v">NodeName = atom()</span></li><li><span class="v">FailOnBadRPC = true | false</span></li></ul><a name="app_node-3"></a><p>Same as
<a href="#app_node-2">app_node-2</a>,
except that argument <strong>FailOnBadRPC</strong> determines if the search
for a candidate node is to stop if <strong>badrpc</strong> is received at
some point.</p><h4>app_node(App, Candidates, FailOnBadRPC, Cookie) -&gt; NodeName</h4><p>Same as app_node/2, except that argument FailOnBadRPC determines if the search for a candidate node is to stop if badrpc is received at some point.</p><ul><li><span class="v">App = atom()</span></li><li><span class="v">Candidates = [NodeName]</span></li><li><span class="v">NodeName = atom()</span></li><li><span class="v">FailOnBadRPC = true | false</span></li><li><span class="v">Cookie = atom()</span></li></ul><a name="app_node-4"></a><p>Same as
<a href="#app_node-2">app_node-2</a>,
except that argument <strong>FailOnBadRPC</strong> determines if the search
for a candidate node is to stop if <strong>badrpc</strong> is received at
some point.</p><p>The cookie on the client node is set to <strong>Cookie</strong> for this
<strong>rpc</strong> operation (used to match the server node cookie).</p><h4>call(Node, Module, Function, Args) -&gt; term() | {badrpc, Reason}</h4><p>Same as call(Node, Module, Function, Args, infinity).</p><a name="call-4"></a><p>Same as <strong>call(Node, Module, Function, Args, infinity)</strong>.</p><h4>call(Node, Module, Function, Args, TimeOut) -&gt; term() | {badrpc, Reason}</h4><p>Evaluates apply(Module, Function, Args) on the node Node.</p><ul><li><span class="v">Node = NodeName | {Fun, FunArgs}</span></li><li><span class="v">Fun = function()</span></li><li><span class="v">FunArgs = term()</span></li><li><span class="v">NodeName = atom()</span></li><li><span class="v">Module = atom()</span></li><li><span class="v">Function = atom()</span></li><li><span class="v">Args = [term()]</span></li><li><span class="v">Reason = timeout | term()</span></li></ul><a name="call-5"></a><p>Evaluates <strong>apply(Module, Function, Args)</strong> on the node
<strong>Node</strong>. Returns either whatever <strong>Function</strong> returns, or
<strong>{badrpc, Reason}</strong> if the remote procedure call fails. If
<strong>Node</strong> is <strong>{Fun, FunArgs}</strong>, applying <strong>Fun</strong> to
<strong>FunArgs</strong> is to return a node name.</p><h4>call(Node, Module, Function, Args, TimeOut, Cookie) -&gt; term() | {badrpc, Reason}</h4><p>Evaluates apply(Module, Function, Args) on the node Node.</p><ul><li><span class="v">Node = NodeName | {Fun, FunArgs}</span></li><li><span class="v">Fun = function()</span></li><li><span class="v">FunArgs = term()</span></li><li><span class="v">NodeName = atom()</span></li><li><span class="v">Module = atom()</span></li><li><span class="v">Function = atom()</span></li><li><span class="v">Args = [term()]</span></li><li><span class="v">Reason = timeout | term()</span></li><li><span class="v">Cookie = atom()</span></li></ul><a name="call-6"></a><p>Evaluates <strong>apply(Module, Function, Args)</strong> on the node
<strong>Node</strong>. Returns either whatever <strong>Function</strong> returns, or
<strong>{badrpc, Reason}</strong> if the remote procedure call fails. If
<strong>Node</strong> is <strong>{Fun, FunArgs}</strong>, applying <strong>Fun</strong> to
<strong>FunArgs</strong> is to return a node name.</p><p>The cookie on the client node is set to <strong>Cookie</strong> for this
<strong>rpc</strong> operation (used to match the server node cookie).</p><h4>cast(Node, Module, Function, Args) -&gt; ok</h4><p>Evaluates apply(Module, Function, Args) on the node Node.</p><ul><li><span class="v">Node = NodeName | {Fun, FunArgs}</span></li><li><span class="v">Fun = function()</span></li><li><span class="v">FunArgs = term()</span></li><li><span class="v">NodeName = atom()</span></li><li><span class="v">Module = atom()</span></li><li><span class="v">Function = atom()</span></li><li><span class="v">Args = [term()]</span></li><li><span class="v">Reason = timeout | term()</span></li></ul><a name="cast-4"></a><p>Evaluates <strong>apply(Module, Function, Args)</strong> on the node
<strong>Node</strong>. No response is delivered and the process that makes
the call is not suspended until the evaluation is completed as in
the case of <strong>call/3,4</strong>. If <strong>Node</strong> is
<strong>{Fun, FunArgs}</strong>, applying <strong>Fun</strong> to <strong>FunArgs</strong> is to
return a node name.</p><h4>cast(Node, Module, Function, Args, Cookie) -&gt; ok</h4><p>Evaluates apply(Module, Function, Args) on the node Node.</p><ul><li><span class="v">Node = NodeName | {Fun, FunArgs}</span></li><li><span class="v">Fun = function()</span></li><li><span class="v">FunArgs = term()</span></li><li><span class="v">NodeName = atom()</span></li><li><span class="v">Module = atom()</span></li><li><span class="v">Function = atom()</span></li><li><span class="v">Args = [term()]</span></li><li><span class="v">Reason = timeout | term()</span></li><li><span class="v">Cookie = atom()</span></li></ul><a name="cast-5"></a><p>Evaluates <strong>apply(Module, Function, Args)</strong> on the node
<strong>Node</strong>. No response is delivered and the process that makes
the call is not suspended until the evaluation is completed as in
the case of <strong>call/3,4</strong>. If <strong>Node</strong> is
<strong>{Fun, FunArgs}</strong>, applying <strong>Fun</strong> to <strong>FunArgs</strong> is to
return a node name.</p><p>The cookie on the client node is set to <strong>Cookie</strong> for this
<strong>rpc</strong> operation (used to match the server node cookie).</p><h3>ct_snmp</h3><p>Common Test user interface module for the SNMP application.</p><p><strong>Common Test</strong> user interface module for the <strong>SNMP</strong>
application.The purpose of this module is to simplify SNMP configuration for the
test case writer. Many test cases can use default values for common
operations and then no SNMP configuration files need to be supplied.
When it is necessary to change particular configuration parameters, a
subset of the relevant SNMP configuration files can be passed to
<strong>ct_snmp</strong> by <strong>Common Test</strong> configuration files. For more
specialized configuration parameters, a simple SNMP configuration file
can be placed in the test suite data directory. To simplify the test
suite, <strong>Common Test</strong> keeps track of some of the SNMP manager
information. This way the test suite does not have to handle as many
input parameters as if it had to interface wthe OTP SNMP manager
directly.<em>Configurable SNMP Manager and Agent Parameters:</em>Manager configuration:<dl><dt><strong>[{start_manager, boolean()}</strong></dt><dd><p>Optional. Default is <strong>true</strong>.</p></dd><dt><strong>{users, [{user_name(), [call_back_module(), user_data()]}]}</strong></dt><dd><p>Optional.</p></dd><dt><strong>{usm_users, [{usm_user_name(), [usm_config()]}]}</strong></dt><dd><p>Optional. SNMPv3 only.</p></dd><dt><strong>{managed_agents,[{agent_name(), [user_name(), agent_ip(), agent_port(), [agent_config()]]}]}</strong></dt><dd><p><strong>managed_agents</strong> is optional.</p></dd><dt><strong>{max_msg_size, integer()}</strong></dt><dd><p>Optional. Default is <strong>484</strong>.</p></dd><dt><strong>{mgr_port, integer()}</strong></dt><dd><p>Optional. Default is <strong>5000</strong>.</p></dd><dt><strong>{engine _id, string()}</strong></dt><dd><p>Optional. Default is <strong>"mgrEngine"</strong>.</p></dd></dl>Agent configuration:<dl><dt><strong>{start_agent, boolean()}</strong></dt><dd><p>Optional. Default is <strong>false</strong>.</p></dd><dt><strong>{agent_sysname, string()}</strong></dt><dd><p>Optional. Default is <strong>"ct_test"</strong>.</p></dd><dt><strong>{agent_manager_ip, manager_ip()}</strong></dt><dd><p>Optional. Default is <strong>localhost</strong>.</p></dd><dt><strong>{agent_vsns, list()}</strong></dt><dd><p>Optional. Default is <strong>[v2]</strong>.</p></dd><dt><strong>{agent_trap_udp, integer()}</strong></dt><dd><p>Optional. Default is <strong>5000</strong>.</p></dd><dt><strong>{agent_udp, integer()}</strong></dt><dd><p>Optional. Default is <strong>4000</strong>.</p></dd><dt><strong>{agent_notify_type, atom()}</strong></dt><dd><p>Optional. Default is <strong>trap</strong>.</p></dd><dt><strong>{agent_sec_type, sec_type()}</strong></dt><dd><p>Optional. Default is <strong>none</strong>.</p></dd><dt><strong>{agent_passwd, string()}</strong></dt><dd><p>Optional. Default is <strong>""</strong>.</p></dd><dt><strong>{agent_engine_id, string()}</strong></dt><dd><p>Optional. Default is <strong>"agentEngine"</strong>.</p></dd><dt><strong>{agent_max_msg_size, string()}</strong></dt><dd><p>Optional. Default is <strong>484</strong>.</p></dd></dl>The following parameters represents the SNMP configuration files
<strong>context.conf</strong>, <strong>standard.conf</strong>, <strong>community.conf</strong>,
<strong>vacm.conf</strong>, <strong>usm.conf</strong>, <strong>notify.conf</strong>,
<strong>target_addr.conf</strong>, and <strong>target_params.conf</strong>. Notice that
all values in <strong>agent.conf</strong> can be modified by the parameters
listed above. All these configuration files have default values set by
the <strong>SNMP</strong> application. These values can be overridden by suppling
a list of valid configuration values or a file located in the test
suites data directory, which can produce a list of valid configuration
values if you apply function <strong>file:consult/1</strong> to the file.<dl><dt><strong>{agent_contexts, [term()] | {data_dir_file, rel_path()}}</strong></dt><dd><p>Optional.</p></dd><dt><strong>{agent_community, [term()] | {data_dir_file, rel_path()}}</strong></dt><dd><p>Optional.</p></dd><dt><strong>{agent_sysinfo,  [term()] | {data_dir_file, rel_path()}}</strong></dt><dd><p>Optional.</p></dd><dt><strong>{agent_vacm, [term()] | {data_dir_file, rel_path()}}</strong></dt><dd><p>Optional.</p></dd><dt><strong>{agent_usm, [term()] | {data_dir_file, rel_path()}}</strong></dt><dd><p>Optional.</p></dd><dt><strong>{agent_notify_def, [term()] | {data_dir_file, rel_path()}}</strong></dt><dd><p>Optional.</p></dd><dt><strong>{agent_target_address_def, [term()] | {data_dir_file, rel_path()}}</strong></dt><dd><p>Optional.</p></dd><dt><strong>{agent_target_param_def, [term()] | {data_dir_file, rel_path()}}</strong></dt><dd><p>Optional.</p></dd></dl>Parameter <strong>MgrAgentConfName</strong> in the functions is to be a name
you allocate in your test suite using a <strong>require</strong> statement.
Example (where <strong>MgrAgentConfName = snmp_mgr_agent</strong>):<pre>
 suite() -&gt; [{require, snmp_mgr_agent, snmp}].</pre>or<pre>
 ct:require(snmp_mgr_agent, snmp).</pre>Notice that USM users are needed for SNMPv3 configuration and are
not to be confused with users.SNMP traps, inform, and report messages are handled by the user
callback module. For details, see the
<a href="./index">snmp/index</a> application.It is recommended to use the <strong>.hrl</strong> files created by the
Erlang/OTP MIB compiler to define the Object Identifiers (OIDs).
For example, to get the Erlang node name from <strong>erlNodeTable</strong>
in the OTP-MIB:<pre>
 Oid = ?erlNodeEntry ++ [?erlNodeName, 1]</pre>Furthermore, values can be set for <strong>SNMP</strong> application configuration
parameters, <strong>config</strong>, <strong>server</strong>, <strong>net_if</strong>, and so on (for
a list of valid parameters and types, see the  <a href="./users_guide">snmp/users_guide</a>). This is
done by defining a configuration data variable on the following form:<pre>
 {snmp_app, [{manager, [snmp_app_manager_params()]},
             {agent, [snmp_app_agent_params()]}]}.</pre>A name for the data must be allocated in the suite using
<strong>require</strong> (see the example above). Pass this name as argument
<strong>SnmpAppConfName</strong> to
<a href="#start-3">start-3</a>.
<strong>ct_snmp</strong> specifies default values for some <strong>SNMP</strong> application
configuration parameters (such as <strong>{verbosity,trace}</strong> for parameter
<strong>config</strong>). This set of defaults is merged with the parameters
specified by the user. The user values override <strong>ct_snmp</strong>
defaults.</p><h4>Data Types</h4><a name="types"></a><dl><dt><strong>agent_config() = {Item, Value}</strong></dt><dd><a name="type-agent_config"></a> </dd><dt><strong>agent_ip() = ip()</strong></dt><dd><a name="type-agent_ip"></a> </dd><dt><strong>agent_name() = atom()</strong></dt><dd><a name="type-agent_name"></a> </dd><dt><strong>agent_port() = integer()</strong></dt><dd><a name="type-agent_port"></a> </dd><dt><strong>call_back_module() = atom()</strong></dt><dd><a name="type-call_back_module"></a> </dd><dt><strong>error_index() = integer()</strong></dt><dd><a name="type-error_index"></a> </dd><dt><strong>error_status() = noError | atom()</strong></dt><dd><a name="type-error_status"></a> </dd><dt><strong>ip() = string() | {integer(), integer(), integer(), integer()}</strong></dt><dd><a name="type-ip"></a> </dd><dt><strong>manager_ip() = ip()</strong></dt><dd><a name="type-manager_ip"></a> </dd><dt><strong>oid() = [byte()]</strong></dt><dd><a name="type-oid"></a> </dd><dt><strong>oids() = [oid()]</strong></dt><dd><a name="type-oids"></a> </dd><dt><strong>rel_path() = string()</strong></dt><dd><a name="type-rel_path"></a> </dd><dt><strong>sec_type() = none | minimum | semi</strong></dt><dd><a name="type-sec_type"></a> </dd><dt><strong>snmp_app_agent_params() = term()</strong></dt><dd><a name="type-snmp_app_agent_params"></a> </dd><dt><strong>snmp_app_manager_params() = term()</strong></dt><dd><a name="type-snmp_app_manager_params"></a> </dd><dt><strong>snmpreply() = {error_status(), error_index(), varbinds()}</strong></dt><dd><a name="type-snmpreply"></a> </dd><dt><strong>user_data() = term()</strong></dt><dd><a name="type-user_data"></a> </dd><dt><strong>user_name() = atom()</strong></dt><dd><a name="type-user_name"></a> </dd><dt><strong>usm_config() = {Item, Value}</strong></dt><dd><a name="type-usm_config"></a> </dd><dt><strong>usm_user_name() = string()</strong></dt><dd><a name="type-usm_user_name"></a> </dd><dt><strong>value_type() = o('OBJECT IDENTIFIER') | i('INTEGER') | u('Unsigned32') | g('Unsigned32') | s('OCTET STRING')</strong></dt><dd><a name="type-value_type"></a> </dd><dt><strong>var_and_val() = {oid(), value_type(), value()}</strong></dt><dd><a name="type-var_and_val"></a> </dd><dt><strong>varbind() = term()</strong></dt><dd><a name="type-varbind"></a> </dd><dt><strong>varbinds() = [varbind()]</strong></dt><dd><a name="type-varbinds"></a> </dd><dt><strong>varsandvals() = [var_and_val()]</strong></dt><dd><a name="type-varsandvals"></a> </dd></dl><p>These data types are described in the documentation for
the <a href="./index">snmp/index</a> application.</p><h3>Functions</h3><h4>get_next_values(Agent, Oids, MgrAgentConfName) -&gt; SnmpReply</h4><p>Issues a synchronous SNMP get next request.</p><ul><li><span class="v">Agent = agent_name()</span></li><li><span class="v">Oids = oids()</span></li><li><span class="v">MgrAgentConfName = atom()</span></li><li><span class="v">SnmpReply = snmpreply()</span></li></ul><a name="get_next_values-3"></a><p>Issues a synchronous SNMP <strong>get next</strong> request.</p><h4>get_values(Agent, Oids, MgrAgentConfName) -&gt; SnmpReply</h4><p>Issues a synchronous SNMP get request.</p><ul><li><span class="v">Agent = agent_name()</span></li><li><span class="v">Oids = oids()</span></li><li><span class="v">MgrAgentConfName = atom()</span></li><li><span class="v">SnmpReply = snmpreply()</span></li></ul><a name="get_values-3"></a><p>Issues a synchronous SNMP <strong>get</strong> request.</p><h4>load_mibs(Mibs) -&gt; ok | {error, Reason}</h4><p>Loads the MIBs into agent snmp_master_agent.</p><ul><li><span class="v">Mibs = [MibName]</span></li><li><span class="v">MibName = string()</span></li><li><span class="v">Reason = term()</span></li></ul><a name="load_mibs-1"></a><p>Loads the MIBs into agent <strong>snmp_master_agent</strong>.</p><h4>register_agents(MgrAgentConfName, ManagedAgents) -&gt; ok | {error, Reason}</h4><p>Explicitly instructs the manager to handle this agent.</p><ul><li><span class="v">MgrAgentConfName = atom()</span></li><li><span class="v">ManagedAgents = [agent()]</span></li><li><span class="v">Reason = term()</span></li></ul><a name="register_agents-2"></a><p>Explicitly instructs the manager to handle this agent. Corresponds
to making an entry in <strong>agents.conf</strong>.</p><p>This function tries to register the specified managed agents, without
checking if any of them exist. To change a registered managed agent,
the agent must first be unregistered.</p><h4>register_users(MgrAgentConfName, Users) -&gt; ok | {error, Reason}</h4><p>Registers the manager entity (=user) responsible for specific agent(s).</p><ul><li><span class="v">MgrAgentConfName = atom()</span></li><li><span class="v">Users = [user()]</span></li><li><span class="v">Reason = term()</span></li></ul><a name="register_users-2"></a><p>Registers the manager entity (=user) responsible for specific
agent(s). Corresponds to making an entry in <strong>users.conf</strong>.</p><p>This function tries to register the specified users, without checking
if any of them exist. To change a registered user, the user must
first be unregistered.</p><h4>register_usm_users(MgrAgentConfName, UsmUsers) -&gt; ok | {error, Reason}</h4><p>Explicitly instructs the manager to handle this USM user.</p><ul><li><span class="v">MgrAgentConfName = atom()</span></li><li><span class="v">UsmUsers = [usm_user()]</span></li><li><span class="v">Reason = term()</span></li></ul><a name="register_usm_users-2"></a><p>Explicitly instructs the manager to handle this USM user.
Corresponds to making an entry in <strong>usm.conf</strong>.</p><p>This function tries to register the specified users, without checking
if any of them exist. To change a registered user, the user must
first be unregistered.</p><h4>set_info(Config) -&gt; [{Agent, OldVarsAndVals, NewVarsAndVals}]</h4><p>Returns a list of all successful set requests performed in the test case in reverse order.</p><ul><li><span class="v">Config = [{Key, Value}]</span></li><li><span class="v">Agent = agent_name()</span></li><li><span class="v">OldVarsAndVals = varsandvals()</span></li><li><span class="v">NewVarsAndVals = varsandvals()</span></li></ul><a name="set_info-1"></a><p>Returns a list of all successful <strong>set</strong> requests performed in
the test case in reverse order. The list contains the involved user
and agent, the value before <strong>set</strong>, and the new value. This is
intended to simplify the cleanup in function <strong>end_per_testcase</strong>,
that is, the undoing of the <strong>set</strong> requests and their possible
side-effects.</p><h4>set_values(Agent, VarsAndVals, MgrAgentConfName, Config) -&gt; SnmpReply</h4><p>Issues a synchronous SNMP set request.</p><ul><li><span class="v">Agent = agent_name()</span></li><li><span class="v">Oids = oids()</span></li><li><span class="v">MgrAgentConfName = atom()</span></li><li><span class="v">Config = [{Key, Value}]</span></li><li><span class="v">SnmpReply = snmpreply()</span></li></ul><a name="set_values-4"></a><p>Issues a synchronous SNMP <strong>set</strong> request.</p><h4>start(Config, MgrAgentConfName) -&gt; ok</h4><p>Equivalent to start(Config, MgrAgentConfName, undefined).</p><a name="start-2"></a><p>Equivalent to
<a href="#start-3">start-3</a>.</p><h4>start(Config, MgrAgentConfName, SnmpAppConfName) -&gt; ok</h4><p>Starts an SNMP manager and/or agent.</p><ul><li><span class="v">Config = [{Key, Value}]</span></li><li><span class="v">Key = atom()</span></li><li><span class="v">Value = term()</span></li><li><span class="v">MgrAgentConfName = atom()</span></li><li><span class="v">SnmpConfName = atom()</span></li></ul><a name="start-3"></a><p>Starts an SNMP manager and/or agent. In the manager case,
registrations of users and agents, as specified by the configuration
<strong>MgrAgentConfName</strong>, are performed. When using SNMPv3, called
USM users are also registered. Users, <strong>usm_users</strong>, and
managed agents can also be registered later using
<a href="#register_users-2">register_users-2</a>,
<a href="#register_agents-2">register_agents-2</a>,
and
<a href="#register_usm_users-2">register_usm_users-2</a>.</p><p>The agent started is called <strong>snmp_master_agent</strong>. Use
<a href="#load_mibs-1">load_mibs-1</a>
to load MIBs into the agent.</p><p>With <strong>SnmpAppConfName</strong> SNMP applications can be configured
with parameters <strong>config</strong>, <strong>mibs</strong>, <strong>net_if</strong>, and so on.
The values are merged with (and possibly override) default values
set by <strong>ct_snmp</strong>.</p><h4>stop(Config) -&gt; ok</h4><p>Stops the SNMP manager and/or agent, and removes all files created.</p><ul><li><span class="v">Config = [{Key, Value}]</span></li><li><span class="v">Key = atom()</span></li><li><span class="v">Value = term()</span></li></ul><a name="stop-1"></a><p>Stops the SNMP manager and/or agent, and removes all files
created.</p><h4>unload_mibs(Mibs) -&gt; ok | {error, Reason}</h4><p>Unloads the MIBs from agent snmp_master_agent.</p><ul><li><span class="v">Mibs = [MibName]</span></li><li><span class="v">MibName = string()</span></li><li><span class="v">Reason = term()</span></li></ul><a name="unload_mibs-1"></a><p>Unloads the MIBs from agent <strong>snmp_master_agent</strong>.</p><h4>unregister_agents(MgrAgentConfName) -&gt; ok</h4><p>Unregisters all managed agents.</p><ul><li><span class="v">MgrAgentConfName = atom()</span></li><li><span class="v">Reason = term()</span></li></ul><a name="unregister_agents-1"></a><p>Unregisters all managed agents.</p><h4>unregister_agents(MgrAgentConfName, ManagedAgents) -&gt; ok</h4><p>Unregisters the specified managed agents.</p><ul><li><span class="v">MgrAgentConfName = atom()</span></li><li><span class="v">ManagedAgents = [agent_name()]</span></li><li><span class="v">Reason = term()</span></li></ul><a name="unregister_agents-2"></a><p>Unregisters the specified managed agents.</p><h4>unregister_users(MgrAgentConfName) -&gt; ok</h4><p>Unregisters all users.</p><ul><li><span class="v">MgrAgentConfName = atom()</span></li><li><span class="v">Reason = term()</span></li></ul><a name="unregister_users-1"></a><p>Unregisters all users.</p><h4>unregister_users(MgrAgentConfName, Users) -&gt; ok</h4><p>Unregisters the specified users.</p><ul><li><span class="v">MgrAgentConfName = atom()</span></li><li><span class="v">Users = [user_name()]</span></li><li><span class="v">Reason = term()</span></li></ul><a name="unregister_users-2"></a><p>Unregisters the specified users.</p><h4>unregister_usm_users(MgrAgentConfName) -&gt; ok</h4><p>Unregisters all USM users.</p><ul><li><span class="v">MgrAgentConfName = atom()</span></li><li><span class="v">Reason = term()</span></li></ul><a name="unregister_usm_users-1"></a><p>Unregisters all USM users.</p><h4>unregister_usm_users(MgrAgentConfName, UsmUsers) -&gt; ok</h4><p>Unregisters the specified USM users.</p><ul><li><span class="v">MgrAgentConfName = atom()</span></li><li><span class="v">UsmUsers = [usm_user_name()]</span></li><li><span class="v">Reason = term()</span></li></ul><a name="unregister_usm_users-2"></a><p>Unregisters the specified USM users.</p><h3>ct_telnet</h3><p>Common Test specific layer on top of Telnet client ct_telnet_client.erl</p><p><strong>Common Test</strong> specific layer on top of Telnet client
<strong>ct_telnet_client.erl</strong>.Use this module to set up Telnet connections, send commands, and
perform string matching on the result. For information about how to use
<strong>ct_telnet</strong> and configure connections, specifically for UNIX hosts,
see the
<a href="unix_telnet">unix_telnet</a> manual page.
Default values defined in <strong>ct_telnet</strong>:<a name="Default_values"></a><ul><li><p>Connection timeout (time to wait for connection) = 10
seconds</p></li><li><p>Command timeout (time to wait for a command to return) =
10 seconds</p></li><li><p>Max number of reconnection attempts = 3</p></li><li><p>Reconnection interval (time to wait in between
reconnection attempts) = 5 seconds</p></li><li><p>Keep alive (sends NOP to the server every 8 sec if
connection is idle) = <strong>true</strong></p></li><li><p>Polling limit (max number of times to poll to get a
remaining string terminated) = 0</p></li><li><p>Polling interval (sleep time between polls) = 1 second</p> </li><li><p>The TCP_NODELAY option for the telnet socket
is disabled (set to <strong>false</strong>) per default</p></li></ul>These parameters can be modified by the user with the following
configuration term:<pre>
 {telnet_settings, [{connect_timeout,Millisec},
                    {command_timeout,Millisec},
                    {reconnection_attempts,N},
                    {reconnection_interval,Millisec},
                    {keep_alive,Bool},
                    {poll_limit,N},
                    {poll_interval,Millisec},
                    {tcp_nodelay,Bool}]}.</pre><strong>Millisec = integer(), N = integer()</strong>Enter the <strong>telnet_settings</strong> term in a configuration file included
in the test and <strong>ct_telnet</strong> retrieves the information
automatically.<strong>keep_alive</strong> can be specified per connection, if necessary. For
details, see
<a href="unix_telnet">unix_telnet</a>.</p><h4>Logging</h4><a name="Logging"></a><p>The default logging behavior of <strong>ct_telnet</strong> is to print information
about performed operations, commands, and their corresponding results to
the test case HTML log. The following is not printed to the HTML
log: text strings sent from the Telnet server that are not explicitly
received by a <strong>ct_telnet</strong> function, such as <strong>expect/3</strong>.
However, <strong>ct_telnet</strong> can be configured to use a special purpose
event handler, implemented in <strong>ct_conn_log_h</strong>, for logging
<em>all</em> Telnet traffic. To use this handler, install a <strong>Common Test</strong> hook named <strong>cth_conn_log</strong>. Example (using the test suite
information function):</p><pre>
 suite() -&gt;
     [{ct_hooks, [{cth_conn_log, [{conn_mod(),hook_options()}]}]}].</pre><p><strong>conn_mod()</strong> is the name of the <strong>Common Test</strong> module
implementing the connection protocol, that is, <strong>ct_telnet</strong>.</p><p>The <strong>cth_conn_log</strong> hook performs unformatted logging of Telnet
data to a separate text file. All Telnet communication is captured and
printed, including any data sent from the server. The link to
this text file is located at the top of the test case HTML log.</p><p>By default, data for all Telnet connections is logged in one common
file (named <strong>default</strong>), which can get messy, for example, if
multiple Telnet sessions are running in parallel. Therefore a separate
log file can be created for each connection. To configure this, use hook
option <strong>hosts</strong> and list the names of the servers/connections
to be used in the suite. The connections must be named for this to
work (see
<a href="#open-1">open-1</a>).</p><p>Hook option <strong>log_type</strong> can be used to change the
<strong>cth_conn_log</strong> behavior. The default value of this option is
<strong>raw</strong>, which results in the behavior described above. If the value
is set to <strong>html</strong>, all Telnet communication is printed to the test
case HTML log instead.</p><p>All <strong>cth_conn_log</strong> hook options described can also be
specified in a configuration file with configuration variable
<strong>ct_conn_log</strong>.</p><p><em>Example:</em></p><pre>
 {ct_conn_log, [{ct_telnet,[{log_type,raw},
                            {hosts,[key_or_name()]}]}]}</pre><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>Hook options specified in a configuration file overwrite any
hard-coded hook options in the test suite.</p></div><a name="Logging_example"></a><p><em>Logging Example:</em></p><p>The following <strong>ct_hooks</strong> statement causes printing of Telnet
traffic to separate logs for the connections <strong>server1</strong> and
<strong>server2</strong>. Traffic for any other connections is logged in the
default Telnet log.</p><pre>
 suite() -&gt;
     [{ct_hooks,
       [{cth_conn_log, [{ct_telnet,[{hosts,[server1,server2]}]}]}]}].</pre><p>As previously explained, this specification can also be provided by an
entry like the following in a configuration file:</p><pre>
 {ct_conn_log, [{ct_telnet,[{hosts,[server1,server2]}]}]}.</pre><p>In this case the <strong>ct_hooks</strong> statement in the test suite can look
as follows:</p><pre>
 suite() -&gt;
     [{ct_hooks, [{cth_conn_log, []}]}].</pre><h4>Data Types</h4><a name="types"></a><dl><dt><strong>connection() = handle() | {target_name(), connection_type()} | target_name()</strong></dt><dd><a name="type-connection"></a> <p>For <strong>target_name()</strong>, see module
<a href="ct">ct</a>.</p></dd><dt><strong>connection_type() = telnet | ts1 | ts2</strong></dt><dd><a name="type-connection_type"></a> </dd><dt><strong>handle() = handle()</strong></dt><dd><a name="type-handle"></a> <p>Handle for a specific Telnet connection, see module
<a href="ct">ct</a>.</p></dd><dt><strong>prompt_regexp() = string()</strong></dt><dd><a name="type-prompt_regexp"></a> <p>Regular expression matching all possible prompts for a specific
target type. <strong>regexp</strong> must not have any groups, that is, when
matching, <strong>re:run/3</strong> (in STDLIB) must return a list with
one single element.</p></dd></dl><h3>Functions</h3><h4>close(Connection) -&gt; ok | {error, Reason}</h4><p>Closes the Telnet connection and stops the process managing it.</p><ul><li><span class="v">Connection = connection()</span></li><li><span class="v">Reason = term()</span></li></ul><a name="close-1"></a><p>Closes the Telnet connection and stops the process managing it.</p><p>A connection can be associated with a target name and/or a handle.
If <strong>Connection</strong> has no associated target name, it can only
be closed with the handle value (see
<a href="#open-4">open-4</a>).</p><h4>cmd(Connection, Cmd) -&gt; {ok, Data} | {error, Reason}</h4><p>Equivalent to cmd(Connection, Cmd, []).</p><a name="cmd-2"></a><p>Equivalent to
<a href="#cmd-3">cmd-3</a>.</p><h4>cmd(Connection, Cmd, Opts) -&gt; {ok, Data} | {error, Reason}</h4><p>Sends a command through Telnet and waits for prompt.</p><ul><li><span class="v">Connection = connection()</span></li><li><span class="v">Cmd = string()</span></li><li><span class="v">Opts = [Opt]</span></li><li><span class="v">Opt = {timeout, timeout()} | {newline, boolean() | string()}</span></li><li><span class="v">Data = [string()]</span></li><li><span class="v">Reason = term()</span></li></ul><a name="cmd-3"></a><p>Sends a command through Telnet and waits for prompt.</p><p>By default, this function adds "\n" to the end of the
specified command. If this is not desired, use option
<strong>{newline,false}</strong>. This is necessary, for example, when
sending Telnet command sequences prefixed with character
Interpret As Command (IAC). Option <strong>{newline,string()}</strong>
can also be used if a different line end than "\n" is
required, for instance <strong>{newline,"\r\n"}</strong>, to add both
carriage return and newline characters.</p><p>Option <strong>timeout</strong> specifies how long the client must wait
for prompt. If the time expires, the function returns
<strong>{error,timeout}</strong>. For information about the default value
for the command timeout, see the
<a href="#Default_values">list of default values</a>
in the beginning of this module.</p><h4>cmdf(Connection, CmdFormat, Args) -&gt; {ok, Data} | {error, Reason}</h4><p>Equivalent to cmdf(Connection, CmdFormat, Args, []).</p><a name="cmdf-3"></a><p>Equivalent to
<a href="#cmdf-4">cmdf-4</a>.</p><h4>cmdf(Connection, CmdFormat, Args, Opts) -&gt; {ok, Data} | {error, Reason}</h4><p>Sends a Telnet command and waits for prompt (uses a format string and a list of arguments to build the command).</p><ul><li><span class="v">Connection = connection()</span></li><li><span class="v">CmdFormat = string()</span></li><li><span class="v">Args = list()</span></li><li><span class="v">Opts = [Opt]</span></li><li><span class="v">Opt = {timeout, timeout()} | {newline, boolean() | string()}</span></li><li><span class="v">Data = [string()]</span></li><li><span class="v">Reason = term()</span></li></ul><a name="cmdf-4"></a><p>Sends a Telnet command and waits for prompt (uses a format string
and a list of arguments to build the command).</p><p>For details, see
<a href="#cmd-3">cmd-3</a>.</p><h4>expect(Connection, Patterns) -&gt; term()</h4><p>Equivalent to expect(Connections, Patterns, []).</p><a name="expect-2"></a><p>Equivalent to
<a href="#expect-3">expect-3</a>.</p><h4>expect(Connection, Patterns, Opts) -&gt; {ok, Match} | {ok, MatchList, HaltReason} | {error, Reason}</h4><p>Gets data from Telnet and waits for the expected pattern.</p><ul><li><span class="v">Connection = connection()</span></li><li><span class="v">Patterns = Pattern | [Pattern]</span></li><li><span class="v">Pattern = string() | {Tag, string()} | prompt | {prompt, Prompt}</span></li><li><span class="v">Prompt = string()</span></li><li><span class="v">Tag = term()</span></li><li><span class="v">Opts = [Opt]</span></li><li><span class="v">Opt = {idle_timeout, IdleTimeout} | {total_timeout, TotalTimeout} | repeat | {repeat, N} | sequence | {halt, HaltPatterns} | ignore_prompt | no_prompt_check | wait_for_prompt | {wait_for_prompt, Prompt}</span></li><li><span class="v">IdleTimeout = infinity | integer()</span></li><li><span class="v">TotalTimeout = infinity | integer()</span></li><li><span class="v">N = integer()</span></li><li><span class="v">HaltPatterns = Patterns</span></li><li><span class="v">MatchList = [Match]</span></li><li><span class="v">Match = RxMatch | {Tag, RxMatch} | {prompt, Prompt}</span></li><li><span class="v">RxMatch = [string()]</span></li><li><span class="v">HaltReason = done | Match</span></li><li><span class="v">Reason = timeout | {prompt, Prompt}</span></li></ul><a name="expect-3"></a><p>Gets data from Telnet and waits for the expected pattern.</p><p><strong>Pattern</strong> can be a POSIX regular expression. The function
returns when a pattern is successfully matched (at least one, in
the case of multiple patterns).</p><p><strong>RxMatch</strong> is a list of matched strings. It looks as
follows <strong>[FullMatch, SubMatch1, SubMatch2, ...]</strong>, where
<strong>FullMatch</strong> is the string matched by the whole regular
expression, and <strong>SubMatchN</strong> is the string that matched
subexpression number <strong>N</strong>. Subexpressions are denoted with
<strong>'(' ')'</strong> in the regular expression.</p><p>If a <strong>Tag</strong> is specified, the returned <strong>Match</strong> also
includes the matched <strong>Tag</strong>. Otherwise, only <strong>RxMatch</strong>
is returned.</p><p><em>Options:</em></p><dl><dt><strong>idle_timeout</strong></dt><dd><p>Indicates that the function must return if the Telnet
client is idle (that is, if no data is received) for more than
<strong>IdleTimeout</strong> milliseconds. Default time-out is 10
seconds.</p></dd><dt><strong>total_timeout</strong></dt><dd><p>Sets a time limit for the complete <strong>expect</strong> operation.
After <strong>TotalTimeout</strong> milliseconds, <strong>{error,timeout}</strong>
is returned. Default is <strong>infinity</strong> (that is, no time
limit).</p></dd><dt><strong>ignore_prompt | no_prompt_check</strong></dt><dd><p>&gt;The function returns when a prompt is received, even if
no pattern has yet been matched, and 
<strong>{error,{prompt,Prompt}}</strong> is returned. However, this
behavior can be modified with option <strong>ignore_prompt</strong> or
option <strong>no_prompt_check</strong>, which tells <strong>expect</strong> to
return only when a match is found or after a time-out.</p></dd><dt><strong>ignore_prompt</strong></dt><dd><p><strong>ct_telnet</strong> ignores any prompt found. This option is
useful if data sent by the server can include a pattern
matching prompt <strong>regexp</strong> (as returned by
<strong>TargedMod:get_prompt_regexp/0</strong>), but is not to not cause
the function to return.</p></dd><dt><strong>no_prompt_check</strong></dt><dd><p><strong>ct_telnet</strong> does not search for a prompt at all. This
is useful if, for example, <strong>Pattern</strong> itself matches the
prompt.</p></dd><dt><strong>wait_for_prompt</strong></dt><dd><p>Forces <strong>ct_telnet</strong> to wait until the prompt string
is received before returning (even if a pattern has already been
matched). This is equal to calling
<strong>expect(Conn, Patterns++[{prompt,Prompt}], [sequence|Opts])</strong>.
Notice that option <strong>idle_timeout</strong> and <strong>total_timeout</strong>
can abort the operation of waiting for prompt.</p></dd><dt><strong>repeat | repeat, N</strong></dt><dd><p>The pattern(s) must be matched multiple times. If <strong>N</strong>
is specified, the pattern(s) are matched <strong>N</strong> times, and
the function returns <strong>HaltReason = done</strong>. This option can be
interrupted by one or more <strong>HaltPatterns</strong>. <strong>MatchList</strong>
is always returned, that is, a list of <strong>Match</strong> instead of
only one <strong>Match</strong>. Also <strong>HaltReason</strong> is returned.</p> </dd><dt><strong>sequence</strong></dt><dd><p>All patterns must be matched in a sequence. A match is not
concluded until all patterns are matched. This option can be
interrupted by one or more <strong>HaltPatterns</strong>. <strong>MatchList</strong>
is always returned, that is, a list of <strong>Match</strong> instead of
only one <strong>Match</strong>. Also <strong>HaltReason</strong> is returned.</p>  </dd></dl><p><em>Example 1:</em></p><pre>
 expect(Connection,[{abc,"ABC"},{xyz,"XYZ"}],[sequence,{halt,[{nnn,"NNN"}]}])</pre><p>First this tries to match <strong>"ABC"</strong>, and then <strong>"XYZ"</strong>, but
if <strong>"NNN"</strong> appears,  the function returns
<strong>{error,{nnn,["NNN"]}}</strong>. If both <strong>"ABC"</strong> and <strong>"XYZ"</strong>
are matched, the function returns <strong>{ok,[AbcMatch,XyzMatch]}</strong>.</p><p><em>Example 2:</em></p><pre>
 expect(Connection,[{abc,"ABC"},{xyz,"XYZ"}],[{repeat,2},{halt,[{nnn,"NNN"}]}])</pre><p>This tries to match <strong>"ABC"</strong> or <strong>"XYZ"</strong> twice. If
<strong>"NNN"</strong> appears, the function returns
<strong>HaltReason = {nnn,["NNN"]}</strong>.</p><p>Options <strong>repeat</strong> and <strong>sequence</strong> can be combined to
match a sequence multiple times.</p><h4>get_data(Connection) -&gt; {ok, Data} | {error, Reason}</h4><p>Gets all data received by the Telnet client since the last command was sent.</p><ul><li><span class="v">Connection = connection()</span></li><li><span class="v">Data = [string()]</span></li><li><span class="v">Reason = term()</span></li></ul><a name="get_data-1"></a><p>Gets all data received by the Telnet client since the last
command was sent. Only newline-terminated strings are returned.
If the last received string has not yet been terminated, the
connection can be polled automatically until the string is
complete.</p><p>The polling feature is controlled by the configuration values
<strong>poll_limit</strong> and <strong>poll_interval</strong> and is by default
disabled. This means that the function immediately returns all
complete strings received and saves a remaining non-terminated
string for a later <strong>get_data</strong> call.</p><h4>open(Name) -&gt; {ok, Handle} | {error, Reason}</h4><p>Equivalent to open(Name, telnet).</p><a name="open-1"></a><p>Equivalent to
<a href="#open-2">open-2</a>.</p><h4>open(Name, ConnType) -&gt; {ok, Handle} | {error, Reason}</h4><p>Opens a Telnet connection to the specified target host.</p><ul><li><span class="v">Name = target_name()</span></li><li><span class="v">ConnType = connection_type()</span></li><li><span class="v">Handle = handle()</span></li><li><span class="v">Reason = term()</span></li></ul><a name="open-2"></a><p>Opens a Telnet connection to the specified target host.</p><h4>open(KeyOrName, ConnType, TargetMod) -&gt; {ok, Handle} | {error, Reason}</h4><p>Equivalent to open(KeyOrName, ConnType, TargetMod, []).</p><a name="open-3"></a><p>Equivalent to
<a href="#open-4">open-4</a>.</p><h4>open(KeyOrName, ConnType, TargetMod, Extra) -&gt; {ok, Handle} | {error, Reason}</h4><p>Opens a Telnet connection to the specified target host.</p><ul><li><span class="v">KeyOrName = Key | Name</span></li><li><span class="v">Key = atom()</span></li><li><span class="v">Name = target_name()</span></li><li><span class="v">ConnType = connection_type()</span></li><li><span class="v">TargetMod = atom()</span></li><li><span class="v">Extra = term()</span></li><li><span class="v">Handle = handle()</span></li><li><span class="v">Reason = term()</span></li></ul><a name="open-4"></a><p>Opens a Telnet connection to the specified target host.</p><p>The target data must exist in a configuration file. The connection
can be associated with <strong>Name</strong> and/or the returned <strong>Handle</strong>.
To allocate a name for the target, use one of the following
alternatives:</p><ul><li><p><a href="./ct#require-2">ct#require-2</a>
in a test case</p></li><li><p>A <strong>require</strong> statement in the suite information
function (<strong>suite/0</strong>)</p></li><li><p>A <strong>require</strong> statement in a test case information
function</p></li></ul><p>If you want the connection to be associated with <strong>Handle</strong> only
(if you, for example, need to open multiple connections to a host),
use <strong>Key</strong>, the configuration variable name, to specify the
target. Notice that a connection without an associated target name
can only be closed with the <strong>Handle</strong> value.</p><p><strong>TargetMod</strong> is a module that exports the functions
<strong>connect(Ip, Port, KeepAlive, Extra)</strong> and
<strong>get_prompt_regexp()</strong> for the specified <strong>TargetType</strong>
(for example, <strong>unix_telnet</strong>).</p><p>For <strong>target_name()</strong>, see module
<a href="ct">ct</a>.</p><p>See also
<a href="./ct#require-2">ct#require-2</a>.</p><h4>send(Connection, Cmd) -&gt; ok | {error, Reason}</h4><p>Equivalent to send(Connection, Cmd, []).</p><a name="send-2"></a><p>Equivalent to
<a href="#send-3">send-3</a>.</p><h4>send(Connection, Cmd, Opts) -&gt; ok | {error, Reason}</h4><p>Sends a Telnet command and returns immediately.</p><ul><li><span class="v">Connection = connection()</span></li><li><span class="v">Cmd = string()</span></li><li><span class="v">Opts = [Opt]</span></li><li><span class="v">Opt = {newline, boolean() | string()}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="send-3"></a><p>Sends a Telnet command and returns immediately.</p><p>By default, this function adds "\n" to the end of the
specified command. If this is not desired, option
<strong>{newline,false}</strong> can be used. This is necessary, for example,
when sending Telnet command sequences prefixed with character   
Interpret As Command (IAC). Option <strong>{newline,string()}</strong>
can also be used if a different line end than "\n" is
required, for instance <strong>{newline,"\r\n"}</strong>, to add both
carriage return and newline characters.</p><p>The resulting output from the command can be read with
<a href="#get_data-1">get_data-1</a> or
<a href="#expect-2">expect-2</a>.</p><h4>sendf(Connection, CmdFormat, Args) -&gt; ok | {error, Reason}</h4><p>Equivalent to sendf(Connection, CmdFormat, Args, []).</p><a name="sendf-3"></a><p>Equivalent to
<a href="#sendf-4">sendf-4</a>.</p><h4>sendf(Connection, CmdFormat, Args, Opts) -&gt; ok | {error, Reason}</h4><p>Sends a Telnet command and returns immediately (uses a format string and a list of arguments to build the command).</p><ul><li><span class="v">Connection = connection()</span></li><li><span class="v">CmdFormat = string()</span></li><li><span class="v">Args = list()</span></li><li><span class="v">Opts = [Opt]</span></li><li><span class="v">Opt = {newline, boolean() | string()}</span></li><li><span class="v">Reason = term()</span></li></ul><a name="sendf-4"></a><p>Sends a Telnet command and returns immediately (uses a format
string and a list of arguments to build the command).</p><p>For details, see
<a href="#send-3">send-3</a>.</p><h4>See Also</h4><p><a href="unix_telnet">unix_telnet</a></p><h3>unix_telnet</h3><p>Callback module for ct_telnet, for connecting to a Telnet
    server on a UNIX host.</p><p>Callback module for
<a href="ct_telnet">ct_telnet</a>,
for connecting to a Telnet server on a UNIX host.It requires the following entry in the configuration file:<pre>
 {unix,[{telnet,HostNameOrIpAddress},
        {port,PortNum},                 % optional
        {username,UserName},
        {password,Password},
        {keep_alive,Bool}]}.            % optional</pre>To communicate through Telnet to the host specified by
<strong>HostNameOrIpAddress</strong>, use the interface functions in
<a href="ct_telnet">ct_telnet</a>, for example,
<strong>open(Name)</strong> and <strong>cmd(Name,Cmd)</strong>.<strong>Name</strong> is the name you allocated to the Unix host in your
<strong>require</strong> statement, for example:<pre>
 suite() -&gt; [{require,Name,{unix,[telnet]}}].</pre>or<pre>
 ct:require(Name,{unix,[telnet]}).</pre>The "keep alive" activity (that is, that <strong>Common Test</strong> sends NOP
to the server every 10 seconds if the connection is idle) can be
enabled or disabled for one particular connection as described here.
It can be disabled for all connections using <strong>telnet_settings</strong>
(see <a href="ct_telnet">ct_telnet</a>).The <strong>{port,PortNum}</strong> tuple is optional and if omitted, default 
Telnet port 23 is used. Also the <strong>keep_alive</strong> tuple is optional,
and the value defauls to <strong>true</strong> (enabled).</p><h3>Functions</h3><h4>connect(ConnName, Ip, Port, Timeout, KeepAlive, TCPNoDelay, Extra) -&gt; {ok, Handle} | {error, Reason}</h4><p>Callback for ct_telnet.erl.</p><ul><li><span class="v">ConnName = target_name()</span></li><li><span class="v">Ip = string() | {integer(), integer(), integer(), integer()}</span></li><li><span class="v">Port = integer()</span></li><li><span class="v">Timeout = integer()</span></li><li><span class="v">KeepAlive = bool()</span></li><li><span class="v">TCPNoDelay = bool()</span></li><li><span class="v">Extra = target_name() | {Username, Password}</span></li><li><span class="v">Username = string()</span></li><li><span class="v">Password = string()</span></li><li><span class="v">Handle = handle()</span></li><li><span class="v">Reason = term()</span></li></ul><a name="connect-6"></a><p>Callback for <strong>ct_telnet.erl</strong>.</p><p>Setup Telnet connection to a Unix host.</p><p>For <strong>target_name()</strong>, see
<a href="ct">ct</a>. For <strong>handle()</strong>, see
<a href="ct_telnet">ct_telnet</a>.</p><h4>get_prompt_regexp() -&gt; PromptRegexp</h4><p>Callback for ct_telnet.erl.</p><ul><li><span class="v">PromptRegexp = prompt_regexp()</span></li></ul><a name="get_prompt_regexp-0"></a><p>Callback for <strong>ct_telnet.erl</strong>.</p><p>Returns a suitable <strong>regexp</strong> string matching common prompts
for users on Unix hosts.</p><p>For <strong>prompt_regexp()</strong>, see
<a href="ct_telnet">ct_telnet</a>.</p><h4>See Also</h4><p><a href="ct">ct</a>,
<a href="ct_telnet">ct_telnet</a></p><h3>ct_slave</h3><p>Common Test framework functions for starting and stopping
    nodes for Large-Scale Testing.</p><p><strong>Common Test</strong> framework functions for starting and stopping nodes
for Large-Scale Testing.This module exports functions used by the <strong>Common Test</strong>
Master to start and stop "slave" nodes. It is the default callback
module for the <strong>{init, node_start}</strong> term in the Test
Specification.</p><h3>Functions</h3><h4>start(Node) -&gt; Result</h4><p>Starts an Erlang node with name Node on the local host.</p><ul><li><span class="v">Node = atom()</span></li><li><span class="v">Result = {ok, NodeName} | {error, Reason, NodeName}</span></li><li><span class="v">Reason = already_started | started_not_connected | boot_timeout | init_timeout | startup_timeout | not_alive</span></li><li><span class="v">NodeName = atom()</span></li></ul><a name="start-1"></a><p>Starts an Erlang node with name <strong>Node</strong> on the local host.</p><p>See also
<a href="#start-3">start-3</a>.</p><h4>start(HostOrNode, NodeOrOpts) -&gt; Result</h4><p>Starts an Erlang node with default options on a specified host, or on the local host with specified options.</p><ul><li><span class="v">HostOrNode = atom()</span></li><li><span class="v">NodeOrOpts = atom() | list()</span></li><li><span class="v">Result = {ok, NodeName} | {error, Reason, NodeName}</span></li><li><span class="v">Reason = already_started | started_not_connected | boot_timeout | init_timeout | startup_timeout | not_alive</span></li><li><span class="v">NodeName = atom()</span></li></ul><a name="start-2"></a><p>Starts an Erlang node with default options on a specified host, or
on the local host with specified options. That is, the call is
interpreted as <strong>start(Host, Node)</strong> when the second argument is
atom-valued and <strong>start(Node, Opts)</strong> when it is list-valued.</p><p>See also
<a href="#start-3">start-3</a>.</p><h4>start(Host, Node, Opts) -&gt; Result</h4><p>Starts an Erlang node with name Node on host Host as specified by the combination of options in Opts.</p><ul><li><span class="v">Node = atom()</span></li><li><span class="v">Host = atom()</span></li><li><span class="v">Opts = [OptTuples]</span></li><li><span class="v">OptTuples = {username, Username} | {password, Password} | {boot_timeout, BootTimeout} | {init_timeout, InitTimeout} | {startup_timeout, StartupTimeout} | {startup_functions, StartupFunctions} | {monitor_master, Monitor} | {kill_if_fail, KillIfFail} | {erl_flags, ErlangFlags} | {env, [{EnvVar, Value}]}</span></li><li><span class="v">Username = string()</span></li><li><span class="v">Password = string()</span></li><li><span class="v">BootTimeout = integer()</span></li><li><span class="v">InitTimeout = integer()</span></li><li><span class="v">StartupTimeout = integer()</span></li><li><span class="v">StartupFunctions = [StartupFunctionSpec]</span></li><li><span class="v">StartupFunctionSpec = {Module, Function, Arguments}</span></li><li><span class="v">Module = atom()</span></li><li><span class="v">Function = atom()</span></li><li><span class="v">Arguments = [term]</span></li><li><span class="v">Monitor = bool()</span></li><li><span class="v">KillIfFail = bool()</span></li><li><span class="v">ErlangFlags = string()</span></li><li><span class="v">EnvVar = string()</span></li><li><span class="v">Value = string()</span></li><li><span class="v">Result = {ok, NodeName} | {error, Reason, NodeName}</span></li><li><span class="v">Reason = already_started | started_not_connected | boot_timeout | init_timeout | startup_timeout | not_alive</span></li><li><span class="v">NodeName = atom()</span></li></ul><a name="start-3"></a><p>Starts an Erlang node with name <strong>Node</strong> on host <strong>Host</strong> as
specified by the combination of options in <strong>Opts</strong>.</p><p>Options <strong>Username</strong> and <strong>Password</strong> are used to log on to the
remote host <strong>Host</strong>. <strong>Username</strong>, if omitted, defaults to
the current username. <strong>Password</strong> is empty by default.</p><p>A list of functions specified in option <strong>Startup</strong> are
executed after startup of the node. Notice that all used modules
are to be present in the code path on <strong>Host</strong>.</p><p>The time-outs are applied as follows:</p><dl><dt><strong>BootTimeout</strong></dt><dd><p>The time to start the Erlang node, in seconds. Defaults to
3 seconds. If the node is not pingable within this time, the result
<strong>{error, boot_timeout, NodeName}</strong> is returned.</p></dd><dt><strong>InitTimeout</strong></dt><dd><p>The time to wait for the node until it calls the internal
callback function informing master about a successful startup.
Defaults to 1 second. In case of a timed out message, the result
<strong>{error, init_timeout, NodeName}</strong> is returned.</p></dd><dt><strong>StartupTimeout</strong></dt><dd><p>The time to wait until the node stops to run
<strong>StartupFunctions</strong>. Defaults to 1 second. If this time-out
occurs, the result <strong>{error, startup_timeout, NodeName}</strong> is
returned.</p></dd></dl><p><em>Options:</em></p><dl><dt><strong>monitor_master</strong></dt><dd><p>Specifies if the slave node is to be stopped if the
master node stops. Defaults to <strong>false</strong>.</p></dd><dt><strong>kill_if_fail</strong></dt><dd><p>Specifies if the slave node is to be killed if a time-out
occurs during initialization or startup. Defaults to <strong>true</strong>.
Notice that the node can also be still alive it the boot time-out
occurred, but it is not killed in this case.</p></dd><dt><strong>erlang_flags</strong></dt><dd><p>Specifies which flags are added to the parameters of the
executable <strong>erl</strong>.</p></dd><dt><strong>env</strong></dt><dd><p>Specifies a list of environment variables that will extend
the environment.</p></dd></dl><p><em>Special return values:</em></p><ul><li><p><strong>{error, already_started, NodeName}</strong> if the node
with the specified name is already started on a specified
host.</p></li><li><p><strong>{error, started_not_connected, NodeName}</strong> if the
node is started, but not connected to the master node.</p></li><li><p><strong>{error, not_alive, NodeName}</strong> if the node on which
<a href="#start-3">start-3</a> is
called, is not alive. Notice that <strong>NodeName</strong> is the name of
the current node in this case.</p></li></ul><h4>stop(Node) -&gt; Result</h4><p>Stops the running Erlang node with name Node on the local host.</p><ul><li><span class="v">Node = atom()</span></li><li><span class="v">Result = {ok, NodeName} | {error, Reason, NodeName}</span></li><li><span class="v">Reason = not_started | not_connected | stop_timeout</span></li></ul><a name="stop-1"></a><p>Stops the running Erlang node with name <strong>Node</strong> on the local
host.</p><h4>stop(Host, Node) -&gt; Result</h4><p>Stops the running Erlang node with name Node on host Host.</p><ul><li><span class="v">Host = atom()</span></li><li><span class="v">Node = atom()</span></li><li><span class="v">Result = {ok, NodeName} | {error, Reason, NodeName}</span></li><li><span class="v">Reason = not_started | not_connected | stop_timeout</span></li><li><span class="v">NodeName = atom()</span></li></ul><a name="stop-2"></a><p>Stops the running Erlang node with name <strong>Node</strong> on host
<strong>Host</strong>.</p><h3>ct_hooks</h3><p>A callback interface on top of Common Test.</p><p>The <em>Common Test Hook (CTH)</em> framework allows extensions of the
default behavior of <strong>Common Test</strong> by callbacks before and after all
test suite calls. It is intended for advanced users of <strong>Common Test</strong>
who want to abstract out behavior that is common to multiple test suites.
In brief, CTH allows you to:<ul><li><p>Manipulate the runtime configuration before each suite
configuration call.</p></li><li><p>Manipulate the return of all suite configuration calls and by
extension the result of the test themselves.</p></li></ul>The following sections describe the mandatory and optional CTH
functions that <strong>Common Test</strong> calls during test execution.
For more details, see section
<a href="ct_hooks_chapter">Common Test Hooks</a> in the
User's Guide.For information about how to add a CTH to your suite, see section
<a href="./ct_hooks_chapter#installing">Installing a CTH</a>
in the User's Guide.</p><h4>Callback Functions</h4><p>The following functions define the callback interface for a CTH.</p><h3>Functions</h3><h4>Module:init(Id, Opts) -&gt; {ok, State} | {ok, State, Priority}</h4><p>Initiates the Common Test Hook.</p><ul><li><span class="v">Id = reference() | term()</span></li><li><span class="v">Opts = term()</span></li><li><span class="v">State = term()</span></li><li><span class="v">Priority = integer()</span></li></ul><p>MANDATORY</p><p>This function is always called before any other callback function.
Use it to initiate any common state. It is to return a state for
this CTH.</p><p><strong>Id</strong> is either the return value of
<a href="#Module:id-1">Module:id-1</a>,
or a <strong>reference</strong> (created using
<a href="../erts/erlang#make_ref-0">erlang:make_ref/0</a>
in ERTS) if
<a href="#Module:id-1">Module:id-1</a>
is not implemented.</p><p><strong>Priority</strong> is the relative priority of this hook. Hooks with a
lower priority are executed first. If no priority is specified, it 
is set to <strong>0</strong>.</p><p>For details about when <strong>init</strong> is called, see section
<a href="./ct_hooks_chapter#scope">CTH Scope</a>
in the User's Guide.</p><h4>Module:post_groups(SuiteName, GroupDefs) -&gt; NewGroupDefs</h4><p>Called after groups/0.</p><ul><li><span class="v">SuiteName = atom()</span></li><li><span class="v">GroupDefs = NewGroupDefs = [Group]</span></li><li><span class="v">Group = {GroupName,Properties,GroupsAndTestCases}</span></li><li><span class="v">GroupName = atom()</span></li><li><span class="v">Properties = [parallel | sequence | Shuffle | {GroupRepeatType,N}]</span></li><li><span class="v">GroupsAndTestCases = [Group | {group,GroupName} | TestCase | {testcase,TestCase,TCRepeatProps}]</span></li><li><span class="v">TestCase = atom()</span></li><li><span class="v">TCRepeatProps = [{repeat,N} | {repeat_until_ok,N} | {repeat_until_fail,N}]</span></li><li><span class="v">Shuffle = shuffle | {shuffle,Seed}</span></li><li><span class="v">Seed = {integer(),integer(),integer()}</span></li><li><span class="v">GroupRepeatType = repeat | repeat_until_all_ok | repeat_until_all_fail | repeat_until_any_ok | repeat_until_any_fail</span></li><li><span class="v">N = integer() | forever</span></li></ul><p>OPTIONAL</p><p>This function is called after
<a href="./common_test#Module:groups-0">common_test#Module:groups-0</a>.
It is used to modify the test group definitions, for
instance to add or remove groups or change group properties.</p><p><strong>GroupDefs</strong> is what
<a href="./common_test#Module:groups-0">common_test#Module:groups-0</a>
returned, that is, a list of group definitions.</p><p><strong>NewGroupDefs</strong> is the possibly modified version of this list.</p><p>This function is called only if the CTH is added before
<strong>init_per_suite</strong> is run. For details, see section
<a href="./ct_hooks_chapter#scope">CTH Scope</a>
in the User's Guide.</p><p>Notice that for CTHs that are installed by means of the
<a href="./common_test#Module:suite-0">common_test#Module:suite-0</a>
function, <strong>post_groups/2</strong> is called before
the <a href="#Module:init-2">Module:init-2</a>
hook function. However, for CTHs that are installed by means
of the CT start flag,
the <a href="#Module:init-2">Module:init-2</a>
function is called first.</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>Prior to each test execution, Common Test does a
simulated test run in order to count test suites, groups
and cases for logging purposes. This causes
the <strong>post_groups/2</strong> hook function to always be called
twice. For this reason, side effects are best avoided in
this callback.</p></div><h4>Module:post_all(SuiteName, Return, GroupDefs) -&gt; NewReturn</h4><p>Called after all/0.</p><ul><li><span class="v">SuiteName = atom()</span></li><li><span class="v">Return = NewReturn = Tests | {skip,Reason}</span></li><li><span class="v">Tests = [TestCase | {testcase,TestCase,TCRepeatProps} | {group,GroupName} | {group,GroupName,Properties} | {group,GroupName,Properties,SubGroups}]</span></li><li><span class="v">TestCase = atom()</span></li><li><span class="v">TCRepeatProps = [{repeat,N} | {repeat_until_ok,N} | {repeat_until_fail,N}]</span></li><li><span class="v">GroupName = atom()</span></li><li><span class="v">Properties = GroupProperties | default</span></li><li><span class="v">SubGroups = [{GroupName,Properties} | {GroupName,Properties,SubGroups}]</span></li><li><span class="v">Shuffle = shuffle | {shuffle,Seed}</span></li><li><span class="v">Seed = {integer(),integer(),integer()}</span></li><li><span class="v">GroupRepeatType = repeat | repeat_until_all_ok | repeat_until_all_fail | repeat_until_any_ok | repeat_until_any_fail</span></li><li><span class="v">N = integer() | forever</span></li><li><span class="v">GroupDefs = NewGroupDefs = [Group]</span></li><li><span class="v">Group = {GroupName,GroupProperties,GroupsAndTestCases}</span></li><li><span class="v">GroupProperties = [parallel | sequence | Shuffle | {GroupRepeatType,N}]</span></li><li><span class="v">GroupsAndTestCases = [Group | {group,GroupName} | TestCase]</span></li><li><span class="v">Reason = term()</span></li></ul><p>OPTIONAL</p><p>This function is called after
<a href="./common_test#Module:all-0">common_test#Module:all-0</a>.
It is used to modify the set of test cases and test group to
be executed, for instance to add or remove test cases and
groups, change group properties, or even skip all tests in
the suite.</p><p><strong>Return</strong> is what
<a href="./common_test#Module:all-0">common_test#Module:all-0</a>
returned, that is, a list of test cases and groups to be
executed, or a tuple <strong>{skip,Reason}</strong>.</p><p><strong>GroupDefs</strong> is what
<a href="./common_test#Module:groups-0">common_test#Module:groups-0</a>
or the <strong>post_groups/2</strong> hook returned, that is, a list
of group definitions.</p><p><strong>NewReturn</strong> is the possibly modified version of <strong>Return</strong>.</p><p>This function is called only if the CTH is added before
<strong>init_per_suite</strong> is run. For details, see section
<a href="./ct_hooks_chapter#scope">CTH Scope</a>
in the User's Guide.</p><p>Notice that for CTHs that are installed by means of the
<a href="./common_test#Module:suite-0">common_test#Module:suite-0</a>
function, <strong>post_all/2</strong> is called before
the <a href="#Module:init-2">Module:init-2</a>
hook function. However, for CTHs that are installed by means
of the CT start flag,
the <a href="#Module:init-2">Module:init-2</a>
function is called first.</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>Prior to each test execution, Common Test does a
simulated test run in order to count test suites, groups
and cases for logging purposes. This causes
the <strong>post_all/3</strong> hook function to always be called
twice. For this reason, side effects are best avoided in
this callback.</p></div><h4>Module:pre_init_per_suite(SuiteName, InitData, CTHState) -&gt; Result</h4><p>Called before init_per_suite.</p><ul><li><span class="v">SuiteName = atom()</span></li><li><span class="v">InitData = Config | SkipOrFail</span></li><li><span class="v">Config = NewConfig = [{Key,Value}]</span></li><li><span class="v">CTHState = NewCTHState = term()</span></li><li><span class="v">Result = {Return, NewCTHState}</span></li><li><span class="v">Return = NewConfig | SkipOrFail</span></li><li><span class="v">SkipOrFail = {fail, Reason} | {skip, Reason}</span></li><li><span class="v">Key = atom()</span></li><li><span class="v">Value = term()</span></li><li><span class="v">Reason = term()</span></li></ul><p>OPTIONAL</p><p>This function is called before
<a href="./common_test#Module:init_per_suite-1">common_test#Module:init_per_suite-1</a>
if it exists. It typically contains initialization/logging that must
be done before <strong>init_per_suite</strong> is called. If
<strong>{skip,Reason}</strong> or <strong>{fail,Reason}</strong> is returned,
<strong>init_per_suite</strong> and all test cases of the suite are skipped
and <strong>Reason</strong> printed in the overview log of the suite.</p><p><strong>SuiteName</strong> is the name of the suite to be run.</p><p><strong>InitData</strong> is the original configuration list of the test
suite, or a <strong>SkipOrFail</strong> tuple if a previous CTH has returned
this.</p><p><strong>CTHState</strong> is the current internal state of the CTH.</p><p><strong>Return</strong> is the result of the <strong>init_per_suite</strong> function.
If it is <strong>{skip,Reason}</strong> or <strong>{fail,Reason}</strong>, 
<a href="./common_test#Module:init_per_suite-1">common_test#Module:init_per_suite-1</a>
is never called, instead the initiation is considered to be
skipped or failed, respectively. If a <strong>NewConfig</strong> list is
returned,
<a href="./common_test#Module:init_per_suite-1">common_test#Module:init_per_suite-1</a>
is called with that <strong>NewConfig</strong> list. For more details, see
section <a href="./ct_hooks_chapter#pre">Pre Hooks</a>
in the User's Guide.</p><p>This function is called only if the CTH is added before
<strong>init_per_suite is run</strong>. For details, see section
<a href="./ct_hooks_chapter#scope">CTH Scope</a>
in the User's Guide.</p><h4>Module:post_init_per_suite(SuiteName, Config, Return, CTHState) -&gt; Result</h4><p>Called after init_per_suite.</p><ul><li><span class="v">SuiteName = atom()</span></li><li><span class="v">Config = [{Key,Value}]</span></li><li><span class="v">Return = NewReturn = Config | SkipOrFail | term()</span></li><li><span class="v">SkipOrFail = {fail, Reason} | {skip, Reason} | term()</span></li><li><span class="v">CTHState = NewCTHState = term()</span></li><li><span class="v">Result = {NewReturn, NewCTHState}</span></li><li><span class="v">Key = atom()</span></li><li><span class="v">Value = term()</span></li><li><span class="v">Reason = term()</span></li></ul><p>OPTIONAL</p><p>This function is called after
<a href="./common_test#Module:init_per_suite-1">common_test#Module:init_per_suite-1</a>
if it exists. It typically contains extra checks to ensure that all
the correct dependencies are started correctly.</p><p><strong>Return</strong> is what
<a href="./common_test#Module:init_per_suite-1">common_test#Module:init_per_suite-1</a>
returned, that is, <strong>{fail,Reason}</strong>, <strong>{skip,Reason}</strong>, a
<strong>Config</strong> list, or a term describing how
<a href="./common_test#Module:init_per_suite-1">common_test#Module:init_per_suite-1</a>
failed.</p><p><strong>NewReturn</strong> is the possibly modified return value of
<a href="./common_test#Module:init_per_suite-1">common_test#Module:init_per_suite-1</a>.
To recover from a failure in
<a href="./common_test#Module:init_per_suite-1">common_test#Module:init_per_suite-1</a>,
return <strong>ConfigList</strong> with the <strong>tc_status</strong> element removed.
For more details, see
<a href="./ct_hooks_chapter#post"> Post Hooks</a> in
section "Manipulating Tests" in the User's Guide.</p><p><strong>CTHState</strong> is the current internal state of the CTH.</p><p>This function is called only if the CTH is added before or in
<strong>init_per_suite</strong>. For details, see section
<a href="./ct_hooks_chapter#scope">CTH Scope</a>
in the User's Guide.</p><h4>Module:pre_init_per_group(SuiteName, GroupName, InitData, CTHState) -&gt; Result</h4><p>Called before init_per_group.</p><ul><li><span class="v">SuiteName = atom()</span></li><li><span class="v">GroupName = atom()</span></li><li><span class="v">InitData = Config | SkipOrFail</span></li><li><span class="v">Config = NewConfig = [{Key,Value}]</span></li><li><span class="v">CTHState = NewCTHState = term()</span></li><li><span class="v">Result = {NewConfig | SkipOrFail, NewCTHState}</span></li><li><span class="v">SkipOrFail = {fail,Reason} | {skip, Reason}</span></li><li><span class="v">Key = atom()</span></li><li><span class="v">Value = term()</span></li><li><span class="v">Reason = term()</span></li></ul><p>OPTIONAL</p><p>This function is called before
<a href="./common_test#Module:init_per_group-2">common_test#Module:init_per_group-2</a>
if it exists. It behaves the same way as
<a href="./ct_hooks#Module:pre_init_per_suite-3">ct_hooks#Module:pre_init_per_suite-3</a>,
but for function
<a href="./common_test#Module:init_per_group-2">common_test#Module:init_per_group-2</a>
instead.</p><p>If <strong>Module:pre_init_per_group/4</strong> is not exported, common_test
will attempt to call <strong>Module:pre_init_per_group(GroupName, InitData, CTHState)</strong> instead. This is for backwards
compatibility.</p><h4>Module:post_init_per_group(SuiteName, GroupName, Config, Return, CTHState) -&gt; Result</h4><p>Called after init_per_group.</p><ul><li><span class="v">SuiteName = atom()</span></li><li><span class="v">GroupName = atom()</span></li><li><span class="v">Config = [{Key,Value}]</span></li><li><span class="v">Return = NewReturn = Config | SkipOrFail | term()</span></li><li><span class="v">SkipOrFail = {fail,Reason} | {skip, Reason}</span></li><li><span class="v">CTHState = NewCTHState = term()</span></li><li><span class="v">Result = {NewReturn, NewCTHState}</span></li><li><span class="v">Key = atom()</span></li><li><span class="v">Value = term()</span></li><li><span class="v">Reason = term()</span></li></ul><p>OPTIONAL</p><p>This function is called after
<a href="./common_test#Module:init_per_group-2">common_test#Module:init_per_group-2</a>
if it exists. It behaves the same way as
<a href="./ct_hooks#Module:post_init_per_suite-4">ct_hooks#Module:post_init_per_suite-4</a>,
but for function
<a href="./common_test#Module:init_per_group-2">common_test#Module:init_per_group-2</a>
instead.</p><p>If <strong>Module:post_init_per_group/5</strong> is not exported, common_test
will attempt to call <strong>Module:post_init_per_group(GroupName, Config, Return, CTHState)</strong> instead. This is for backwards
compatibility.</p><h4>Module:pre_init_per_testcase(SuiteName, TestcaseName, InitData, CTHState) -&gt; Result</h4><p>Called before init_per_testcase.</p><ul><li><span class="v">SuiteName = atom()</span></li><li><span class="v">TestcaseName = atom()</span></li><li><span class="v">InitData = Config | SkipOrFail</span></li><li><span class="v">Config = NewConfig = [{Key,Value}]</span></li><li><span class="v">CTHState = NewCTHState = term()</span></li><li><span class="v">Result = {NewConfig | SkipOrFail, NewCTHState}</span></li><li><span class="v">SkipOrFail = {fail,Reason} | {skip, Reason}</span></li><li><span class="v">Key = atom()</span></li><li><span class="v">Value = term()</span></li><li><span class="v">Reason = term()</span></li></ul><p>OPTIONAL</p><p>This function is called before
<a href="./common_test#Module:init_per_testcase-2">common_test#Module:init_per_testcase-2</a>
if it exists. It behaves the same way as
<a href="./ct_hooks#Module:pre_init_per_suite-3">ct_hooks#Module:pre_init_per_suite-3</a>,
but for function
<a href="./common_test#Module:init_per_testcase-2">common_test#Module:init_per_testcase-2</a>
instead.</p><p>If <strong>Module:pre_init_per_testcase/4</strong> is not exported, common_test
will attempt to call <strong>Module:pre_init_per_testcase(TestcaseName, InitData, CTHState)</strong> instead. This is for backwards
compatibility.</p><p>CTHs cannot be added here right now. That feature may be added in
a later release, but it would right now break backwards
compatibility.</p><h4>Module:post_init_per_testcase(SuiteName, TestcaseName, Config, Return, CTHState) -&gt; Result</h4><p>Called after init_per_testcase.</p><ul><li><span class="v">SuiteName = atom()</span></li><li><span class="v">TestcaseName = atom()</span></li><li><span class="v">Config = [{Key,Value}]</span></li><li><span class="v">Return = NewReturn = Config | SkipOrFail | term()</span></li><li><span class="v">SkipOrFail = {fail,Reason} | {skip, Reason}</span></li><li><span class="v">CTHState = NewCTHState = term()</span></li><li><span class="v">Result = {NewReturn, NewCTHState}</span></li><li><span class="v">Key = atom()</span></li><li><span class="v">Value = term()</span></li><li><span class="v">Reason = term()</span></li></ul><p>OPTIONAL</p><p>This function is called after
<a href="./common_test#Module:init_per_testcase-2">common_test#Module:init_per_testcase-2</a>
if it exists. It behaves the same way as
<a href="./ct_hooks#Module:post_init_per_suite-4">ct_hooks#Module:post_init_per_suite-4</a>,
but for function
<a href="./common_test#Module:init_per_testcase-2">common_test#Module:init_per_testcase-2</a>
instead.</p><p>If <strong>Module:post_init_per_testcase/5</strong> is not exported, common_test
will attempt to call <strong>Module:post_init_per_testcase(TestcaseName, Config, Return, CTHState)</strong> instead. This is for backwards
compatibility.</p><h4>Module:pre_end_per_testcase(SuiteName, TestcaseName, EndData, CTHState) -&gt; Result</h4><p>Called before end_per_testcase.</p><ul><li><span class="v">SuiteName = atom()</span></li><li><span class="v">TestcaseName = atom()</span></li><li><span class="v">EndData = Config</span></li><li><span class="v">Config = NewConfig = [{Key,Value}]</span></li><li><span class="v">CTHState = NewCTHState = term()</span></li><li><span class="v">Result = {NewConfig, NewCTHState}</span></li><li><span class="v">Key = atom()</span></li><li><span class="v">Value = term()</span></li><li><span class="v">Reason = term()</span></li></ul><p>OPTIONAL</p><p>This function is called before
<a href="./common_test#Module:end_per_testcase-2">common_test#Module:end_per_testcase-2</a>
if it exists. It behaves the same way as
<a href="./ct_hooks#Module:pre_end_per_suite-3">ct_hooks#Module:pre_end_per_suite-3</a>,
but for function
<a href="./common_test#Module:end_per_testcase-2">common_test#Module:end_per_testcase-2</a>
instead.</p><p>This function cannot change the result of the test case by returning skip or fail
tuples, but it may insert items in <strong>Config</strong> that can be read in
<strong>end_per_testcase/2</strong> or in <strong>post_end_per_testcase/5</strong>.</p><p>If <strong>Module:pre_end_per_testcase/4</strong> is not exported, common_test
will attempt to call <strong>Module:pre_end_per_testcase(TestcaseName, EndData, CTHState)</strong> instead. This is for backwards
compatibility.</p><h4>Module:post_end_per_testcase(SuiteName, TestcaseName, Config, Return, CTHState) -&gt; Result</h4><p>Called after end_per_testcase.</p><ul><li><span class="v">SuiteName = atom()</span></li><li><span class="v">TestcaseName = atom()</span></li><li><span class="v">Config = [{Key,Value}]</span></li><li><span class="v">Return = NewReturn = Config | SkipOrFail | term()</span></li><li><span class="v">SkipOrFail = {fail,Reason} | {skip, Reason}</span></li><li><span class="v">CTHState = NewCTHState = term()</span></li><li><span class="v">Result = {NewReturn, NewCTHState}</span></li><li><span class="v">Key = atom()</span></li><li><span class="v">Value = term()</span></li><li><span class="v">Reason = term()</span></li></ul><p>OPTIONAL</p><p>This function is called after
<a href="./common_test#Module:end_per_testcase-2">common_test#Module:end_per_testcase-2</a>
if it exists. It behaves the same way as
<a href="./ct_hooks#Module:post_end_per_suite-4">ct_hooks#Module:post_end_per_suite-4</a>,
but for function
<a href="./common_test#Module:end_per_testcase-2">common_test#Module:end_per_testcase-2</a>
instead.</p><p>If <strong>Module:post_end_per_testcase/5</strong> is not exported, common_test
will attempt to call <strong>Module:post_end_per_testcase(TestcaseName, Config, Return, CTHState)</strong> instead. This is for backwards
compatibility.</p><h4>Module:pre_end_per_group(SuiteName, GroupName, EndData, CTHState) -&gt; Result</h4><p>Called before end_per_group.</p><ul><li><span class="v">SuiteName = atom()</span></li><li><span class="v">GroupName = atom()</span></li><li><span class="v">EndData = Config | SkipOrFail</span></li><li><span class="v">Config = NewConfig = [{Key,Value}]</span></li><li><span class="v">CTHState = NewCTHState = term()</span></li><li><span class="v">Result = {NewConfig | SkipOrFail, NewCTHState}</span></li><li><span class="v">SkipOrFail = {fail,Reason} | {skip, Reason}</span></li><li><span class="v">Key = atom()</span></li><li><span class="v">Value = term()</span></li><li><span class="v">Reason = term()</span></li></ul><p>OPTIONAL</p><p>This function is called before
<a href="./common_test#Module:end_per_group-2">common_test#Module:end_per_group-2</a>
if it exists. It behaves the same way as
<a href="./ct_hooks#Module:pre_init_per_suite-3">ct_hooks#Module:pre_init_per_suite-3</a>,
but for function
<a href="./common_test#Module:end_per_group-2">common_test#Module:end_per_group-2</a>
instead.</p><p>If <strong>Module:pre_end_per_group/4</strong> is not exported, common_test
will attempt to call <strong>Module:pre_end_per_group(GroupName, EndData, CTHState)</strong> instead. This is for backwards
compatibility.</p><h4>Module:post_end_per_group(SuiteName, GroupName, Config, Return, CTHState) -&gt; Result</h4><p>Called after end_per_group.</p><ul><li><span class="v">SuiteName = atom()</span></li><li><span class="v">GroupName = atom()</span></li><li><span class="v">Config = [{Key,Value}]</span></li><li><span class="v">Return = NewReturn = Config | SkipOrFail | term()</span></li><li><span class="v">SkipOrFail = {fail,Reason} | {skip, Reason}</span></li><li><span class="v">CTHState = NewCTHState = term()</span></li><li><span class="v">Result = {NewReturn, NewCTHState}</span></li><li><span class="v">Key = atom()</span></li><li><span class="v">Value = term()</span></li><li><span class="v">Reason = term()</span></li></ul><p>OPTIONAL</p><p>This function is called after
<a href="./common_test#Module:end_per_group-2">common_test#Module:end_per_group-2</a>
if it exists. It behaves the same way as
<a href="./ct_hooks#Module:post_init_per_suite-4">ct_hooks#Module:post_init_per_suite-4</a>,
but for function
<a href="./common_test#Module:end_per_group-2">end_per_group</a>
instead.</p><p>If <strong>Module:post_end_per_group/5</strong> is not exported, common_test
will attempt to call <strong>Module:post_end_per_group(GroupName, Config, Return, CTHState)</strong> instead. This is for backwards
compatibility.</p><h4>Module:pre_end_per_suite(SuiteName, EndData, CTHState) -&gt; Result</h4><p>Called before end_per_suite.</p><ul><li><span class="v">SuiteName = atom()</span></li><li><span class="v">EndData = Config | SkipOrFail</span></li><li><span class="v">Config = NewConfig = [{Key,Value}]</span></li><li><span class="v">CTHState = NewCTHState = term()</span></li><li><span class="v">Result = {NewConfig | SkipOrFail, NewCTHState}</span></li><li><span class="v">SkipOrFail = {fail,Reason} | {skip, Reason}</span></li><li><span class="v">Key = atom()</span></li><li><span class="v">Value = term()</span></li><li><span class="v">Reason = term()</span></li></ul><p>OPTIONAL</p><p>This function is called before
<a href="./common_test#Module:end_per_suite-1">common_test#Module:end_per_suite-1</a>
if it exists. It behaves the same way as
<a href="./ct_hooks#Module:pre_init_per_suite-3">ct_hooks#Module:pre_init_per_suite-3</a>,
but for function
<a href="./common_test#Module:end_per_suite-1">common_test#Module:end_per_suite-1</a>
instead.</p><h4>Module:post_end_per_suite(SuiteName, Config, Return, CTHState) -&gt; Result</h4><p>Called after end_per_suite.</p><ul><li><span class="v">SuiteName = atom()</span></li><li><span class="v">Config = [{Key,Value}]</span></li><li><span class="v">Return = NewReturn = Config | SkipOrFail | term()</span></li><li><span class="v">SkipOrFail = {fail,Reason} | {skip, Reason}</span></li><li><span class="v">CTHState = NewCTHState = term()</span></li><li><span class="v">Result = {NewReturn, NewCTHState}</span></li><li><span class="v">Key = atom()</span></li><li><span class="v">Value = term()</span></li><li><span class="v">Reason = term()</span></li></ul><p>OPTIONAL</p><p>This function is called after
<a href="./common_test#Module:end_per_suite-1">common_test#Module:end_per_suite-1</a>
if it exists. It behaves the same way as
<a href="./ct_hooks#Module:post_init_per_suite-4">ct_hooks#Module:post_init_per_suite-4</a>,
but for function
<a href="./common_test#Module:end_per_suite-1">common_test#Module:end_per_suite-1</a>
instead.</p><h4>Module:on_tc_fail(SuiteName, TestName, Reason, CTHState) -&gt; NewCTHState</h4><p>Called after the CTH scope ends.</p><ul><li><span class="v">SuiteName = atom()</span></li><li><span class="v">TestName = init_per_suite | end_per_suite | {init_per_group,GroupName} | {end_per_group,GroupName} | {FuncName,GroupName} | FuncName</span></li><li><span class="v">FuncName = atom()</span></li><li><span class="v">GroupName = atom()</span></li><li><span class="v">Reason = term()</span></li><li><span class="v">CTHState = NewCTHState = term()</span></li></ul><p>OPTIONAL</p><p>This function is called whenever a test case (or configuration
function) fails. It is called after the post function is called
for the failed test case, that is:</p><ul><li><p>If <strong>init_per_suite</strong> fails, this function is called after
<a href="#Module:post_init_per_suite-4">Module:post_init_per_suite-4</a>.</p></li><li><p>If a test case fails, this funcion is called after
<a href="#Module:post_end_per_testcase-5">Module:post_end_per_testcase-5</a>.</p></li></ul><p>If the failed test case belongs to a test case group, the first
argument is a tuple <strong>{FuncName,GroupName}</strong>, otherwise only
the function name.</p><p>The data that comes with <strong>Reason</strong> follows the same format as
<a href="./event_handler_chapter#failreason">event_handler_chapter#failreason</a>
in event
<a href="./event_handler_chapter#tc_done">event_handler_chapter#tc_done</a>.
For details, see section
<a href="./event_handler_chapter#events">Event Handling</a>
in the User's Guide.</p><p>If <strong>Module:on_tc_fail/4</strong> is not exported, common_test
will attempt to call <strong>Module:on_tc_fail(TestName, Reason, CTHState)</strong> instead. This is for backwards
compatibility.</p><h4>Module:on_tc_skip(SuiteName, TestName, Reason, CTHState) -&gt; NewCTHState</h4><p>Called after the CTH scope ends.</p><ul><li><span class="v">SuiteName = atom()</span></li><li><span class="v">TestName = init_per_suite | end_per_suite | {init_per_group,GroupName} | {end_per_group,GroupName} | {FuncName,GroupName} | FuncName</span></li><li><span class="v">FuncName = atom()</span></li><li><span class="v">GroupName = atom()</span></li><li><span class="v">Reason = {tc_auto_skip | tc_user_skip, term()}</span></li><li><span class="v">CTHState = NewCTHState = term()</span></li></ul><p>OPTIONAL</p><p>This function is called whenever a test case (or configuration
function) is skipped. It is called after the post function is called
for the skipped test case, that is:</p><ul><li><p>If <strong>init_per_group</strong> is skipped, this function is
called after 
<a href="#Module:post_init_per_group-5">Module:post_init_per_group-5</a>.</p></li><li><p>If a test case is skipped, this function is called after
<a href="#Module:post_end_per_testcase-5">Module:post_end_per_testcase-5</a>.</p></li></ul><p>If the skipped test case belongs to a test case group, the first
argument is a tuple <strong>{FuncName,GroupName}</strong>, otherwise only
the function name.</p><p>The data that comes with <strong>Reason</strong> follows the same format as
events
<a href="./event_handler_chapter#tc_auto_skip">event_handler_chapter#tc_auto_skip</a>
and
<a href="./event_handler_chapter#tc_user_skip">event_handler_chapter#tc_user_skip</a>
For details, see section
<a href="./event_handler_chapter#events">Event Handling</a>
in the User's Guide.</p><p>If <strong>Module:on_tc_skip/4</strong> is not exported, common_test
will attempt to call <strong>Module:on_tc_skip(TestName, Reason, CTHState)</strong> instead. This is for backwards
compatibility.</p><h4>Module:terminate(CTHState)</h4><p>Called after the CTH scope ends.</p><ul><li><span class="v">CTHState = term()</span></li></ul><p>OPTIONAL</p><p>This function is called at the end of a CTH
<a href="./ct_hooks_chapter#scope">scope</a>.</p><h4>Module:id(Opts) -&gt; Id</h4><p>Called before the init function of a CTH.</p><ul><li><span class="v">Opts = term()</span></li><li><span class="v">Id = term()</span></li></ul><p>OPTIONAL</p><p>The <strong>Id</strong> identifies a CTH instance uniquely. If two CTHs return
the same <strong>Id</strong>, the second CTH is ignored and subsequent calls to
the CTH are only made to the first instance. For details, see section
<a href="./ct_hooks_chapter#installing">Installing a CTH</a>
in the User's Guide.</p><p>This function is <em>not</em> to have any side effects, as it can 
be called multiple times by <strong>Common Test</strong>.</p><p>If not implemented, the CTH acts as if this function returned a call
to <strong>make_ref/0</strong>.</p><h3>ct_property_test</h3><p>EXPERIMENTAL support in Common Test for calling
    property-based tests.</p><p>EXPERIMENTAL support in <strong>Common Test</strong> for calling property-based
tests.This module is a first step to run property-based tests in the
<strong>Common Test</strong> framework. A property testing tool like QuickCheck
or PropEr is assumed to be installed.The idea is to have a <strong>Common Test</strong> test suite calling a property
testing tool with special property test suites as defined by that tool.
The usual Erlang application directory structure is assumed. The tests
are collected in the <strong>test</strong> directory of the application. The
<strong>test</strong> directory has a subdirectory <strong>property_test</strong>, where
everything needed for the property tests is collected.A typical <strong>Common Test</strong> test suite using <strong>ct_property_test</strong>
is organized as follows:<pre>
 -include_lib("common_test/include/ct.hrl").

 all() -&gt; [prop_ftp_case].

 init_per_suite(Config) -&gt;
     ct_property_test:init_per_suite(Config).

 %%%---- test case
 prop_ftp_case(Config) -&gt;
     ct_property_test:quickcheck(
       ftp_simple_client_server:prop_ftp(Config),
       Config
      ).</pre></p><h3>Functions</h3><h4>init_per_suite(Config) -&gt; Config | {skip, Reason}</h4><p>Initializes Config for property testing.</p><a name="init_per_suite-1"></a><p>Initializes <strong>Config</strong> for property testing.</p><p>This function investigates if support is available for either
Quickcheck, PropEr, or Triq. The options
<strong>{property_dir,AbsPath}</strong> and <strong>{property_test_tool,Tool}</strong>
are set in the <strong>Config</strong> returned.</p><p>The function is intended to be called in function
<strong>init_per_suite</strong> in the test suite.</p><p>The property tests are assumed to be in subdirectory
<strong>property_test</strong>.</p><h4>quickcheck(Property, Config) -&gt; true | {fail, Reason}</h4><p>Calls quickcheck and returns the result in a form suitable for Common Test.</p><a name="quickcheck-2"></a><p>Calls quickcheck and returns the result in a form suitable for
<strong>Common Test</strong>.</p><p>This function is intended to be called in the test cases in the
test suite.</p><h3>ct_testspec</h3><p>Parsing of test specifications for Common Test.
</p><p>Parsing of test specifications for <strong>Common Test</strong>.This module exports help functions for parsing of test specifications.</p><h3>Functions</h3><h4>get_tests(SpecsIn) -&gt; {ok, [{Specs,Tests}]} | {error, Reason}</h4><p>Parse the given test specification files and return the tests to run and skip.</p><ul><li><span class="v">SpecsIn = [string()] | [[string()]]</span></li><li><span class="v">Specs = [string()]</span></li><li><span class="v">Test = [{Node,Run,Skip}]</span></li><li><span class="v">Node = atom()</span></li><li><span class="v">Run = {Dir,Suites,Cases}</span></li><li><span class="v">Skip = {Dir,Suites,Comment} | {Dir,Suites,Cases,Comment}</span></li><li><span class="v">Dir = string()</span></li><li><span class="v">Suites = atom | [atom()] | all</span></li><li><span class="v">Cases = atom | [atom()] | all</span></li><li><span class="v">Comment = string()</span></li><li><span class="v">Reason = term()</span></li></ul><a name="add_nodes-1"></a><p>Parse the given test specification files and return the
tests to run and skip.</p><p>If <strong>SpecsIn=[Spec1,Spec2,...]</strong>, separate tests will be
created per specification. If
<strong>SpecsIn=[[Spec1,Spec2,...]]</strong>, all specifications will be
merge into one test.</p><p>For each test, a <strong>{Specs,Tests}</strong> element is returned,
where <strong>Specs</strong> is a list of all included test
specifications, and <strong>Tests</strong> specifies actual tests to
run/skip per node.</p></body></html>