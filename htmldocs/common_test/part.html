<!doctype html>
<html><head><meta charset="utf-8"><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.0/css/bootstrap.min.css"></head><body style="margin: 4em 10%"><h1>Common Test User's Guide</h1><h3>Scope</h3><p><strong>Common Test</strong> is a portable application for automated 
testing. It is suitable for:</p><ul><li><p>Black-box testing of target systems of any type (that
is, not necessarily implemented in Erlang). This is performed
through standard O&amp;M interfaces (such as SNMP, HTTP, CORBA,
and Telnet) and, if necessary, through user-specific interfaces
(often called test ports).</p></li><li><p>White-box testing of Erlang/OTP programs. This is easily
done by calling the target API functions directly from the test
case functions.</p></li></ul><p><strong>Common Test</strong> also integrates use of the OTP
<a href="./cover">cover</a> tool in application 
Tools for code coverage analysis of Erlang/OTP programs.</p><p><strong>Common Test</strong> executes test suite programs automatically,
without operator interaction. Test progress and results are
printed to logs in HTML format, easily browsed with a standard
web browser. <strong>Common Test</strong> also sends notifications about progress
and results through an OTP event manager to event handlers plugged
in to the system. This way, users can integrate their own
programs for, for example, logging, database storing, or supervision with
<strong>Common Test</strong>.</p><p><strong>Common Test</strong> provides libraries with useful support 
functions to fill various testing needs and requirements. 
There is, for example, support for flexible test declarations 
through test specifications. There is also support 
for central configuration and control of multiple 
independent test sessions (to different target systems)
running in parallel.</p><h3>Prerequisites</h3><p>It is assumed that the reader is familiar with the Erlang
programming language.</p><a name="basics"></a><h3>General</h3><p>The <strong>Common Test</strong> framework is a tool that supports
implementation and automated execution of test cases to any
types of target systems. <strong>Common Test</strong> is the main tool being used 
in all testing- and verification activities that are part of Erlang/OTP 
system development and maintenance.
</p><p>Test cases can be executed individually or in batches. <strong>Common Test</strong>
also features a distributed testing mode with central control and logging.
With this feature, multiple systems can be tested independently in
one common session. This is useful, for example, when running automated 
large-scale regression tests.
</p><p>
The System Under Test (SUT) can consist of one or more target
nodes. <strong>Common Test</strong> contains a generic test server that, 
together with other test utilities, is used to perform test case execution. 
The tests can be started from a GUI, from the OS shell, or from an
Erlang shell. <em>Test suites</em> are files (Erlang
modules) that contain the <em>test cases</em> (Erlang functions)
to be executed. <em>Support modules</em> provide functions
that the test cases use to do the tests.
</p><p>In a black-box testing scenario, <strong>Common Test</strong>-based test programs connect to
the target system(s) through standard O&amp;M and CLI protocols. <strong>Common Test</strong>
provides implementations of, and wrapper interfaces to, some of these
protocols (most of which exist as standalone components and
applications in OTP). The wrappers simplify configuration and add
verbosity for logging purposes. <strong>Common Test</strong> is continously extended with
useful support modules. However, notice that it is
a straightforward task to use any Erlang/OTP component
for testing purposes with <strong>Common Test</strong>, without needing a <strong>Common Test</strong> 
wrapper for it. It is as simple as calling Erlang functions. A number of 
target-independent interfaces are supported in <strong>Common Test</strong>, such as
Generic Telnet and FTP. These can be specialized or used
directly for controlling instruments, traffic load generators, and so on.
</p><p><strong>Common Test</strong> is also a very useful tool for white-box testing Erlang
code (for example, module testing), as the test programs can call exported Erlang
functions directly. There is very little overhead required for
implementing basic test suites and executing simple tests. For black-box
testing Erlang software, Erlang RPC and standard O&amp;M interfaces
can be used for example.
</p><p>A test case can handle several connections to one or
more target systems, instruments, and traffic generators in
parallel to perform the necessary actions for a test. 
The handling of many connections in parallel is one of
the major strengths of <strong>Common Test</strong>, thanks to the efficient
support for concurrency in the Erlang runtime system, which <strong>Common Test</strong> 
users can take great advantage of.
</p><h3>Test Suite Organisation</h3><p>
Test suites are organized in test directories and each test suite
can have a separate data directory. Typically, these files and directories
are version-controlled similar to other forms of source code (possibly by
a version control system like GIT or Subversion). However, <strong>Common Test</strong> 
does not itself put any requirements on (or has any awareness of) 
possible file and directory versions.
</p><h3>Support Libraries</h3><p>
Support libraries contain functions that are useful for all test suites,
or for test suites in a specific functional area or subsystem.
In addition to the general support libraries provided by the
<strong>Common Test</strong> framework, and the various libraries and applications provided by
Erlang/OTP, there can also be a need for customized (user specific) 
support libraries. 
</p><h3>Suites and Test Cases</h3><p>
Testing is performed by running test suites (sets of test cases) or 
individual test cases. A test suite is implemented as an Erlang module named 
<strong>&lt;suite_name&gt;_SUITE.erl</strong> which contains a number of test cases.
A test case is an Erlang function that tests one or more things. 
The test case is the smallest unit that the <strong>Common Test</strong> test server deals with.
</p><p>
Sets of test cases, called test case groups, can also be defined. A test case
group can have execution properties associated with it. Execution properties 
specify if the test cases in the group are to be executed in
random order, in parallel, or in sequence, and if the execution of the group 
is to be repeated. Test case groups can also be nested (that is, a group can,
besides test cases, contain subgroups).
</p><p>
Besides test cases and groups, the test suite can also contain configuration 
functions. These functions are meant to be used for setting up (and verifying)
environment and state in the SUT (and/or the <strong>Common Test</strong> host node), 
required for the tests to execute correctly. Examples of operations are: 
Opening a connection to the SUT, initializing a database, running an installation 
script, and so on. Configuration can be performed per suite, per test case group,
and per individual test case.
</p><p>
The test suite module must conform to a
<a href="common_test">callback interface</a>
specified by the <strong>Common Test</strong> test server. For details, see section
<a href="./write_test_chapter#intro">Writing Test Suites</a>.
</p><p>
A test case is considered successful if it returns to the caller, no matter 
what the returned value is. However, a few return values have special meaning
as follows:</p><ul><li><strong>{skip,Reason}</strong> indicates that the test case is skipped.</li><li><strong>{comment,Comment}</strong> prints a comment in the log for the test case.</li><li><strong>{save_config,Config}</strong> makes the <strong>Common Test</strong> test server pass  <strong>Config</strong> to the next test case.</li></ul><p>
A test case failure is specified as a runtime error (a crash), no matter what 
the reason for termination is. If you use Erlang pattern matching effectively,
you can take advantage of this property. The result is concise and 
readable test case functions that look much more like scripts than actual programs. 
A simple example:
</p><pre>
 session(_Config) -&gt;
     {started,ServerId} = my_server:start(),
     {clients,[]} = my_server:get_clients(ServerId),
     MyId = self(),
     connected = my_server:connect(ServerId, MyId),
     {clients,[MyId]} = my_server:get_clients(ServerId),
     disconnected = my_server:disconnect(ServerId, MyId),
     {clients,[]} = my_server:get_clients(ServerId),
     stopped = my_server:stop(ServerId).</pre><p>
As a test suite runs, all information (including output to <strong>stdout</strong>) is 
recorded in many different log files. A minimum of information is displayed 
in the user console (only start and stop information, plus a note 
for each failed test case).
</p><p>
The result from each test case is recorded in a dedicated HTML log file, created 
for the particular test run. An overview page displays each test case represented 
by a table row showing total execution time, if the case was successful,
failed, or skipped, plus an optional user comment. For a failed test case, the 
reason for termination is also printed in the comment field. The overview page
has a link to each test case log file, providing simple navigation with any standard
HTML browser.
</p><a name="External_Interfaces"></a><h3>External Interfaces</h3><p>
The <strong>Common Test</strong> test server requires that the test suite defines and exports the 
following mandatory or optional callback functions:
</p><dl><dt><strong>all()</strong></dt><dd><p>Returns a list of all test cases and groups in the suite. (Mandatory)</p></dd><dt><strong>suite()</strong></dt><dd><p>Information function used to return properties for the suite. (Optional)</p></dd><dt><strong>groups()</strong></dt><dd><p>For declaring test case groups. (Optional)</p></dd><dt><strong>init_per_suite(Config)</strong></dt><dd><p>Suite level configuration function, executed before the first 
test case. (Optional)</p></dd><dt><strong>end_per_suite(Config)</strong></dt><dd><p>Suite level configuration function, executed after the last 
test case. (Optional)</p></dd><dt><strong>group(GroupName)</strong></dt><dd><p>Information function used to return properties for a test case group. (Optional)</p></dd><dt><strong>init_per_group(GroupName, Config)</strong></dt><dd><p>Configuration function for a group, executed before the first 
test case. (Optional)</p></dd><dt><strong>end_per_group(GroupName, Config)</strong></dt><dd><p>Configuration function for a group, executed after the last 
test case. (Optional)</p></dd><dt><strong>init_per_testcase(TestCase, Config)</strong></dt><dd><p>Configuration function for a testcase, executed before each 
test case. (Optional)</p></dd><dt><strong>end_per_testcase(TestCase, Config)</strong></dt><dd><p>Configuration function for a testcase, executed after each 
test case. (Optional)</p></dd></dl><p>
For each test case, the <strong>Common Test</strong> test server expects the
following functions:
</p><dl><dt>Testcasename()</dt><dd><p>Information function that returns a list of test case properties. (Optional)</p></dd><dt>Testcasename(Config)</dt><dd><p>The test case function.</p></dd></dl><h3>Introduction for Newcomers</h3><p>
The purpose of this section is to let the newcomer get started in
quickly writing and executing some first simple tests with a 
"learning by example" approach. Most explanations are left for later sections. 
If you are not much into "learning by example" and prefer more technical
details, go ahead and skip to the next section.
</p><p>
This section demonstrates how simple it is to write a basic 
(yet for many module testing purposes, often sufficiently complex) 
test suite and execute its test cases. This is not necessarily
obvious when you read the remaining sections in this User's Guide.
</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>
To understand what is discussed and examplified here, we recommended 
you to first read section
<a href="./basics_chapter#basics">Common Test Basics</a>.
</p></div><h3>Test Case Execution</h3><p>Execution of test cases is handled as follows:</p><img src="tc_execution.gif" title="
	Successful and Unsuccessful Test Case Execution
"></img><p>For each test case that <strong>Common Test</strong> is ordered to execute, it spawns a
dedicated process on which the test case function starts
running. (In parallel to the test case process, an idle waiting timer
process is started, which is linked to the test case process. If the timer
process runs out of waiting time, it sends an exit signal to terminate
the test case process. This is called a <em>timetrap</em>).
</p><p>In scenario 1, the test case process terminates normally after 
<strong>case A</strong> has finished executing its test code without detecting 
any errors. The test case function returns a value and <strong>Common Test</strong> 
logs the test case as successful.
</p><p>In scenario 2, an error is detected during test <strong>case B</strong> execution.
This causes the test <strong>case B</strong> function to generate an exception
and, as a result, the test case process exits with reason other than normal. 
<strong>Common Test</strong> logs this as an unsuccessful (Failed) test case.
</p><p>As you can understand from the illustration, <strong>Common Test</strong> requires
a test case to generate a runtime error to indicate failure (for example,
by causing a bad match error or by calling <strong>exit/1</strong>, preferably
through the help function 
<a href="./ct#fail-1">ct#fail-1</a>). A successful 
execution is indicated by a normal return from the test case function.
</p><h3>A Simple Test Suite</h3><p>As shown in section 
<a href="./basics_chapter#External_Interfaces">Common Test Basics</a>,
the test suite module implements
<a href="common_test">callback functions</a>
(mandatory or optional) for various purposes, for example:
</p><ul><li>Init/end configuration function for the test suite</li><li>Init/end configuration function for a test case</li><li>Init/end configuration function for a test case group</li><li>Test cases</li></ul><p> 
The configuration functions are optional. The following example is a test suite 
without configuration functions, including one simple test case, to 
check that module <strong>mymod</strong> exists (that is, can be successfully loaded by the 
code server):
</p><pre>
 -module(my1st_SUITE).
 -compile(export_all).

 all() -&gt;
     [mod_exists].

 mod_exists(_) -&gt;
     {module,mymod} = code:load_file(mymod).</pre><p>
If the operation fails, a bad match error occurs that terminates the test case.
</p><h3>A Test Suite with Configuration Functions</h3><p>
If you need to perform configuration operations to run your test, you can
implement configuration functions in your suite. The result from a
configuration function is configuration data, or <strong>Config</strong>.
This is a list of key-value tuples that get passed from the configuration
function to the test cases (possibly through configuration functions on
"lower level"). The data flow looks as follows:
</p><img src="config.gif" title="
	Configuration Data Flow in a Suite
"></img><p>
The following example shows a test suite that uses configuration functions
to open and close a log file for the test cases (an operation that is
unnecessary and irrelevant to perform by each test case):
</p><pre>
 -module(check_log_SUITE).
 -export([all/0, init_per_suite/1, end_per_suite/1]).
 -export([check_restart_result/1, check_no_errors/1]).

 -define(value(Key,Config), proplists:get_value(Key,Config)).

 all() -&gt; [check_restart_result, check_no_errors].

 init_per_suite(InitConfigData) -&gt;
     [{logref,open_log()} | InitConfigData].

 end_per_suite(ConfigData) -&gt;
     close_log(?value(logref, ConfigData)).

 check_restart_result(ConfigData) -&gt;
     TestData = read_log(restart, ?value(logref, ConfigData)),
     {match,_Line} = search_for("restart successful", TestData).

 check_no_errors(ConfigData) -&gt;
     TestData = read_log(all, ?value(logref, ConfigData)),
     case search_for("error", TestData) of
	 {match,Line} -&gt; ct:fail({error_found_in_log,Line});
	 nomatch -&gt; ok
     end.</pre><p>
The test cases verify, by parsing a log file, that our SUT has performed 
a successful restart and that no unexpected errors are printed.
</p><p>To execute the test cases in the recent test suite, type the 
following on the UNIX/Linux command line (assuming that the suite module
is in the current working directory):
</p><pre>
 $ ct_run -dir .</pre><p>or:</p><pre>
 $ ct_run -suite check_log_SUITE</pre><p>To use the Erlang shell to run our test, you can evaluate the following call:
</p><pre>
 1&gt; ct:run_test([{dir, "."}]).</pre><p>or:</p><pre>
 1&gt; ct:run_test([{suite, "check_log_SUITE"}]).</pre><p>
The result from running the test is printed in log files in HTML format
(stored in unique log directories on a different level). The following 
illustration shows the log file structure:
</p><img src="html_logs.gif" title="
	HTML Log File Structure
"></img><h3>Questions and Answers</h3><p>Here follows some questions that you might have after reading this section 
with corresponding tips and links to the answers:
</p><ul><li><p><em>Question:</em> 
"How and where can I specify variable data for my tests that must not 
be hard-coded in the test suites (such as hostnames, addresses, and
user login data)?"</p> <p><em>Answer:</em> 
See section <a href="./config_file_chapter#top">External Configuration Data</a>.</p> </li><li><p><em>Question:</em> "Is there a way to declare different tests and run them
in one session without having to write my own scripts? Also, can such
declarations be used for regression testing?"</p> <p><em>Answer:</em> See section
<a href="./run_test_chapter#test_specifications">Test Specifications</a>
in section Running Tests and Analyzing Results.
</p> </li><li><p><em>Question:</em> "Can test cases and/or test runs be automatically repeated?"</p> <p><em>Answer:</em> Learn more about
<a href="./write_test_chapter#test_case_groups">Test Case Groups</a>
and read about start flags/options in section
<a href="./run_test_chapter#ct_run">Running Tests</a> and in
the Reference Manual.</p> </li><li><p><em>Question:</em> "Does <strong>Common Test</strong> execute my test cases in sequence or in parallel?"</p> <p><em>Answer:</em> See
<a href="./write_test_chapter#test_case_groups">Test Case Groups</a>
in section Writing Test Suites.</p> </li><li><p><em>Question:</em> "What is the syntax for timetraps (mentioned earlier), and how do I set them?"</p> <p><em>Answer:</em> This is explained in the
<a href="./write_test_chapter#timetraps">Timetrap Time-Outs</a>
part of section Writing Test Suites.</p> </li><li><p><em>Question:</em> "What functions are available for logging and printing?"</p> <p><em>Answer:</em> See
<a href="./write_test_chapter#logging">Logging</a>
in section Writing Test Suites.</p> </li><li><p><em>Question:</em> "I need data files for my tests. Where do I store them preferably?"</p> <p><em>Answer:</em> See
<a href="./write_test_chapter#data_priv_dir">Data and Private Directories</a>.</p> </li><li><p><em>Question:</em> "Can I start with a test suite example, please?"</p> <p><em>Answer:</em> <a href="./example_chapter#top">Welcome!</a></p> </li></ul><p>You probably want to get started on your own first test suites now, while
at the same time digging deeper into the <strong>Common Test</strong> User's Guide and Reference Manual.
There are much more to learn about the things that have been introduced
in this section. There are also many other useful features to learn, 
so please continue to the other sections and have fun.
</p><a name="general"></a><h3>General Information</h3><p>The two main interfaces for running tests with <strong>Common Test</strong>
are an executable program named 
<a href="ct_run">ct_run</a> and the
Erlang module <a href="ct">ct</a>. 
<strong>ct_run</strong> is compiled for the underlying operating system (for example,
Unix/Linux or Windows) during the build of the Erlang/OTP system, 
and is installed automatically with other executable programs in
the top level <strong>bin</strong> directory of Erlang/OTP.
The <strong>ct</strong> interface functions can be called from the Erlang shell,
or from any Erlang function, on any supported platform.</p><p>The <strong>Common Test</strong> application is installed with the Erlang/OTP
system. No extra installation step is required to start using
<strong>Common Test</strong> through the <strong>ct_run</strong> executable program, 
and/or the interface functions in the <strong>ct</strong> module.</p><a name="intro"></a><h3>Support for Test Suite Authors</h3><p>The <a href="ct">ct</a> module provides the main 
interface for writing test cases. This includes for example, the following:</p><ul><li>Functions for printing and logging</li><li>Functions for reading configuration data</li><li>Function for terminating a test case with error reason</li><li>Function for adding comments to the HTML overview page</li></ul><p>For details about these functions, see module <a href="ct">ct</a>.</p><p>The <strong>Common Test</strong> application also includes other modules named 
<strong>ct_&lt;component&gt;</strong>, which
provide various support, mainly simplified use of communication
protocols such as RPC, SNMP, FTP, Telnet, and others.</p><h3>Test Suites</h3><p>A test suite is an ordinary Erlang module that contains test
cases. It is recommended that the module has a name on the form
<strong>*_SUITE.erl</strong>. Otherwise, the directory and auto compilation 
function in <strong>Common Test</strong> cannot locate it (at least not by default).
</p><p>It is also recommended that the <strong>ct.hrl</strong> header file is included
in all test suite modules.
</p><p>Each test suite module must export function 
<a href="./common_test#Module:all-0">common_test#Module:all-0</a>,
which returns the list of all test case groups and test cases 
to be executed in that module. 
</p><p>The callback functions to be implemented by the test suite are
all listed in module <a href="common_test">common_test </a>. They are also described in more detail later in this User's Guide.
</p><h3>Init and End per Suite</h3><p>Each test suite module can contain the optional configuration functions
<a href="./common_test#Module:init_per_suite-1">common_test#Module:init_per_suite-1</a>
and <a href="./common_test#Module:end_per_suite-1">common_test#Module:end_per_suite-1</a>. 
If the init function is defined, so must the end function be.
</p><p>If <strong>init_per_suite</strong> exists, it is called initially before the
test cases are executed. It typically contains initializations common
for all test cases in the suite, which are only to be performed once. 
<strong>init_per_suite</strong> is recommended for setting up and verifying state 
and environment on the System Under Test (SUT) or the <strong>Common Test</strong> 
host node, or both, so that the test cases in the suite executes correctly. 
The following are examples of initial configuration operations:
</p><ul><li>Opening a connection to the SUT</li><li>Initializing a database</li><li>Running an installation script</li></ul><p><strong>end_per_suite</strong> is called as the final stage of the test suite execution
(after the last test case has finished). The function is meant to be used 
for cleaning up after <strong>init_per_suite</strong>. 
</p><p><strong>init_per_suite</strong> and <strong>end_per_suite</strong> execute on dedicated
Erlang processes, just like the test cases do. The result of these functions
is however not included in the test run statistics of successful, failed, and
skipped cases.
</p><p>The argument to <strong>init_per_suite</strong> is <strong>Config</strong>, that is, the
same key-value list of runtime configuration data that each test case takes
as input argument. <strong>init_per_suite</strong> can modify this parameter with 
information that the test cases need. The possibly modified <strong>Config</strong>
list is the return value of the function.
</p><p>If <strong>init_per_suite</strong> fails, all test cases in the test
suite are skipped automatically (so called <em>auto skipped</em>), 
including <strong>end_per_suite</strong>.
</p><p>Notice that if <strong>init_per_suite</strong> and <strong>end_per_suite</strong> do not exist
in the suite, <strong>Common Test</strong> calls dummy functions (with the same names)
instead, so that output generated by hook functions can be saved to the log
files for these dummies. For details, see
<a href="./ct_hooks_chapter#manipulating">Common Test Hooks</a>.
</p><a name="per_testcase"></a><h3>Init and End per Test Case</h3><p>Each test suite module can contain the optional configuration functions
<a href="./common_test#Module:init_per_testcase-2">common_test#Module:init_per_testcase-2</a>
and <a href="./common_test#Module:end_per_testcase-2">common_test#Module:end_per_testcase-2</a>. 
If the init function is defined, so must the end function be.</p><p>If <strong>init_per_testcase</strong> exists, it is called before each
test case in the suite. It typically contains initialization that
must be done for each test case (analog to <strong>init_per_suite</strong> for the 
suite).</p><p><strong>end_per_testcase/2</strong> is called after each test case has
finished, enabling cleanup after <strong>init_per_testcase</strong>.</p><p>The first argument to these functions is the name of the test
case. This value can be used with pattern matching in function clauses
or conditional expressions to choose different initialization and cleanup
routines for different test cases, or perform the same routine for many,
or all, test cases.</p><p>The second argument is the <strong>Config</strong> key-value list of runtime
configuration data, which has the same value as the list returned by
<strong>init_per_suite</strong>. <strong>init_per_testcase/2</strong> can modify this
parameter or return it "as is". The return value of <strong>init_per_testcase/2</strong> 
is passed as parameter <strong>Config</strong> to the test case itself.</p><p>The return value of <strong>end_per_testcase/2</strong> is ignored by the
test server, with exception of the 
<a href="./dependencies_chapter#save_config">dependencies_chapter#save_config</a>
and <strong>fail</strong> tuple.</p><p><strong>end_per_testcase</strong> can check if the test case was successful. 
(which in turn can determine how cleanup is to be performed). 
This is done by reading the value tagged with <strong>tc_status</strong> from 
<strong>Config</strong>. The value is one of the following:
</p><ul><li> <p><strong>ok</strong></p> </li><li> <p><strong>{failed,Reason}</strong></p> <p>where <strong>Reason</strong> is <strong>timetrap_timeout</strong>, information from <strong>exit/1</strong>, 
or details of a runtime error</p></li><li> <p><strong>{skipped,Reason}</strong></p> <p>where <strong>Reason</strong> is a user-specific term</p></li></ul><p>Function <strong>end_per_testcase/2</strong> is even called if a
test case terminates because of a call to 
<a href="./ct#abort_current_testcase-1">ct#abort_current_testcase-1</a>,
or after a timetrap time-out. However, <strong>end_per_testcase</strong>
then executes on a different process than the test case
function. In this situation, <strong>end_per_testcase</strong> cannot
change the reason for test case termination by returning <strong>{fail,Reason}</strong>
or save data with <strong>{save_config,Data}</strong>.</p><p>The test case is skipped in the following two cases:
</p><ul><li>If <strong>init_per_testcase</strong> crashes (called <em>auto skipped</em>).</li><li>If <strong>init_per_testcase</strong> returns a tuple <strong>{skip,Reason}</strong>  (called <em>user skipped</em>).</li></ul><p>The test case can also be marked as failed without executing it
by returning a tuple <strong>{fail,Reason}</strong> from <strong>init_per_testcase</strong>.</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>If <strong>init_per_testcase</strong> crashes, or returns <strong>{skip,Reason}</strong>
or <strong>{fail,Reason}</strong>, function <strong>end_per_testcase</strong> is not called.
</p></div><p>If it is determined during execution of <strong>end_per_testcase</strong> that
the status of a successful test case is to be changed to failed, 
<strong>end_per_testcase</strong> can return the tuple <strong>{fail,Reason}</strong>
(where <strong>Reason</strong> describes why the test case fails).</p><p>As <strong>init_per_testcase</strong> and <strong>end_per_testcase</strong> execute on the
same Erlang process as the test case, printouts from these
configuration functions are included in the test case log file.</p><a name="test_cases"></a><h3>Test Cases</h3><p>The smallest unit that the test server is concerned with is a
test case. Each test case can test many things, for
example, make several calls to the same interface function with
different parameters.
</p><p>The author can choose to put many or few tests into each test
case. Some things to keep in mind follows:
</p><ul><li><p>Many small test cases tend to result in extra, and possibly
duplicated code, as well as slow test execution because of
large overhead for initializations and cleanups. Avoid duplicated 
code, for example, by using common help functions. Otherwise,
the resulting suite becomes difficult to read and understand, and
expensive to maintain.
</p></li><li><p>Larger test cases make it harder to tell what went wrong if it
fails. Also, large portions of test code risk being skipped
when errors occur.</p> </li><li><p>Readability and maintainability suffer 
when test cases become too large and extensive. It is not certain 
that the resulting log files reflect very well the  number of tests 
performed.
</p></li></ul><p>The test case function takes one argument, <strong>Config</strong>, which
contains configuration information such as <strong>data_dir</strong> and
<strong>priv_dir</strong>. (For details about these, see section 
<a href="#data_priv_dir">Data and Private Directories</a>.
The value of <strong>Config</strong> at the time of the call, is the same 
as the return value from <strong>init_per_testcase</strong>, mentioned earlier.
</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>The test case function argument <strong>Config</strong> is not to be 
confused with the information that can be retrieved from the
configuration files (using <a href="./ct#get_config-1">ct#get_config-1</a>). The test case argument <strong>Config</strong>
is to be used for runtime configuration of the test suite and the 
test cases, while configuration files are to contain data 
related to the SUT. These two types of configuration data are handled 
differently.</p></div><p>As parameter <strong>Config</strong> is a list of key-value tuples, that is,
a data type called a property list, it can be handled by the
<a href="./proplists">stdlib/proplists</a> module.
A value can, for example, be searched for and returned with function 
<a href="../stdlib/proplists#get_value-2">stdlib/proplists#get_value-2</a>.
Also, or alternatively, the general <a href="./lists">stdlib/lists</a>
module contains useful functions. Normally, the only operations 
performed on <strong>Config</strong> is insert (adding a tuple to the head of the list) 
and lookup. <strong>Common Test</strong> provides a simple macro named <strong>?config</strong>, 
which returns a value of an item in <strong>Config</strong> given the key (exactly like 
<strong>proplists:get_value</strong>). Example: <strong>PrivDir = ?config(priv_dir, Config)</strong>.
</p><p>If the test case function crashes or exits purposely, it is considered 
<em>failed</em>. If it returns a value (no matter what value), it is 
considered successful. An exception to this rule is the return value 
<strong>{skip,Reason}</strong>. If this tuple is returned, the test case is considered 
skipped and is logged as such.</p><p>If the test case returns the tuple <strong>{comment,Comment}</strong>, the case
is considered successful and <strong>Comment</strong> is printed in the overview 
log file. This is equal to calling 
<a href="./ct#comment-1">ct#comment-1</a>.
</p><a name="info_function"></a><h3>Test Case Information Function</h3><p>For each test case function there can be an extra function
with the same name but without arguments. This is the test case
information function. It is expected to return a list of tagged 
tuples that specifies various properties regarding the test case.
</p><p>The following tags have special meaning:</p><dl><dt><strong>timetrap</strong></dt><dd> <p>
Sets the maximum time the test case is allowed to execute. If
this time is exceeded, the test case fails with
reason <strong>timetrap_timeout</strong>. Notice that <strong>init_per_testcase</strong> 
and <strong>end_per_testcase</strong> are included in the timetrap time.
For details, see section 
<a href="./write_test_chapter#timetraps">Timetrap Time-Outs</a>.
</p> </dd><dt><strong>userdata</strong></dt><dd> <p>
Specifies any data related to the test case. This
data can be retrieved at any time using the 
<a href="./ct#userdata-3">ct#userdata-3</a>
utility function.
</p> </dd><dt><strong>silent_connections</strong></dt><dd> <p>
For details, see section 
<a href="./run_test_chapter#silent_connections">Silent Connections</a>.
</p> </dd><dt><strong>require</strong></dt><dd> <p>
Specifies configuration variables required by the
test case. If the required configuration variables are not
found in any of the test system configuration files, the test case is
skipped.</p>  <p>
A required variable can also be given a default value to 
be used if the variable is not found in any configuration file. To specify 
a default value, add a tuple on the form 
<strong>{default_config,ConfigVariableName,Value}</strong> to the test case information list 
(the position in the list is irrelevant).
</p> <p><em>Examples:</em></p> <pre>
 testcase1() -&gt; 
     [{require, ftp},
      {default_config, ftp, [{ftp, "my_ftp_host"},
                             {username, "aladdin"},
                             {password, "sesame"}]}}].</pre> <pre>
 testcase2() -&gt; 
     [{require, unix_telnet, unix},
      {require, {unix, [telnet, username, password]}},
      {default_config, unix, [{telnet, "my_telnet_host"},
                              {username, "aladdin"},
                              {password, "sesame"}]}}].</pre> </dd></dl><p>For more information about <strong>require</strong>, see section
<a href="./config_file_chapter#require_config_data"> Requiring and Reading Configuration Data</a>
in section External Configuration Data and function 
<a href="./ct#require-1">ct#require-1</a>.</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>Specifying a default value for a required variable can result
in a test case always getting executed. This might not be a desired behavior.</p></div><p>If <strong>timetrap</strong> or <strong>require</strong>, or both, is not set specifically for
a particular test case, default values specified by function
<a href="./common_test#Module:suite-0">common_test#Module:suite-0</a> 
are used.
</p><p>Tags other than the earlier mentioned are ignored by the test server.
</p><p>
An example of a test case information function follows:
</p><pre>
 reboot_node() -&gt;
     [
      {timetrap,{seconds,60}},
      {require,interfaces},
      {userdata,
          [{description,"System Upgrade: RpuAddition Normal RebootNode"},
           {fts,"http://someserver.ericsson.se/test_doc4711.pdf"}]}                  
     ].</pre><a name="suite"></a><h3>Test Suite Information Function</h3><p>Function <a href="./common_test#Module:suite-0">common_test#Module:suite-0</a> 
can, for example, be used in a test suite module to set a default 
<strong>timetrap</strong> value and to <strong>require</strong> external configuration data. 
If a test case, or a group information function also specifies any of the information tags, it
overrides the default values set by <strong>suite/0</strong>. For details, 
see 
<a href="#info_function">Test Case Information Function</a> and
<a href="#test_case_groups">Test Case Groups</a>.
</p><p>The following options can also be specified with the suite information list:</p><ul><li><strong>stylesheet</strong>,  see <a href="./run_test_chapter#html_stylesheet">HTML Style Sheets</a></li><li><strong>userdata</strong>,  see <a href="#info_function">Test Case Information Function</a></li><li><strong>silent_connections</strong>,  see <a href="./run_test_chapter#silent_connections">Silent Connections</a></li></ul><p>
An example of the suite information function follows:
</p><pre>
 suite() -&gt;
     [
      {timetrap,{minutes,10}},
      {require,global_names},
      {userdata,[{info,"This suite tests database transactions."}]},
      {silent_connections,[telnet]},
      {stylesheet,"db_testing.css"}
     ].</pre><a name="test_case_groups"></a><h3>Test Case Groups</h3><p>A test case group is a set of test cases sharing configuration 
functions and execution properties. Test case groups are defined by
function 
<a href="./common_test#Module:groups-0">common_test#Module:groups-0</a>
according to the following syntax:</p><pre>
 groups() -&gt; GroupDefs

 Types:

 GroupDefs = [GroupDef]
 GroupDef = {GroupName,Properties,GroupsAndTestCases}
 GroupName = atom()
 GroupsAndTestCases = [GroupDef | {group,GroupName} | TestCase |
                      {testcase,TestCase,TCRepeatProps}]
 TestCase = atom()
 TCRepeatProps = [{repeat,N} | {repeat_until_ok,N} | {repeat_until_fail,N}]</pre><p><strong>GroupName</strong> is the name of the group and must be unique within
the test suite module. Groups can be nested, by including a group definition 
within the <strong>GroupsAndTestCases</strong> list of another group. 
<strong>Properties</strong> is the list of execution 
properties for the group. The possible values are as follows:</p><pre>
 Properties = [parallel | sequence | Shuffle | {GroupRepeatType,N}]
 Shuffle = shuffle | {shuffle,Seed}
 Seed = {integer(),integer(),integer()}
 GroupRepeatType = repeat | repeat_until_all_ok | repeat_until_all_fail |
                   repeat_until_any_ok | repeat_until_any_fail
 N = integer() | forever</pre><p><em>Explanations:</em></p><dl><dt><strong>parallel</strong></dt><dd><p><strong>Common Test</strong> executes all test cases in the group in parallel.</p></dd><dt><strong>sequence</strong></dt><dd><p>The cases are executed in a sequence as described in section
<a href="./dependencies_chapter#sequences">Sequences</a> in section
Dependencies Between Test Cases and Suites.</p></dd><dt><strong>shuffle</strong></dt><dd><p>The cases in the group are executed in random order.</p></dd><dt><strong>repeat, repeat_until_*</strong></dt><dd><p>Orders <strong>Common Test</strong> to repeat execution of all the cases in the 
group a given number of times, or until any, or all, cases fail or succeed.</p></dd></dl><p><em>Example:</em></p><pre>
 groups() -&gt; [{group1, [parallel], [test1a,test1b]},
              {group2, [shuffle,sequence], [test2a,test2b,test2c]}].</pre><p>To specify in which order groups are to be executed (also with respect
to test cases that are not part of any group), add tuples on the form 
<strong>{group,GroupName}</strong> to the <strong>all/0</strong> list.</p><p><em>Example:</em></p><pre>
 all() -&gt; [testcase1, {group,group1}, {testcase,testcase2,[{repeat,10}]}, {group,group2}].</pre><p>Execution properties with a group tuple in 
<strong>all/0</strong>: <strong>{group,GroupName,Properties}</strong> can also be specified. 
These properties override those specified in the group definition (see
<strong>groups/0</strong> earlier). This way, the same set of tests can be run,
but with different properties, without having to make copies of the group
definition in question.</p><p>If a group contains subgroups, the execution properties for these can
also be specified in the group tuple:
<strong>{group,GroupName,Properties,SubGroups}</strong>
Where, <strong>SubGroups</strong> is a list of tuples, <strong>{GroupName,Properties}</strong> or
<strong>{GroupName,Properties,SubGroups}</strong> representing the subgroups.
Any subgroups defined in <strong>group/0</strong> for a group, that are not specified
in the <strong>SubGroups</strong> list, executes with their predefined
properties.</p><p><em>Example:</em></p><pre>
 groups() -&gt; {tests1, [], [{tests2, [], [t2a,t2b]},
                           {tests3, [], [t31,t3b]}]}.</pre><p>To execute group <strong>tests1</strong> twice with different properties for <strong>tests2</strong>
each time:</p><pre>
 all() -&gt;
    [{group, tests1, default, [{tests2, [parallel]}]},
     {group, tests1, default, [{tests2, [shuffle,{repeat,10}]}]}].</pre><p>This is equivalent to the following specification:</p><pre>
 all() -&gt;
    [{group, tests1, default, [{tests2, [parallel]},
                               {tests3, default}]},
     {group, tests1, default, [{tests2, [shuffle,{repeat,10}]},
                               {tests3, default}]}].</pre><p>Value <strong>default</strong> states that the predefined properties
are to be used.</p><p>The following example shows how to override properties in a scenario
with deeply nested groups:</p><pre>
 groups() -&gt;
    [{tests1, [], [{group, tests2}]},
     {tests2, [], [{group, tests3}]},
     {tests3, [{repeat,2}], [t3a,t3b,t3c]}].

 all() -&gt;
    [{group, tests1, default, 
      [{tests2, default,
        [{tests3, [parallel,{repeat,100}]}]}]}].</pre><p>The described syntax can also be used in test specifications
to change group properties at the time of execution,
without having to edit the test suite. For more information, see
section <a href="./run_test_chapter#test_specifications">Test Specifications</a> in section Running Tests and Analyzing Results.</p><p>As illustrated, properties can be combined. If, for example,
<strong>shuffle</strong>, <strong>repeat_until_any_fail</strong>, and <strong>sequence</strong>
are all specified, the test cases in the group are executed
repeatedly, and in random order, until a test case fails. Then
execution is immediately stopped and the remaining cases are skipped.</p><p>Before execution of a group begins, the configuration function
<a href="./common_test#Module:init_per_group-2">common_test#Module:init_per_group-2</a> 
is called. The list of tuples returned from this function is passed to the 
test cases in the usual manner by argument <strong>Config</strong>. 
<strong>init_per_group/2</strong> is meant to be used for initializations common 
for the test cases in the group. After execution of the group is finished, function
<a href="./common_test#Module:end_per_group-2">common_test#Module:end_per_group-2</a> 
is called. This function is meant to be used for cleaning up after 
<strong>init_per_group/2</strong>. If the init function is defined, so must the end function be.</p><p>Whenever a group is executed, if <strong>init_per_group</strong> and
<strong>end_per_group</strong> do not exist in the suite, <strong>Common Test</strong> calls
dummy functions (with the same names) instead. Output generated by
hook functions are saved to the log files for these dummies.
For more information, see section 
<a href="./ct_hooks_chapter#manipulating">Manipulating Tests</a>
in section Common Test Hooks.
</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p><strong>init_per_testcase/2</strong> and <strong>end_per_testcase/2</strong>
are always called for each individual test case, no matter if the case 
belongs to a group or not.</p></div><p>The properties for a group are always printed in the top of the HTML log 
for <strong>init_per_group/2</strong>. The total execution time for a group is
included at the bottom of the log for <strong>end_per_group/2</strong>.</p><p>Test case groups can be nested so sets of groups can be 
configured with the same <strong>init_per_group/2</strong> and <strong>end_per_group/2</strong>
functions. Nested groups can be defined by including a group definition,
or a group name reference, in the test case list of another group.</p><p><em>Example:</em></p><pre>
 groups() -&gt; [{group1, [shuffle], [test1a,
                                   {group2, [], [test2a,test2b]},
                                   test1b]},
              {group3, [], [{group,group4},
                            {group,group5}]},
              {group4, [parallel], [test4a,test4b]},
              {group5, [sequence], [test5a,test5b,test5c]}].</pre><p>In the previous example, if <strong>all/0</strong> returns group name references
in the order <strong>[{group,group1},{group,group3}]</strong>, the order of the 
configuration functions and test cases becomes the following (notice that
<strong>init_per_testcase/2</strong> and <strong>end_per_testcase/2:</strong> are also
always called, but not included in this example for simplification):</p><pre>
 init_per_group(group1, Config) -&gt; Config1  (*)
      test1a(Config1)
      init_per_group(group2, Config1) -&gt; Config2
           test2a(Config2), test2b(Config2)
      end_per_group(group2, Config2)
      test1b(Config1)
 end_per_group(group1, Config1) 
 init_per_group(group3, Config) -&gt; Config3
      init_per_group(group4, Config3) -&gt; Config4
           test4a(Config4), test4b(Config4)  (**)
      end_per_group(group4, Config4)
      init_per_group(group5, Config3) -&gt; Config5
           test5a(Config5), test5b(Config5), test5c(Config5)
      end_per_group(group5, Config5)
 end_per_group(group3, Config3)</pre><p>(*) The order of test case <strong>test1a</strong>, <strong>test1b</strong>, and <strong>group2</strong> is
undefined, as <strong>group1</strong> has a shuffle property.</p><p>(**) These cases are not executed in order, but in parallel.</p><p>Properties are not inherited from top-level groups to nested 
subgroups. For instance, in the previous example, the test cases in <strong>group2</strong> 
are not executed in random order (which is the property of <strong>group1</strong>).</p><h3>Parallel Property and Nested Groups</h3><p>If a group has a parallel property, its test cases are spawned
simultaneously and get executed in parallel. However, a test case is not 
allowed to execute in parallel with <strong>end_per_group/2</strong>, which means
that the time to execute a parallel group is equal to the
execution time of the slowest test case in the group. A negative side
effect of running test cases in parallel is that the HTML summary pages
are not updated with links to the individual test case logs until function 
<strong>end_per_group/2</strong> for the group has finished.</p><p>A group nested under a parallel group starts executing in parallel 
with previous (parallel) test cases (no matter what properties the nested 
group has). However, as test cases are never executed in parallel with 
<strong>init_per_group/2</strong> or <strong>end_per_group/2</strong> of the same group, it is 
only after a nested group has finished that remaining parallel cases 
in the previous group become spawned.</p><h3>Parallel Test Cases and I/O</h3><p>A parallel test case has a private I/O server as its group leader. 
(For a description of the group leader concept, see
<a href="./index">ERTS</a>).
The central I/O server process, which handles the output from 
regular test cases and configuration functions, does not respond to I/O messages
during execution of parallel groups. This is important to understand
to avoid certain traps, like the following:</p><p>If a process, <strong>P</strong>, is spawned during execution of, for example,
<strong>init_per_suite/1</strong>, it inherits the group leader of the
<strong>init_per_suite</strong> process. This group leader is the central I/O server
process mentioned earlier. If, at a later time, <em>during parallel test case execution</em>, some event triggers process <strong>P</strong> to call
<strong>io:format/1/2</strong>, that call never returns (as the group leader
is in a non-responsive state) and causes <strong>P</strong> to hang.
</p><h3>Repeated Groups</h3><a name="repeated_groups"></a><p>A test case group can be repeated a certain number of times
(specified by an integer) or indefinitely (specified by <strong>forever</strong>).
The repetition can also be stopped too early if any or all cases
fail or succeed, that is, if any of the properties <strong>repeat_until_any_fail</strong>,
<strong>repeat_until_any_ok</strong>, <strong>repeat_until_all_fail</strong>, or 
<strong>repeat_until_all_ok</strong> is used. If the basic <strong>repeat</strong>
property is used, status of test cases is irrelevant for the repeat 
operation.</p><p>The status of a subgroup can be returned (<strong>ok</strong> or
<strong>failed</strong>), to affect the execution of the group on the level above. 
This is accomplished by, in <strong>end_per_group/2</strong>, looking up the value
of <strong>tc_group_properties</strong> in the <strong>Config</strong> list and checking the
result of the test cases in the group. If status <strong>failed</strong> is to be
returned from the group as a result, <strong>end_per_group/2</strong> is to return
the value <strong>{return_group_result,failed}</strong>. The status of a subgroup
is taken into account by <strong>Common Test</strong> when evaluating if execution of a
group is to be repeated or not (unless the basic <strong>repeat</strong>
property is used).</p><p>The value of <strong>tc_group_properties</strong> is a list of status tuples, 
each with the key <strong>ok</strong>, <strong>skipped</strong>, and <strong>failed</strong>. The
value of a status tuple is a list with names of test cases 
that have been executed with the corresponding status as result.</p><p>The following is an example of how to return the status from a group:</p><pre>
 end_per_group(_Group, Config) -&gt;
     Status = ?config(tc_group_result, Config),
     case proplists:get_value(failed, Status) of
         [] -&gt;                                   % no failed cases 
             {return_group_result,ok};
         _Failed -&gt;                              % one or more failed
             {return_group_result,failed}
     end.</pre><p>It is also possible, in <strong>end_per_group/2</strong>, to check the status of
a subgroup (maybe to determine what status the current group is to
return). This is as simple as illustrated in the previous example, only the
group name is stored in a tuple <strong>{group_result,GroupName}</strong>,
which can be searched for in the status lists.</p><p><em>Example:</em></p><pre>
 end_per_group(group1, Config) -&gt;
     Status = ?config(tc_group_result, Config),
     Failed = proplists:get_value(failed, Status),
     case lists:member({group_result,group2}, Failed) of
           true -&gt;
               {return_group_result,failed};
           false -&gt;                                                    
               {return_group_result,ok}
     end; 
 ...</pre><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>When a test case group is repeated, the configuration 
functions <strong>init_per_group/2</strong> and <strong>end_per_group/2</strong> are 
also always called with each repetition.</p></div><h3>Shuffled Test Case Order</h3><p>The order in which test cases in a group are executed is under normal
circumstances the same as the order specified in the test case list 
in the group definition. With property <strong>shuffle</strong> set, however,
<strong>Common Test</strong> instead executes the test cases in random order.</p><p>You can provide a seed value (a tuple of three integers) with
the shuffle property <strong>{shuffle,Seed}</strong>. This way, the same shuffling
order can be created every time the group is executed. If no seed value
is specified, <strong>Common Test</strong> creates a "random" seed for the shuffling operation 
(using the return value of <strong>erlang:timestamp/0</strong>). The seed value is always
printed to the <strong>init_per_group/2</strong> log file so that it can be used to
recreate the same execution order in a subsequent test run.</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>If a shuffled test case group is repeated, the seed is not
reset between turns.</p></div><p>If a subgroup is specified in a group with a <strong>shuffle</strong> property,
the execution order of this subgroup in relation to the test cases 
(and other subgroups) in the group, is random. The order of the
test cases in the subgroup is however not random (unless the 
subgroup has a <strong>shuffle</strong> property).</p><a name="group_info"></a><h3>Group Information Function</h3><p>The test case group information function, <strong>group(GroupName)</strong>,
serves the same purpose as the suite- and test case information
functions previously described. However, the scope for
the group information function, is all test cases and subgroups in the
group in question (<strong>GroupName</strong>).</p><p><em>Example:</em></p><pre>
 group(connection_tests) -&gt;
    [{require,login_data},
     {timetrap,1000}].</pre><p>The group information properties override those set with the
suite information function, and can in turn be overridden by test
case information properties. For a list of valid information properties 
and more general information, see the
<a href="#info_function">Test Case Information Function</a>.
</p><h3>Information Functions for Init- and End-Configuration</h3><p>Information functions can also be used for functions <strong>init_per_suite</strong>,
<strong>end_per_suite</strong>, <strong>init_per_group</strong>, and <strong>end_per_group</strong>,
and they work the same way as with the
<a href="#info_function">Test Case Information Function</a>. 
This is useful, for example, for setting timetraps and requiring 
external configuration data relevant only for the configuration 
function in question (without affecting properties set for groups 
and test cases in the suite).</p><p>The information function <strong>init/end_per_suite()</strong> is called for
<strong>init/end_per_suite(Config)</strong>, and information function
<strong>init/end_per_group(GroupName)</strong> is called for
<strong>init/end_per_group(GroupName,Config)</strong>. However, information functions
cannot be used with <strong>init/end_per_testcase(TestCase, Config)</strong>,
as these configuration functions execute on the test case process
and use the same properties as the test case (that is, the properties
set by the test case information function, <strong>TestCase()</strong>). For a list 
of valid information properties and more general information, see the
<a href="#info_function">Test Case Information Function</a>.
</p><a name="data_priv_dir"></a><h3>Data and Private Directories</h3><p>In the data directory, <strong>data_dir</strong>, the test module has 
its own files needed for the testing. The name of <strong>data_dir</strong> 
is the the name of the test suite followed by <strong>"_data"</strong>. 
For example, <strong>"some_path/foo_SUITE.beam"</strong> has the data directory
<strong>"some_path/foo_SUITE_data/"</strong>. Use this directory for portability,
that is, to avoid hardcoding directory names in your suite. As the data
directory is stored in the same directory as your test suite, you can
rely on its existence at runtime, even if the path to your
test suite directory has changed between test suite implementation and
execution.
</p><p>
<strong>priv_dir</strong> is the private directory for the test cases.
This directory can be used whenever a test case (or configuration function)
needs to write something to file. The name of the private directory is
generated by <strong>Common Test</strong>, which also creates the directory.
</p><p>By default, <strong>Common Test</strong> creates one central private directory
per test run, shared by all test cases. This is not always suitable.
Especially if the same test cases are executed multiple times during
a test run (that is, if they belong to a test case group with property
<strong>repeat</strong>) and there is a risk that files in the private directory get
overwritten. Under these circumstances, <strong>Common Test</strong> can be 
configured to create one dedicated private directory per
test case and execution instead. This is accomplished with
the flag/option <strong>create_priv_dir</strong> (to be used with the
<a href="ct_run">ct_run</a> program, the 
<a href="./ct#run_test-1">ct#run_test-1</a> function, or
as test specification term). There are three possible values
for this option as follows:
</p><ul><li><strong>auto_per_run</strong></li><li><strong>auto_per_tc</strong></li><li><strong>manual_per_tc</strong></li></ul><p>
The first value indicates the default <strong>priv_dir</strong> behavior, that is,
one private directory created per test run. The two latter
values tell <strong>Common Test</strong> to generate a unique test directory name
per test case and execution. If the auto version is used, <em>all</em>
private directories are created automatically. This can become very 
inefficient for test runs with many test cases or repetitions, or both. 
Therefore, if the manual version is used instead, the test case must tell 
<strong>Common Test</strong> to create <strong>priv_dir</strong> when it needs it.
It does this by calling the function 
<a href="./ct#make_priv_dir-0">ct#make_priv_dir-0</a>.
</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>Do not depend on the current working directory for
reading and writing data files, as this is not portable. All 
scratch files are to be written in the <strong>priv_dir</strong> and all 
data files are to be located in <strong>data_dir</strong>. Also, 
the <strong>Common Test</strong> server sets the current working directory to 
the test case log directory at the start of every case.
</p></div><h3>Execution Environment</h3><p>Each test case is executed by a dedicated Erlang process. The
process is spawned when the test case starts, and terminated when
the test case is finished. The configuration functions 
<strong>init_per_testcase</strong> and <strong>end_per_testcase</strong> execute on the 
same process as the test case.
</p><p>The configuration functions <strong>init_per_suite</strong> and 
<strong>end_per_suite</strong> execute, like test cases, on dedicated Erlang
processes.
</p><a name="timetraps"></a><h3>Timetrap Time-Outs</h3><p>The default time limit for a test case is 30 minutes, unless a
<strong>timetrap</strong> is specified either by the suite-, group-,
or test case information function. The timetrap time-out value defined by
<strong>suite/0</strong> is the value that is used for each test case
in the suite (and for the configuration functions
<strong>init_per_suite/1</strong>, <strong>end_per_suite/1</strong>, <strong>init_per_group/2</strong>,
and <strong>end_per_group/2</strong>). A timetrap value defined by
<strong>group(GroupName)</strong> overrides one defined by <strong>suite()</strong>
and is used for each test case in group <strong>GroupName</strong>, and any
of its subgroups. If a timetrap value is defined by <strong>group/1</strong>
for a subgroup, it overrides that of its higher level groups. Timetrap
values set by individual test cases (by the test case information
function) override both group- and suite- level timetraps.</p><p>A timetrap can also be set or reset dynamically during the
execution of a test case, or configuration function. 
This is done by calling
<a href="./ct#timetrap-1">ct#timetrap-1</a>. 
This function cancels the current timetrap and starts a new one 
(that stays active until time-out, or end of the current function).</p><p>Timetrap values can be extended with a multiplier value specified at
startup with option <strong>multiply_timetraps</strong>. It is also possible
to let the test server decide to scale up timetrap time-out values
automatically. That is, if tools such as <strong>cover</strong> or <strong>trace</strong> 
are running during the test. This feature is disabled by default and 
can be enabled with start option <strong>scale_timetraps</strong>.</p><p>If a test case needs to suspend itself for a time that also gets
multipled by <strong>multiply_timetraps</strong> (and possibly also scaled up if
<strong>scale_timetraps</strong> is enabled), the function 
<a href="./ct#sleep-1">ct#sleep-1</a>
can be used (instead of, for example, <strong>timer:sleep/1</strong>).</p><p>A function (<strong>fun/0</strong> or <strong>{Mod,Func,Args}</strong> (MFA) tuple) can be 
specified as timetrap value in the suite-, group- and test case information 
function, and as argument to function 
<a href="./ct#timetrap-1">ct#timetrap-1</a>.</p><p><em>Examples:</em></p><p><strong>{timetrap,{my_test_utils,timetrap,[?MODULE,system_start]}}</strong></p><p><strong>ct:timetrap(fun() -&gt; my_timetrap(TestCaseName, Config) end)</strong></p><p>The user timetrap function can be used for two things as follows:</p><ul><li>To act as a timetrap. The time-out is triggered when the function returns.</li><li>To return a timetrap time value (other than a function).</li></ul><p>Before execution of the timetrap function (which is performed
on a parallel, dedicated timetrap process), <strong>Common Test</strong> cancels
any previously set timer for the test case or configuration function.    
When the timetrap function returns, the time-out is triggered, <em>unless</em>
the return value is a valid timetrap time, such as an integer,
or a <strong>{SecMinOrHourTag,Time}</strong> tuple (for details, see module
<a href="common_test">common_test</a>). If a time value 
is returned, a new timetrap is started to generate a time-out after 
the specified time.</p><p>The user timetrap function can return a time value after a delay.
The effective timetrap time is then the delay time <em>plus</em> the
returned time.</p><a name="logging"></a><h3>Logging - Categories and Verbosity Levels</h3><p><strong>Common Test</strong> provides the following three main functions for 
printing strings:</p><ul><li><strong>ct:log(Category, Importance, Format, FormatArgs, Opts)</strong></li><li><strong>ct:print(Category, Importance, Format, FormatArgs)</strong></li><li><strong>ct:pal(Category, Importance, Format, FormatArgs)</strong></li></ul><p>The <a href="./ct#log-1">ct#log-1</a> function 
prints a string to the test case log file. 
The <a href="./ct#print-1">ct#print-1</a> function 
prints the string to screen.
The <a href="./ct#pal-1">ct#pal-1</a> function 
prints the same string both to file and screen. The functions are described 
in module <a href="ct">ct</a>.
</p><p>The optional <strong>Category</strong> argument can be used to categorize the
log printout. Categories can be used for two things as follows:</p><ul><li>To compare the importance of the printout to a specific verbosity level.</li><li>To format the printout according to a user-specific HTML Style Sheet (CSS).</li></ul><p>Argument <strong>Importance</strong> specifies a level of importance
that, compared to a verbosity level (general and/or set per category),
determines if the printout is to be visible. <strong>Importance</strong>
is any integer in the range 0..99. Predefined constants
exist in the <strong>ct.hrl</strong> header file. The default importance level,
<strong>?STD_IMPORTANCE</strong> (used if argument <strong>Importance</strong> is not
provided), is 50. This is also the importance used for standard I/O,
for example, from printouts made with <strong>io:format/2</strong>, 
<strong>io:put_chars/1</strong>, and so on.</p><p><strong>Importance</strong> is compared to a verbosity level set by the
<strong>verbosity</strong> start flag/option. The level can be set per
category or generally, or both. If <strong>verbosity</strong> is not set by the user,
a level of 100 (<strong>?MAX_VERBOSITY</strong> = all printouts visible) is used as
default value. <strong>Common Test</strong> performs the following test:</p><pre>
Importance &gt;= (100-VerbosityLevel)</pre><p>The constant <strong>?STD_VERBOSITY</strong> has value 50 (see <strong>ct.hrl</strong>).
At this level, all standard I/O gets printed. If a lower verbosity level
is set, standard I/O printouts are ignored. Verbosity level 0 effectively
turns all logging off (except from printouts made by <strong>Common Test</strong>
itself).</p><p>The general verbosity level is not associated with any particular
category. This level sets the threshold for the standard I/O printouts,
uncategorized <strong>ct:log/print/pal</strong> printouts, and
printouts for categories with undefined verbosity level.</p><p><em>Examples:</em></p><p>Some printouts during test case execution:</p><pre>
 io:format("1. Standard IO, importance = ~w~n", [?STD_IMPORTANCE]),
 ct:log("2. Uncategorized, importance = ~w", [?STD_IMPORTANCE]),
 ct:log(info, "3. Categorized info, importance = ~w", [?STD_IMPORTANCE]),
 ct:log(info, ?LOW_IMPORTANCE, "4. Categorized info, importance = ~w", [?LOW_IMPORTANCE]),
 ct:log(error, ?HI_IMPORTANCE, "5. Categorized error, importance = ~w", [?HI_IMPORTANCE]),
 ct:log(error, ?MAX_IMPORTANCE, "6. Categorized error, importance = ~w", [?MAX_IMPORTANCE]),</pre><p>If starting the test with a general verbosity level of 50 (<strong>?STD_VERBOSITY</strong>):</p><pre>
 $ ct_run -verbosity 50</pre><p>the following is printed:</p><pre>
 1. Standard IO, importance = 50
 2. Uncategorized, importance = 50
 3. Categorized info, importance = 50
 5. Categorized error, importance = 75
 6. Categorized error, importance = 99</pre><p>If starting the test with:</p><pre>
 $ ct_run -verbosity 1 and info 75</pre><p>the following is printed:</p><pre>
 3. Categorized info, importance = 50
 4. Categorized info, importance = 25
 6. Categorized error, importance = 99</pre><p>Note that the category argument is not required in order to only specify the
importance of a printout. Example:</p><pre>
ct:pal(?LOW_IMPORTANCE, "Info report: ~p", [Info])</pre><p>Or perhaps in combination with constants:</p><pre>
-define(INFO, ?LOW_IMPORTANCE).
-define(ERROR, ?HI_IMPORTANCE).

ct:log(?INFO, "Info report: ~p", [Info])
ct:pal(?ERROR, "Error report: ~p", [Error])</pre><p>The functions <a href="./ct#set_verbosity-2">ct#set_verbosity-2</a>
and <a href="./ct#get_verbosity-1">ct#get_verbosity-1</a> may be used
to modify and read verbosity levels during test execution.</p><p>The arguments <strong>Format</strong> and <strong>FormatArgs</strong> in <strong>ct:log/print/pal</strong> are
always passed on to the STDLIB function <strong>io:format/3</strong> (For details,
see the <a href="./io">stdlib/io</a> manual page).</p><p><strong>ct:pal/4</strong> and <strong>ct:log/5</strong> add headers to strings being printed to the
log file. The strings are also wrapped in div tags with a CSS class
attribute, so that stylesheet formatting can be applied. To disable this feature for
a printout (i.e. to get a result similar to using <strong>io:format/2</strong>),
call <strong>ct:log/5</strong> with the <strong>no_css</strong> option.</p><p>How categories can be mapped to CSS tags is documented in section
<a href="./run_test_chapter#html_stylesheet">HTML Style Sheets</a>
in section Running Tests and Analyzing Results.</p><p>Common Test will escape special HTML characters (&lt;, &gt; and &amp;) in printouts
to the log file made with <strong>ct:pal/4</strong> and <strong>io:format/2</strong>. In order to print
strings with HTML tags to the log, use the <strong>ct:log/3,4,5</strong> function. The character
escaping feature is per default disabled for <strong>ct:log/3,4,5</strong> but can be enabled with
the <strong>esc_chars</strong> option in the <strong>Opts</strong> list, see <a href="./ct#log-5">ct#log-5</a>.</p><p>If the character escaping feature needs to be disabled (typically for backwards
compatibility reasons), use the <strong>ct_run</strong> start flag <strong>-no_esc_chars</strong>, or the
<strong>ct:run_test/1</strong> start option <strong>{esc_chars,Bool}</strong> (this start option is also
supported in test specifications).</p><p>For more information about log files, see section
<a href="./run_test_chapter#log_files">Log Files</a> 
in section Running Tests and Analyzing Results.</p><h3>Illegal Dependencies</h3><p>Even though it is highly efficient to write test suites with
the <strong>Common Test</strong> framework, mistakes can be made,
mainly because of illegal dependencies. Some of the 
more frequent mistakes from our own experience with running the 
Erlang/OTP test suites follows:</p><ul><li><p>Depending on current directory, and writing there:</p> <p>This is a common error in test suites. It is assumed that
the current directory is the same as the author used as
current directory when the test case was developed. Many test
cases even try to write scratch files to this directory. Instead
<strong>data_dir</strong> and <strong>priv_dir</strong> are to be used to locate 
data and for writing scratch files.
</p> </li><li><p>Depending on execution order:</p> <p>During development of test suites, make no assumptions on the 
execution order of the test cases or suites. For example, a test 
case must not assume that a server it depends on is already 
started by a previous test case. Reasons for this follows:
</p> <ul><li>The user/operator can specify the order at will, and maybe a different execution order is sometimes more relevant or  efficient.</li><li>If the user specifies a whole directory of test suites  for the test, the execution order of the suites depends on  how the files are listed by the operating system, which varies  between systems.</li><li>If a user wants to run only a subset of a test suite,  there is no way one test case could successfully depend on  another.</li></ul> </li><li><p>Depending on Unix:</p> <p>Running Unix commands through <strong>os:cmd</strong> are likely 
not to work on non-Unix platforms.
</p> </li><li><p>Nested test cases:</p> <p>Starting a test case from another not only tests the same
thing twice, but also makes it harder to follow what is being 
tested. Also, if the called test case fails for some
reason, so do the caller. This way, one error gives cause to
several error reports, which is to be avoided.
</p> <p>Functionality common for many test case functions can be 
implemented in common help functions. If these functions are 
useful for test cases across suites, put the help functions 
into common help modules.
</p> </li><li><p>Failure to crash or exit when things go wrong:</p> <p>Making requests without checking that the return value
indicates success can be OK if the test case fails
later, but it is never acceptable just to print an error
message (into the log file) and return successfully. Such test 
cases do harm, as they create a false sense of security when 
overviewing the test results.
</p> </li><li><p>Messing up for subsequent test cases:</p> <p>Test cases are to restore as much of the execution
environment as possible, so that subsequent test cases
do not crash because of their execution order. 
The function 
<a href="./common_test#Module:end_per_testcase-2">common_test#Module:end_per_testcase-2</a> 
is suitable for this.
</p> </li></ul><h3>General</h3><p>A test is performed by running one or more test suites. A test suite
consists of test cases, configuration functions, and information 
functions. Test cases can be grouped in so called test case groups. 
A test suite is an Erlang module and test cases are implemented as 
Erlang functions. Test suites are stored in test directories.</p><a name="skipping_test_cases"></a><h3>Skipping Test Cases</h3><p>Certain test cases can be skipped, for example, if you
know beforehand that a specific test case fails. The reason can be
functionality that is not yet implemented, a bug that is known but
not yet fixed, or some functionality that does not work or is not
applicable on a specific platform.</p><p>Test cases can be skipped in the following ways:</p><ul><li>Using <strong>skip_suites</strong> and <strong>skip_cases</strong> terms in  <a href="./run_test_chapter#test_specifications">test specifications</a>. </li><li>Returning <strong>{skip,Reason}</strong> from function <a href="./common_test#Module:init_per_testcase-2">common_test#Module:init_per_testcase-2</a> or  <a href="./common_test#Module:init_per_suite-1">common_test#Module:init_per_suite-1</a>.</li><li>Returning <strong>{skip,Reason}</strong> from the execution clause of the test case. The execution clause is called, so the author  must ensure that the test case does not run.</li></ul><p>When a test case is skipped, it is noted as <strong>SKIPPED</strong>
in the HTML log.</p><h3>Definition of Terms</h3><dl><dt><em>Auto-skipped test case</em></dt><dd> <p>When a configuration function fails (that is, terminates unexpectedly), 
the test cases depending on the configuration function are
skipped automatically by <strong>Common Test</strong>. The status of the test cases 
is then "auto-skipped".	Test cases are also "auto-skipped" by
<strong>Common Test</strong> if the required configuration data is unavailable at
runtime.</p> </dd><dt><em>Configuration function</em></dt><dd> <p>A function in a test suite that is meant to be used for
setting up, cleaning up, and/or verifying the state and 
environment on the System Under Test (SUT) and/or the <strong>Common Test</strong> 
host node, so that a test case (or a set of test cases) can 
execute correctly.</p>      </dd><dt><em>Configuration file</em></dt><dd> <p>A file containing data related to a test and/or an SUT,
for example, protocol server addresses, client
login details, and hardware interface addresses. That is, any data
that is to be handled as variable in the suite and not
be hard-coded.</p>    </dd><dt><em>Configuration variable</em></dt><dd> <p>A name (an Erlang atom) associated with a data value read from
a configuration file.</p> </dd><dt><strong>data_dir</strong></dt><dd> <p>Data directory for a test suite. This directory contains
any files used by the test suite, for example, extra Erlang
modules, binaries, or data files.</p> </dd><dt><em>Information function</em></dt><dd> <p>A function in a test suite that returns a list of properties
(read by the <strong>Common Test</strong> server) that describes the conditions 
for executing the test cases in the suite.</p> </dd><dt><em>Major log file</em></dt><dd> <p>An overview and summary log file for one or more test suites.</p> </dd><dt><em>Minor log file</em></dt><dd> <p>A log file for one particular test case. Also called the 
test case log file.</p> </dd><dt><strong>priv_dir</strong></dt><dd> <p>Private directory for a test suite. This directory is to
be used when the test suite needs to write to files.</p> </dd><dt><strong>ct_run</strong></dt><dd> <p>The name of an executable program that can be
used as an interface for specifying and running
tests with <strong>Common Test</strong>.</p> </dd><dt><em>Test case</em></dt><dd> <p>A single test included in a test suite. A test case is
implemented as a function in a test suite module.</p> </dd><dt><em>Test case group</em></dt><dd> <p>A set of test cases sharing configuration functions and 
execution properties. The execution properties specify if 
the test cases in the group are to be executed in random order,
in parallel, or in sequence, and if the execution of the group 
is be repeated. Test case groups can also be nested. That is, 
a group can, besides test cases, contain subgroups.</p> </dd><dt><em>Test suite</em></dt><dd> <p>An Erlang module containing a collection of test cases for
a specific functional area.</p> </dd><dt><em>Test directory</em></dt><dd> <p>A directory containing one or more test suite modules,
that is, a group of test suites.</p> </dd><dt><em>Argument</em> <strong>Config</strong></dt><dd> <p>A list of key-value tuples (that is, a property list) containing
runtime configuration data passed from the configuration
functions to the test cases.</p> </dd><dt><em>User-skipped test case</em></dt><dd> <p>The status of a test case explicitly skipped in any of 
the ways described in section
<a href="#skipping_test_cases">Skipping Test Cases</a>.
</p> </dd></dl><a name="top"></a><h3>Test Suite Example</h3><p>The following example test suite shows some tests of a database server:
</p><pre><code class="">
 -module(db_data_type_SUITE).

 -include_lib("common_test/include/ct.hrl").

 %% Test server callbacks
 -export([suite/0, all/0, 
	  init_per_suite/1, end_per_suite/1,
	  init_per_testcase/2, end_per_testcase/2]).

 %% Test cases
 -export([string/1, integer/1]).

 -define(CONNECT_STR, "DSN=sqlserver;UID=alladin;PWD=sesame").

 %%--------------------------------------------------------------------
 %% COMMON TEST CALLBACK FUNCTIONS
 %%--------------------------------------------------------------------

 %%--------------------------------------------------------------------
 %% Function: suite() -&gt; Info
 %%
 %% Info = [tuple()]
 %%   List of key/value pairs.
 %%
 %% Description: Returns list of tuples to set default properties
 %%              for the suite.
 %%--------------------------------------------------------------------
 suite() -&gt; 
     [{timetrap,{minutes,1}}].  

 %%--------------------------------------------------------------------
 %% Function: init_per_suite(Config0) -&gt; Config1
 %%
 %% Config0 = Config1 = [tuple()]
 %%   A list of key/value pairs, holding the test case configuration.
 %%
 %% Description: Initialization before the suite.
 %%--------------------------------------------------------------------
 init_per_suite(Config) -&gt; 
     {ok, Ref} = db:connect(?CONNECT_STR, []),
     TableName = db_lib:unique_table_name(),	
     [{con_ref, Ref },{table_name, TableName}| Config]. 

 %%--------------------------------------------------------------------
 %% Function: end_per_suite(Config) -&gt; term()
 %%
 %% Config = [tuple()]
 %%   A list of key/value pairs, holding the test case configuration.
 %%
 %% Description: Cleanup after the suite.
 %%--------------------------------------------------------------------
 end_per_suite(Config) -&gt;    
     Ref = ?config(con_ref, Config),
     db:disconnect(Ref),
     ok.
 
 %%--------------------------------------------------------------------
 %% Function: init_per_testcase(TestCase, Config0) -&gt; Config1
 %%
 %% TestCase = atom()
 %%   Name of the test case that is about to run.
 %% Config0 = Config1 = [tuple()]
 %%   A list of key/value pairs, holding the test case configuration.
 %%
 %% Description: Initialization before each test case.
 %%--------------------------------------------------------------------
 init_per_testcase(Case, Config) -&gt;
     Ref = ?config(con_ref, Config),   
     TableName = ?config(table_name, Config),
     ok = db:create_table(Ref, TableName, table_type(Case)),
     Config.

 %%--------------------------------------------------------------------
 %% Function: end_per_testcase(TestCase, Config) -&gt; term()
 %%
 %% TestCase = atom()
 %%   Name of the test case that is finished.
 %% Config = [tuple()]
 %%   A list of key/value pairs, holding the test case configuration.
 %%
 %% Description: Cleanup after each test case.
 %%--------------------------------------------------------------------
 end_per_testcase(_Case, Config) -&gt; 
     Ref = ?config(con_ref, Config),   
     TableName = ?config(table_name, Config),
     ok = db:delete_table(Ref, TableName),   
     ok. 

 %%--------------------------------------------------------------------
 %% Function: all() -&gt; GroupsAndTestCases
 %%
 %% GroupsAndTestCases = [{group,GroupName} | TestCase]
 %% GroupName = atom()
 %%   Name of a test case group.
 %% TestCase = atom()
 %%   Name of a test case.
 %%
 %% Description: Returns the list of groups and test cases that
 %%              are to be executed.
 %%--------------------------------------------------------------------
 all() -&gt;
     [string, integer]. 


 %%--------------------------------------------------------------------
 %% TEST CASES
 %%--------------------------------------------------------------------

 string(Config) -&gt; 
     insert_and_lookup(dummy_key, "Dummy string", Config).

 integer(Config) -&gt; 
     insert_and_lookup(dummy_key, 42, Config).


 insert_and_lookup(Key, Value, Config) -&gt;
     Ref = ?config(con_ref, Config),   
     TableName = ?config(table_name, Config),
     ok = db:insert(Ref, TableName, Key, Value),
     [Value] = db:lookup(Ref, TableName, Key),
     ok = db:delete(Ref, TableName, Key),
     [] = db:lookup(Ref, TableName, Key),
     ok.</code></pre><h3>Test Suite Templates</h3><p>The Erlang mode for the Emacs editor includes two <strong>Common Test</strong> test 
suite templates, one with extensive information in the function headers, and
one with minimal information. A test suite template provides a quick start
for implementing a suite from scratch and gives a good overview
of the available callback functions. The two templates follows:
</p><p><em>Large Common Test Suite</em></p><pre><code class="">
 %%%-------------------------------------------------------------------
 %%% File    : example_SUITE.erl
 %%% Author  : 
 %%% Description : 
 %%%
 %%% Created : 
 %%%-------------------------------------------------------------------
 -module(example_SUITE).

 %% Note: This directive should only be used in test suites.
 -compile(export_all).

 -include_lib("common_test/include/ct.hrl").

 %%--------------------------------------------------------------------
 %% COMMON TEST CALLBACK FUNCTIONS
 %%--------------------------------------------------------------------

 %%--------------------------------------------------------------------
 %% Function: suite() -&gt; Info
 %%
 %% Info = [tuple()]
 %%   List of key/value pairs.
 %%
 %% Description: Returns list of tuples to set default properties
 %%              for the suite.
 %%
 %% Note: The suite/0 function is only meant to be used to return
 %% default data values, not perform any other operations.
 %%--------------------------------------------------------------------
 suite() -&gt;
     [{timetrap,{minutes,10}}].

 %%--------------------------------------------------------------------
 %% Function: init_per_suite(Config0) -&gt;
 %%               Config1 | {skip,Reason} | {skip_and_save,Reason,Config1}
 %%
 %% Config0 = Config1 = [tuple()]
 %%   A list of key/value pairs, holding the test case configuration.
 %% Reason = term()
 %%   The reason for skipping the suite.
 %%
 %% Description: Initialization before the suite.
 %%
 %% Note: This function is free to add any key/value pairs to the Config
 %% variable, but should NOT alter/remove any existing entries.
 %%--------------------------------------------------------------------
 init_per_suite(Config) -&gt;
     Config.

 %%--------------------------------------------------------------------
 %% Function: end_per_suite(Config0) -&gt; term() | {save_config,Config1}
 %%
 %% Config0 = Config1 = [tuple()]
 %%   A list of key/value pairs, holding the test case configuration.
 %%
 %% Description: Cleanup after the suite.
 %%--------------------------------------------------------------------
 end_per_suite(_Config) -&gt;
     ok.

 %%--------------------------------------------------------------------
 %% Function: init_per_group(GroupName, Config0) -&gt;
 %%               Config1 | {skip,Reason} | {skip_and_save,Reason,Config1}
 %%
 %% GroupName = atom()
 %%   Name of the test case group that is about to run.
 %% Config0 = Config1 = [tuple()]
 %%   A list of key/value pairs, holding configuration data for the group.
 %% Reason = term()
 %%   The reason for skipping all test cases and subgroups in the group.
 %%
 %% Description: Initialization before each test case group.
 %%--------------------------------------------------------------------
 init_per_group(_GroupName, Config) -&gt;
     Config.

 %%--------------------------------------------------------------------
 %% Function: end_per_group(GroupName, Config0) -&gt;
 %%               term() | {save_config,Config1}
 %%
 %% GroupName = atom()
 %%   Name of the test case group that is finished.
 %% Config0 = Config1 = [tuple()]
 %%   A list of key/value pairs, holding configuration data for the group.
 %%
 %% Description: Cleanup after each test case group.
 %%--------------------------------------------------------------------
 end_per_group(_GroupName, _Config) -&gt;
     ok.

 %%--------------------------------------------------------------------
 %% Function: init_per_testcase(TestCase, Config0) -&gt;
 %%               Config1 | {skip,Reason} | {skip_and_save,Reason,Config1}
 %%
 %% TestCase = atom()
 %%   Name of the test case that is about to run.
 %% Config0 = Config1 = [tuple()]
 %%   A list of key/value pairs, holding the test case configuration.
 %% Reason = term()
 %%   The reason for skipping the test case.
 %%
 %% Description: Initialization before each test case.
 %%
 %% Note: This function is free to add any key/value pairs to the Config
 %% variable, but should NOT alter/remove any existing entries.
 %%--------------------------------------------------------------------
 init_per_testcase(_TestCase, Config) -&gt;
     Config.

 %%--------------------------------------------------------------------
 %% Function: end_per_testcase(TestCase, Config0) -&gt;
 %%               term() | {save_config,Config1} | {fail,Reason}
 %%
 %% TestCase = atom()
 %%   Name of the test case that is finished.
 %% Config0 = Config1 = [tuple()]
 %%   A list of key/value pairs, holding the test case configuration.
 %% Reason = term()
 %%   The reason for failing the test case.
 %%
 %% Description: Cleanup after each test case.
 %%--------------------------------------------------------------------
 end_per_testcase(_TestCase, _Config) -&gt;
     ok.

 %%--------------------------------------------------------------------
 %% Function: groups() -&gt; [Group]
 %%
 %% Group = {GroupName,Properties,GroupsAndTestCases}
 %% GroupName = atom()
 %%   The name of the group.
 %% Properties = [parallel | sequence | Shuffle | {RepeatType,N}]
 %%   Group properties that may be combined.
 %% GroupsAndTestCases = [Group | {group,GroupName} | TestCase]
 %% TestCase = atom()
 %%   The name of a test case.
 %% Shuffle = shuffle | {shuffle,Seed}
 %%   To get cases executed in random order.
 %% Seed = {integer(),integer(),integer()}
 %% RepeatType = repeat | repeat_until_all_ok | repeat_until_all_fail |
 %%              repeat_until_any_ok | repeat_until_any_fail
 %%   To get execution of cases repeated.
 %% N = integer() | forever
 %%
 %% Description: Returns a list of test case group definitions.
 %%--------------------------------------------------------------------
 groups() -&gt;
     [].

 %%--------------------------------------------------------------------
 %% Function: all() -&gt; GroupsAndTestCases | {skip,Reason}
 %%
 %% GroupsAndTestCases = [{group,GroupName} | TestCase]
 %% GroupName = atom()
 %%   Name of a test case group.
 %% TestCase = atom()
 %%   Name of a test case.
 %% Reason = term()
 %%   The reason for skipping all groups and test cases.
 %%
 %% Description: Returns the list of groups and test cases that
 %%              are to be executed.
 %%--------------------------------------------------------------------
 all() -&gt; 
     [my_test_case].


 %%--------------------------------------------------------------------
 %% TEST CASES
 %%--------------------------------------------------------------------

 %%--------------------------------------------------------------------
 %% Function: TestCase() -&gt; Info
 %%
 %% Info = [tuple()]
 %%   List of key/value pairs.
 %%
 %% Description: Test case info function - returns list of tuples to set
 %%              properties for the test case.
 %%
 %% Note: This function is only meant to be used to return a list of
 %% values, not perform any other operations.
 %%--------------------------------------------------------------------
 my_test_case() -&gt; 
     [].

 %%--------------------------------------------------------------------
 %% Function: TestCase(Config0) -&gt;
 %%               ok | exit() | {skip,Reason} | {comment,Comment} |
 %%               {save_config,Config1} | {skip_and_save,Reason,Config1}
 %%
 %% Config0 = Config1 = [tuple()]
 %%   A list of key/value pairs, holding the test case configuration.
 %% Reason = term()
 %%   The reason for skipping the test case.
 %% Comment = term()
 %%   A comment about the test case that will be printed in the html log.
 %%
 %% Description: Test case function. (The name of it must be specified in
 %%              the all/0 list or in a test case group for the test case
 %%              to be executed).
 %%--------------------------------------------------------------------
 my_test_case(_Config) -&gt; 
     ok.</code></pre><br/><p><em>Small Common Test Suite</em></p><pre><code class="">
 %%%-------------------------------------------------------------------
 %%% File    : example_SUITE.erl
 %%% Author  : 
 %%% Description : 
 %%%
 %%% Created : 
 %%%-------------------------------------------------------------------
 -module(example_SUITE).

 -compile(export_all).

 -include_lib("common_test/include/ct.hrl").

 %%--------------------------------------------------------------------
 %% Function: suite() -&gt; Info
 %% Info = [tuple()]
 %%--------------------------------------------------------------------
 suite() -&gt;
     [{timetrap,{seconds,30}}].

 %%--------------------------------------------------------------------
 %% Function: init_per_suite(Config0) -&gt;
 %%               Config1 | {skip,Reason} | {skip_and_save,Reason,Config1}
 %% Config0 = Config1 = [tuple()]
 %% Reason = term()
 %%--------------------------------------------------------------------
 init_per_suite(Config) -&gt;
     Config.

 %%--------------------------------------------------------------------
 %% Function: end_per_suite(Config0) -&gt; term() | {save_config,Config1}
 %% Config0 = Config1 = [tuple()]
 %%--------------------------------------------------------------------
 end_per_suite(_Config) -&gt;
     ok.

 %%--------------------------------------------------------------------
 %% Function: init_per_group(GroupName, Config0) -&gt;
 %%               Config1 | {skip,Reason} | {skip_and_save,Reason,Config1}
 %% GroupName = atom()
 %% Config0 = Config1 = [tuple()]
 %% Reason = term()
 %%--------------------------------------------------------------------
 init_per_group(_GroupName, Config) -&gt;
     Config.

 %%--------------------------------------------------------------------
 %% Function: end_per_group(GroupName, Config0) -&gt;
 %%               term() | {save_config,Config1}
 %% GroupName = atom()
 %% Config0 = Config1 = [tuple()]
 %%--------------------------------------------------------------------
 end_per_group(_GroupName, _Config) -&gt;
     ok.

 %%--------------------------------------------------------------------
 %% Function: init_per_testcase(TestCase, Config0) -&gt;
 %%               Config1 | {skip,Reason} | {skip_and_save,Reason,Config1}
 %% TestCase = atom()
 %% Config0 = Config1 = [tuple()]
 %% Reason = term()
 %%--------------------------------------------------------------------
 init_per_testcase(_TestCase, Config) -&gt;
     Config.

 %%--------------------------------------------------------------------
 %% Function: end_per_testcase(TestCase, Config0) -&gt;
 %%               term() | {save_config,Config1} | {fail,Reason}
 %% TestCase = atom()
 %% Config0 = Config1 = [tuple()]
 %% Reason = term()
 %%--------------------------------------------------------------------
 end_per_testcase(_TestCase, _Config) -&gt;
     ok.

 %%--------------------------------------------------------------------
 %% Function: groups() -&gt; [Group]
 %% Group = {GroupName,Properties,GroupsAndTestCases}
 %% GroupName = atom()
 %% Properties = [parallel | sequence | Shuffle | {RepeatType,N}]
 %% GroupsAndTestCases = [Group | {group,GroupName} | TestCase]
 %% TestCase = atom()
 %% Shuffle = shuffle | {shuffle,{integer(),integer(),integer()}}
 %% RepeatType = repeat | repeat_until_all_ok | repeat_until_all_fail |
 %%              repeat_until_any_ok | repeat_until_any_fail
 %% N = integer() | forever
 %%--------------------------------------------------------------------
 groups() -&gt;
     [].

 %%--------------------------------------------------------------------
 %% Function: all() -&gt; GroupsAndTestCases | {skip,Reason}
 %% GroupsAndTestCases = [{group,GroupName} | TestCase]
 %% GroupName = atom()
 %% TestCase = atom()
 %% Reason = term()
 %%--------------------------------------------------------------------
 all() -&gt; 
     [my_test_case].

 %%--------------------------------------------------------------------
 %% Function: TestCase() -&gt; Info
 %% Info = [tuple()]
 %%--------------------------------------------------------------------
 my_test_case() -&gt; 
     [].

 %%--------------------------------------------------------------------
 %% Function: TestCase(Config0) -&gt;
 %%               ok | exit() | {skip,Reason} | {comment,Comment} |
 %%               {save_config,Config1} | {skip_and_save,Reason,Config1}
 %% Config0 = Config1 = [tuple()]
 %% Reason = term()
 %% Comment = term()
 %%--------------------------------------------------------------------
 my_test_case(_Config) -&gt; 
     ok.</code></pre><h3>Using the Common Test Framework</h3><p>The <strong>Common Test</strong> framework provides a high-level
operator interface for testing, providing the following features:</p><ul><li>Automatic compilation of test suites (and help modules)</li><li>Creation of extra HTML pages for improved overview.</li><li>Single-command interface for running all available tests</li><li>Handling of configuration files specifying data related to the System Under Test (SUT) (and any other variable data)</li><li>Mode for running multiple independent test sessions in parallel with central control and configuration</li></ul><h3>Automatic Compilation of Test Suites and Help Modules</h3><p>When <strong>Common Test</strong> starts, it automatically attempts to compile any
suites included in the specified tests. If particular
suites are specified, only those suites are compiled. If a
particular test object directory is specified (meaning all suites
in this directory are to be part of the test), <strong>Common Test</strong> runs
function <strong>make:all/1</strong> in the directory to compile the suites.</p><p>If compilation fails for one or more suites, the compilation errors
are printed to tty and the operator is asked if the test run is to proceed
without the missing suites, or be aborted. If the operator chooses to proceed, 
the tests having missing suites are noted in the HTML log. If <strong>Common Test</strong> is
unable to prompt the user after compilation failure (if <strong>Common Test</strong> does not
control <strong>stdin</strong>), the test run proceeds automatically without the missing
suites. This behavior can however be modified with the
<strong>ct_run</strong> flag <strong>-abort_if_missing_suites</strong>, 
or the <a href="./ct#run_test-1">ct#run_test-1</a> option
<strong>{abort_if_missing_suites,TrueOrFalse}</strong>. If 
<strong>abort_if_missing_suites</strong> is set to <strong>true</strong>, the test run
stops immediately if some suites fail to compile.</p><p>Any help module (that is, regular Erlang module with name not ending with
"_SUITE") that resides in the same test object directory as a suite, 
which is part of the test, is also automatically compiled. A help
module is not mistaken for a test suite (unless it has a "_SUITE" name).
All help modules in a particular test object directory
are compiled, no matter if all or only particular suites in the directory 
are part of the test.</p><p>If test suites or help modules include header files stored in other
locations than the test directory, these include directories can be specified
by using flag <strong>-include</strong> with 
<a href="ct_run">ct_run</a>, 
or option <strong>include</strong> with <strong>ct:run_test/1</strong>.
Also, an include path can be specified with an OS
environment variable, <strong>CT_INCLUDE_PATH</strong>.</p><p><em>Example (bash):</em></p><p><strong>$ export CT_INCLUDE_PATH=~testuser/common_suite_files/include:~testuser/common_lib_files/include</strong></p><p><strong>Common Test</strong> passes all include directories (specified either with flag/option
<strong>include</strong>, or variable <strong>CT_INCLUDE_PATH</strong>
, or both, to the compiler.</p><p>Include directories can also be specified in test specifications,
see <a href="#test_specifications">Test Specifications</a>.</p><p>If the user wants to run all test suites for a test object (or an OTP application)
by specifying only the top directory (for example, with start flag/option <strong>dir</strong>),
<strong>Common Test</strong> primarily looks for test suite modules in a subdirectory named 
<strong>test</strong>. If this subdirectory does not exist, the specified top directory
is assumed to be the test directory, and test suites are read from
there instead.</p><p>To disable the automatic compilation feature, use flag
<strong>-no_auto_compile</strong> with <strong>ct_run</strong>, or
option <strong>{auto_compile,false}</strong> with 
<strong>ct:run_test/1</strong>. With automatic compilation
disabled, the user is responsible for compiling the test suite modules 
(and any help modules) before the test run. If the modules cannot be loaded
from the local file system during startup of <strong>Common Test</strong>, the user must
preload the modules before starting the test. <strong>Common Test</strong> only verifies
that the specified test suites exist (that is, that they are, or can be, loaded).
This is useful, for example, if the test suites are transferred and loaded as 
binaries through RPC from a remote node.</p><a name="ct_run"></a><h3>Running Tests from the OS Command Line</h3><p>The <a href="ct_run">ct_run</a> program can be used 
for running tests from the OS command line, for example, as follows:
</p><ul><li><strong>ct_run -config &lt;configfilenames&gt; -dir &lt;dirs&gt;</strong></li><li><strong>ct_run -config &lt;configfilenames&gt; -suite &lt;suiteswithfullpath&gt;</strong> </li><li><strong>ct_run -userconfig &lt;callbackmodulename&gt; &lt;configfilenames&gt; -suite &lt;suiteswithfullpath&gt;</strong> </li><li><strong>ct_run -config &lt;configfilenames&gt; -suite &lt;suitewithfullpath&gt; -group &lt;groups&gt; -case &lt;casenames&gt;</strong></li></ul><p><em>Examples:</em></p><pre>
 $ ct_run -config $CFGS/sys1.cfg $CFGS/sys2.cfg -dir $SYS1_TEST $SYS2_TEST
 $ ct_run -userconfig ct_config_xml $CFGS/sys1.xml $CFGS/sys2.xml -dir $SYS1_TEST $SYS2_TEST
 $ ct_run -suite $SYS1_TEST/setup_SUITE $SYS2_TEST/config_SUITE
 $ ct_run -suite $SYS1_TEST/setup_SUITE -case start stop
 $ ct_run -suite $SYS1_TEST/setup_SUITE -group installation -case start stop</pre><p>The flags <strong>dir</strong>, <strong>suite</strong>, and <strong>group/case</strong> can be combined.
For example, to run <strong>x_SUITE</strong> and <strong>y_SUITE</strong> 
in directory <strong>testdir</strong>, as follows:</p><pre>
 $ ct_run -dir ./testdir -suite x_SUITE y_SUITE</pre><p>This has the same effect as the following:</p><pre>
 $ ct_run -suite ./testdir/x_SUITE ./testdir/y_SUITE</pre><p>For details, see 
<a href="./run_test_chapter#group_execution">Test Case Group Execution</a>.</p><p>The following flags can also be used with 
<a href="ct_run">ct_run</a>:</p><dl><dt><strong>-help</strong></dt><dd><p>Lists all available start flags.</p></dd><dt><strong>-logdir &lt;dir&gt;</strong></dt><dd><p>Specifies where the HTML log files are to be written.</p></dd><dt><strong>-label &lt;name_of_test_run&gt;</strong></dt><dd><p>Associates the test run with a name that gets printed
in the overview HTML log files.</p></dd><dt><strong>-refresh_logs</strong></dt><dd><p>Refreshes the top-level HTML index files.</p></dd><dt><strong>-vts</strong></dt><dd><p>Starts web-based GUI (described later).</p></dd><dt><strong>-shell</strong></dt><dd><p>Starts interactive shell mode (described later).</p></dd><dt><strong>-step [step_opts]</strong></dt><dd><p>Steps through test cases using the Erlang Debugger (described later).</p></dd><dt><strong>-spec &lt;testspecs&gt;</strong></dt><dd><p>Uses test specification as input (described later).</p></dd><dt><strong>-allow_user_terms</strong></dt><dd><p>Allows user-specific terms in a test specification (described later).</p></dd><dt><strong>-silent_connections [conn_types]</strong></dt><dd><p>, tells <strong>Common Test</strong> to suppress printouts for
specified connections (described later).</p></dd><dt><strong>-stylesheet &lt;css_file&gt;</strong></dt><dd><p>Points out a user HTML style sheet (described later).</p></dd><dt><strong>-cover &lt;cover_cfg_file&gt;</strong></dt><dd><p>To perform code coverage test (see 
<a href="./cover_chapter#cover">Code Coverage Analysis</a>).</p></dd><dt><strong>-cover_stop &lt;bool&gt;</strong></dt><dd><p>To specify if the <strong>cover</strong> tool is to be stopped 
after the test is completed (see
<a href="./cover_chapter#cover_stop">Code Coverage Analysis</a>).</p></dd><dt><strong>-event_handler &lt;event_handlers&gt;</strong></dt><dd><p>To install 
<a href="./event_handler_chapter#event_handling">event handlers</a>.</p></dd><dt><strong>-event_handler_init &lt;event_handlers&gt;</strong></dt><dd><p>To install
<a href="./event_handler_chapter#event_handling">event handlers</a> 
including start arguments.</p></dd><dt><strong>-ct_hooks &lt;ct_hooks&gt;</strong></dt><dd><p>To install
<a href="./ct_hooks_chapter#installing">Common Test Hooks</a> 
including start arguments.</p></dd><dt><strong>-enable_builtin_hooks &lt;bool&gt;</strong></dt><dd><p>To enable or disable
<a href="./ct_hooks_chapter#builtin_cths">Built-in Common Test Hooks</a>. 
Default is <strong>true</strong>.</p></dd><dt><strong>-include</strong></dt><dd><p>Specifies include directories (described earlier).</p></dd><dt><strong>-no_auto_compile</strong></dt><dd><p>Disables the automatic test suite compilation feature (described earlier).</p></dd><dt><strong>-abort_if_missing_suites</strong></dt><dd><p>Aborts the test run if one or more suites fail to compile (described earlier).</p></dd><dt><strong>-multiply_timetraps &lt;n&gt;</strong></dt><dd><p>Extends <a href="./write_test_chapter#timetraps">timetrap time-out</a> values.</p></dd><dt><strong>-scale_timetraps &lt;bool&gt;</strong></dt><dd><p>Enables automatic <a href="./write_test_chapter#timetraps">timetrap time-out</a> scaling.</p></dd><dt><strong>-repeat &lt;n&gt;</strong></dt><dd><p>Tells <strong>Common Test</strong> to repeat the tests <strong>n</strong> times (described later).</p></dd><dt><strong>-duration &lt;time&gt;</strong></dt><dd><p>Tells <strong>Common Test</strong> to repeat the tests for duration of time (described later).</p></dd><dt><strong>-until &lt;stop_time&gt;</strong></dt><dd><p>Tells <strong>Common Test</strong> to repeat the tests until <strong>stop_time</strong> (described later).</p></dd><dt><strong>-force_stop [skip_rest]</strong></dt><dd><p>On time-out, the test run is aborted when the current test job is finished. If <strong>skip_rest</strong> 
is provided, the remaining test cases in the current test job are skipped (described later).</p></dd><dt><strong>-decrypt_key &lt;key&gt;</strong></dt><dd><p>Provides a decryption key for 
<a href="./config_file_chapter#encrypted_config_files">encrypted configuration files</a>.</p></dd><dt><strong>-decrypt_file &lt;key_file&gt;</strong></dt><dd><p>Points out a file containing a decryption key for 
<a href="./config_file_chapter#encrypted_config_files">encrypted configuration files</a>.</p></dd><dt><strong>-basic_html</strong></dt><dd><p>Switches off HTML enhancements that can be incompatible with older browsers.</p></dd><dt><strong>-logopts &lt;opts&gt;</strong></dt><dd><p>Enables modification of the logging behavior, see
<a href="./run_test_chapter#logopts">Log options</a>.</p></dd><dt><strong>-verbosity &lt;levels&gt;</strong></dt><dd><p>Sets <a href="./write_test_chapter#logging">verbosity levels for printouts</a>.</p></dd><dt><strong>-no_esc_chars</strong></dt><dd><p>Disables automatic escaping of special HTML characters.
See the <a href="./write_test_chapter#logging">Logging chapter</a>.</p></dd></dl><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>Directories passed to <strong>Common Test</strong> can have either relative or absolute paths.</p></div><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>Any start flags to the Erlang runtime system (application ERTS) can also be passed as
parameters to <strong>ct_run</strong>. It is, for example, useful to be able to
pass directories to be added to the Erlang code server search path
with flag <strong>-pa</strong> or <strong>-pz</strong>. If you have common help- or library 
modules for test suites (separately compiled), stored in other directories 
than the test suite directories, these <strong>help/lib</strong> directories are preferably
added to the code path this way.</p><p><em>Example:</em></p><p><strong>$ ct_run -dir ./chat_server -logdir ./chat_server/testlogs -pa $PWD/chat_server/ebin</strong></p><p>The absolute path of directory <strong>chat_server/ebin</strong>
is here passed to the code server. This is essential because relative
paths are stored by the code server as relative, and <strong>Common Test</strong> changes 
the current working directory of ERTS during the test run.</p></div><p>The <strong>ct_run</strong> program sets the exit status before shutting down. The following values
are defined:</p><ul><li><strong>0</strong> indicates a successful testrun, that is, without failed or auto-skipped test cases.</li><li><strong>1</strong> indicates that one or more test cases have failed, or have been auto-skipped.</li><li><strong>2</strong> indicates that the test execution has failed because of, for example, compilation errors, or an illegal return value from an information function.</li></ul><p>If auto-skipped test cases do not affect the exit status. The default
behavior can be changed using start flag:</p><pre>
 -exit_status ignore_config</pre><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>Executing <strong>ct_run</strong> without start flags is equal to the command:
<strong>ct_run -dir ./</strong></p></div><p>For more information about the <strong>ct_run</strong> program, see module
<a href="ct_run">ct_run</a> and section
<a href="./install_chapter#general">Installation</a>.
</p><a name="erlang_shell_or_program"></a><h3>Running Tests from the Erlang Shell or from an Erlang Program</h3><p><strong>Common Test</strong> provides an Erlang API for running tests. The main 
(and most flexible) function for specifying and executing tests is
<a href="./ct#run_test-1">ct#run_test-1</a>.
It takes the same start parameters as
<a href="./run_test_chapter#ct_run">run_test_chapter#ct_run</a>,
but the flags are instead specified as options in a list of key-value tuples. 
For example, a test specified with <strong>ct_run</strong> as follows:</p><p><strong>$ ct_run -suite ./my_SUITE -logdir ./results</strong></p><p>is with <a href="./ct#run_test-1">ct#run_test-1</a> specified as:</p><p><strong>1&gt; ct:run_test([{suite,"./my_SUITE"},{logdir,"./results"}]).</strong></p><p>The function returns the test result, represented by the tuple
<strong>{Ok,Failed,{UserSkipped,AutoSkipped}}</strong>, where each element is an
integer. If test execution fails, the function returns the tuple
<strong>{error,Reason}</strong>, where the term <strong>Reason</strong> explains the
failure.</p><p>The default start option <strong>{dir,Cwd}</strong> (to run all suites in the current
working directory) is used if the function is called with an empty
list of options.</p><h3>Releasing the Erlang Shell</h3><p>During execution of tests started with
<a href="./ct#run_test-1">ct#run_test-1</a>,
the Erlang shell process, controlling <strong>stdin</strong>, remains the top-level
process of the <strong>Common Test</strong> system of processes. Consequently,
the Erlang shell is not available for interaction during
the test run. If this is not desirable, for example, because the shell 
is needed for debugging purposes or for interaction with the SUT during test
execution, set start option <strong>release_shell</strong> to
<strong>true</strong> (in the call to <strong>ct:run_test/1</strong> or by
using the corresponding test specification term, described later). This
makes <strong>Common Test</strong> release the shell immediately after the test suite
compilation stage. To accomplish this, a test runner process
is spawned to take control of the test execution. The effect is that
<strong>ct:run_test/1</strong> returns the pid of this process rather than the
test result, which instead is printed to tty at the end of the test run.</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>To use the functions
<a href="./ct#break-1">ct#break-1</a> and
<a href="./ct#continue-0">ct#continue-0</a>,
<strong>release_shell</strong> <em>must</em> be set to <strong>true</strong>.</p></div><p>For details, see
<a href="./ct#run_test-1">ct#run_test-1</a> manual page.</p><a name="group_execution"></a><h3>Test Case Group Execution</h3><p>With the <strong>ct_run</strong> flag, or <strong>ct:run_test/1</strong> option <strong>group</strong>,
one or more test case groups can be specified, optionally in combination
with specific test cases. The syntax for specifying groups on the command line
is as follows:</p><pre>
 $ ct_run -group &lt;group_names_or_paths&gt; [-case &lt;cases&gt;]</pre><p>The syntax in the Erlang shell is as follows:</p><pre>
 1&gt; ct:run_test([{group,GroupsNamesOrPaths}, {case,Cases}]).</pre><p>Parameter <strong>group_names_or_paths</strong> specifies one
or more group names and/or one or more group paths. At startup,
<strong>Common Test</strong> searches for matching groups in the group definitions
tree (that is, the list returned from <strong>Suite:groups/0</strong>; for details, see section
<a href="./write_test_chapter#test_case_groups">Test Case Groups</a>.
</p><p>Given a group name, say <strong>g</strong>, <strong>Common Test</strong> searches for all paths
leading to <strong>g</strong>. By path is meant a sequence of nested groups,
which must be followed to get from the top-level
group to <strong>g</strong>. To execute the test cases in group <strong>g</strong>, 
<strong>Common Test</strong> must call the <strong>init_per_group/2</strong> function for 
each group in the path to <strong>g</strong>, and all corresponding <strong>end_per_group/2</strong>
functions afterwards. This is because the configuration
of a test case in <strong>g</strong> (and its <strong>Config</strong> input data) depends on
<strong>init_per_testcase(TestCase, Config)</strong> and its return value, which
in turn depends on <strong>init_per_group(g, Config)</strong> and its return value,
which in turn depends on <strong>init_per_group/2</strong> of the group above
<strong>g</strong>, and so on, all the way up to the top-level group.</p><p>This means that if there is more than one way to locate a group 
(and its test cases) in a path, the result of the group search operation 
is a number of tests, all of which are to be performed.
<strong>Common Test</strong> interprets a group specification that consists of a
single name as follows:</p><p>"Search and find all paths in the group definitions tree that lead
to the specified group and, for each path, create a test that does the following, 
in order:</p><ul><li>Executes all configuration functions in the path to the specified group.</li><li>Executes all, or all matching, test cases in this group.</li><li>Executes all, or all matching, test cases in all subgroups of the group."</li></ul><p>The user can specify a specific group path with
parameter <strong>group_names_or_paths</strong>. With this type of specification
execution of unwanted groups (in otherwise matching paths),
and/or the execution of subgroups can be avoided. The command line syntax of the 
group path is a list of group names in the path, for example:
</p><p><strong>$ ct_run -suite "./x_SUITE" -group [g1,g3,g4] -case tc1 tc5</strong></p><p>The syntax in the Erlang shell is as follows (requires a list within the groups list):</p><p><strong>1&gt; ct:run_test([{suite,"./x_SUITE"}, {group,[[g1,g3,g4]]}, {testcase,[tc1,tc5]}]).</strong></p><p>The last group in the specified path is the terminating group in
the test, that is, no subgroups following this group are executed. In the
previous example, <strong>g4</strong> is the terminating group. Hence, <strong>Common Test</strong>
executes a test that calls all <strong>init</strong> configuration functions in the path to
<strong>g4</strong>, that is, <strong>g1..g3..g4</strong>. It then calls test cases <strong>tc1</strong>
and <strong>tc5</strong> in <strong>g4</strong>, and finally all <strong>end</strong> configuration functions 
in order <strong>g4..g3..g1</strong>.</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>The group path specification does not necessarily
have to include <em>all</em> groups in the path to the terminating group.
<strong>Common Test</strong> searches for all matching paths if an incomplete 
group path is specified.</p></div><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>Group names and group paths can be combined with parameter
<strong>group_names_or_paths</strong>. Each element is treated as an individual specification 
in combination with parameter <strong>cases</strong>.
The following examples illustrates this.</p></div><p><em>Examples:</em></p><pre>
 -module(x_SUITE).
 ...
 %% The group definitions:      
 groups() -&gt;
   [{top1,[],[tc11,tc12,
	      {sub11,[],[tc12,tc13]},
	      {sub12,[],[tc14,tc15,
			 {sub121,[],[tc12,tc16]}]}]},

    {top2,[],[{group,sub21},{group,sub22}]},
    {sub21,[],[tc21,{group,sub2X2}]},
    {sub22,[],[{group,sub221},tc21,tc22,{group,sub2X2}]},
    {sub221,[],[tc21,tc23]},
    {sub2X2,[],[tc21,tc24]}].</pre><p>The following executes two tests, one for all cases and all subgroups 
under <strong>top1</strong>, and one for all under <strong>top2</strong>:</p><pre>
 $ ct_run -suite "x_SUITE" -group all
 1&gt; ct:run_test([{suite,"x_SUITE"}, {group,all}]).</pre><p>Using <strong>-group top1 top2</strong>, or <strong>{group,[top1,top2]}</strong> gives the same result.</p><p>The following executes one test for all cases and subgroups under <strong>top1</strong>:</p><pre>
 $ ct_run -suite "x_SUITE" -group top1
 1&gt; ct:run_test([{suite,"x_SUITE"}, {group,[top1]}]).</pre><p>The following runs a test executing <strong>tc12</strong> in <strong>top1</strong> and any subgroup
under <strong>top1</strong> where it can be found (<strong>sub11</strong> and <strong>sub121</strong>):</p><pre>
 $ ct_run -suite "x_SUITE" -group top1 -case tc12
 1&gt; ct:run_test([{suite,"x_SUITE"}, {group,[top1]}, {testcase,[tc12]}]).</pre><p>The following executes <strong>tc12</strong> <em>only</em> in group <strong>top1</strong>:</p><pre>
 $ ct_run -suite "x_SUITE" -group [top1] -case tc12
 1&gt; ct:run_test([{suite,"x_SUITE"}, {group,[[top1]]}, {testcase,[tc12]}]).</pre><p>The following searches <strong>top1</strong> and all its subgroups for <strong>tc16</strong> resulting
in that this test case executes in group <strong>sub121</strong>:</p><pre>
 $ ct_run -suite "x_SUITE" -group top1 -case tc16
 1&gt; ct:run_test([{suite,"x_SUITE"}, {group,[top1]}, {testcase,[tc16]}]).</pre><p>Using the specific path <strong>-group [sub121]</strong> or <strong>{group,[[sub121]]}</strong> gives
the same result in this example.</p><p>The following executes two tests, one including all cases and subgroups under
<strong>sub12</strong>, and one with <em>only</em> the test cases in <strong>sub12</strong>:</p><pre>
 $ ct_run -suite "x_SUITE" -group sub12 [sub12]
 1&gt; ct:run_test([{suite,"x_SUITE"}, {group,[sub12,[sub12]]}]).</pre><p>In the following example, <strong>Common Test</strong> finds and executes two tests, 
one for the path from <strong>top2</strong> to <strong>sub2X2</strong> through <strong>sub21</strong>, 
and one from <strong>top2</strong> to <strong>sub2X2</strong> through <strong>sub22</strong>:</p><pre>
 $ ct_run -suite "x_SUITE" -group sub2X2
 1&gt; ct:run_test([{suite,"x_SUITE"}, {group,[sub2X2]}]).</pre><p>In the following example, by specifying the unique path <strong>top2 -&gt; sub21 -&gt; sub2X2</strong>, 
only one test is executed. The second possible path, from <strong>top2</strong> to <strong>sub2X2</strong> 
(from the former example) is discarded:</p><pre>
 $ ct_run -suite "x_SUITE" -group [sub21,sub2X2]
 1&gt; ct:run_test([{suite,"x_SUITE"}, {group,[[sub21,sub2X2]]}]).</pre><p>The following executes only the test cases for <strong>sub22</strong> and in reverse order 
compared to the group definition:</p><pre>
 $ ct_run -suite "x_SUITE" -group [sub22] -case tc22 tc21
 1&gt; ct:run_test([{suite,"x_SUITE"}, {group,[[sub22]]}, {testcase,[tc22,tc21]}]).</pre><p>If a test case belonging to a group (according to the group definition) is executed
without a group specification, that is, simply by
(using the command line):</p><p><strong>$ ct_run -suite "my_SUITE" -case my_tc</strong></p><p>or (using the Erlang shell):</p><p><strong>1&gt; ct:run_test([{suite,"my_SUITE"}, {testcase,my_tc}]).</strong></p><p>then <strong>Common Test</strong> ignores the group definition and executes the test case 
in the scope of the test suite only (no group configuration functions are called).</p><p>The group specification feature, as presented in this section, can also
be used in <a href="./run_test_chapter#test_specifications">Test Specifications</a> (with some extra features added).</p><h3>Running the Interactive Shell Mode</h3><p>You can start <strong>Common Test</strong> in an interactive shell mode where no
automatic testing is performed. Instead, <strong>Common Test</strong>
starts its utility processes, installs configuration data (if any),
and waits for the user to call functions (typically test case support
functions) from the Erlang shell.</p><p>The shell mode is useful, for example, for debugging test suites, analyzing
and debugging the SUT during "simulated" test case execution, and 
trying out various operations during test suite development.</p><p>To start the interactive shell mode, start an Erlang shell 
manually and call <a href="./ct#install-1">ct#install-1</a> 
to install any configuration data you might need (use <strong>[]</strong> as argument otherwise). 
Then call <a href="./ct#start_interactive-0">ct#start_interactive-0</a> 
to start <strong>Common Test</strong>.</p><p>If you use the <strong>ct_run</strong> program, you can start 
the Erlang shell and <strong>Common Test</strong> in one go by using the flag <strong>-shell</strong> and, 
optionally, flag <strong>-config</strong> and/or <strong>-userconfig</strong>.</p><p><em>Examples:</em></p><ul><li><strong>ct_run -shell</strong></li><li><strong>ct_run -shell -config cfg/db.cfg</strong></li><li><strong>ct_run -shell -userconfig db_login testuser x523qZ</strong></li></ul><p>If no configuration file is specified with command <strong>ct_run</strong>,
a warning is displayed. If <strong>Common Test</strong> has been run from the same
directory earlier, the same configuration file(s) are used again. If <strong>Common Test</strong> 
has not been run from this directory before, no configuration files are available.</p><p>If any functions using "required configuration data" (for example, functions 
<strong>ct_telnet</strong> or <strong>ct_ftp</strong>) are to be called from the Erlang shell, first require 
configuration data with <a href="./ct#require-1">ct#require-1</a>. This is equivalent to a <strong>require</strong> statement 
in the <a href="./write_test_chapter#suite">Test Suite Information Function</a> 
or in the <a href="./write_test_chapter#info_function">Test Case Information Function</a>.</p><p><em>Example:</em></p><pre> 
 1&gt; ct:require(unix_telnet, unix).
 ok
 2&gt; ct_telnet:open(unix_telnet).
 {ok,&lt;0.105.0&gt;}
 4&gt; ct_telnet:cmd(unix_telnet, "ls .").
 {ok,["ls .","file1  ...",...]}</pre><p>Everything that <strong>Common Test</strong> normally prints in the test case logs,
are in the interactive mode written to a log named <strong>ctlog.html</strong> 
in directory  <strong>ct_run.&lt;timestamp&gt;</strong>. A link to this 
file is available in the file named <strong>last_interactive.html</strong> in the 
directory from which you execute <strong>ct_run</strong>. Specifying a different
root directory for the logs than the current working directory
is not supported.</p><p>If you wish to exit the interactive mode (for example, to start an automated 
test run with <a href="./ct#run_test-1">ct#run_test-1</a>), 
call function
<a href="./ct#stop_interactive-0">ct#stop_interactive-0</a>. 
This shuts down the running <strong>ct</strong> application. Associations between
configuration names and data created with <strong>require</strong> are 
consequently deleted. Function
<a href="./ct#start_interactive-0">ct#start_interactive-0</a> 
takes you back into interactive mode, but the previous state is not restored.</p><h3>Step-by-Step Execution of Test Cases with the Erlang Debugger</h3><p>Using <strong>ct_run -step [opts]</strong>, or by passing option <strong>{step,Opts}</strong> 
to <a href="./ct#run_test-1">ct#run_test-1</a>, 
the following is possible:</p><ul><li>Get the Erlang Debugger started automatically.</li><li>Use its graphical interface to investigate the state of the current test case.</li><li>Execute the test case step-by-step and/or set execution breakpoints.</li></ul><p>If no extra options are specified with flag/option <strong>step</strong>,
breakpoints are set automatically on the test cases that
are to be executed by <strong>Common Test</strong>, and those functions only. If
step option <strong>config</strong> is specified, breakpoints are also initially 
set on the configuration functions in the suite, that is,
<strong>init_per_suite/1</strong>, <strong>end_per_suite/1</strong>,
<strong>init_per_group/2</strong>, <strong>end_per_group/2</strong>,
<strong>init_per_testcase/2</strong> and <strong>end_per_testcase/2</strong>.</p><p><strong>Common Test</strong> enables the Debugger auto-attach feature, which means
that for every new interpreted test case function that starts to execute, 
a new trace window automatically pops up (as each test 
case executes on a dedicated Erlang process). Whenever a new test case starts,
<strong>Common Test</strong> attempts to close the inactive trace window of the previous 
test case. However, if you prefer <strong>Common Test</strong> to leave inactive trace 
windows, use option <strong>keep_inactive</strong>.</p><p>The step functionality can be used together with flag/option <strong>suite</strong> and 
<strong>suite</strong> + <strong>case/testcase</strong>, but not together with <strong>dir</strong>.</p><a name="test_specifications"></a><h3>Test Specifications</h3><h3>General Description</h3><p>The most flexible way to specify what to test, is to use a
test specification, which is a sequence of
Erlang terms. The terms are normally declared in one or more text files
(see <a href="./ct#run_test-1">ct#run_test-1</a>), but
can also be passed to <strong>Common Test</strong> on the form of a list (see
<a href="./ct#run_testspec-1">ct#run_testspec-1</a>).
There are two general types of terms: configuration terms and test
specification terms.</p><p>With configuration terms it is, for example, possible to do the following:</p><ul><li>Label the test run (similar to <strong>ct_run -label</strong>).</li><li>Evaluate any expressions before starting the test.</li><li>Import configuration data (similar to <strong>ct_run -config/-userconfig</strong>).</li><li>Specify the top-level HTML log directory (similar to <strong>ct_run -logdir</strong>).</li><li>Enable code coverage analysis (similar to <strong>ct_run -cover</strong>).</li><li>Install <strong>Common Test Hooks</strong> (similar to <strong>ct_run -ch_hooks</strong>).</li><li>Install <strong>event_handler</strong> plugins (similar to <strong>ct_run -event_handler</strong>).</li><li>Specify include directories to be passed to the compiler for  automatic compilation (similar to <strong>ct_run -include</strong>).</li><li>Disable the auto-compilation feature (similar to <strong>ct_run -no_auto_compile</strong>).</li><li>Set verbosity levels (similar to <strong>ct_run -verbosity</strong>).</li></ul><p>Configuration terms can be combined with <strong>ct_run</strong> start flags
or <strong>ct:run_test/1</strong> options. The result is, for some flags/options
and terms, that the values are merged (for example, configuration files,
include directories, verbosity levels, and silent connections) and for
others that the start flags/options override the test specification
terms (for example, log directory, label, style sheet, and auto-compilation).</p><p>With test specification terms, it is possible to state exactly
which tests to run and in which order. A test term specifies
either one or more suites, one or more test case groups (possibly nested),
or one or more test cases in a group (or in multiple groups) or in a suite.</p><p>Any number of test terms can be declared in sequence.
<strong>Common Test</strong> compiles by default the terms into one or more tests 
to be performed in one resulting test run. A term that
specifies a set of test cases "swallows" one that only
specifies a subset of these cases. For example, the result of merging
one term specifying that all cases in suite S are to be
executed, with another term specifying only test case X and Y in
S, is a test of all cases in S. However, if a term specifying
test case X and Y in S is merged with a term specifying case Z
in S, the result is a test of X, Y, and Z in S. To disable this
behavior, that is, to instead perform each test sequentially in a 
"script-like" manner, set term <strong>merge_tests</strong> to <strong>false</strong> 
in the test specification.</p><p>A test term can also specify one or more test suites, groups,
or test cases to be skipped. Skipped suites, groups, and cases
are not executed and show up in the HTML log files as <strong>SKIPPED</strong>.</p><h3>Using Multiple Test Specification Files</h3><p>When multiple test specification files are specified at startup (either
with <strong>ct_run -spec file1 file2 ...</strong> or 
<strong>ct:run_test([{spec, [File1,File2,...]}])</strong>), 
<strong>Common Test</strong> either executes one test run per specification file, 
or joins the files and performs all tests within one single test run. 
The first behavior is the default one. The latter requires that start
flag/option <strong>join_specs</strong> is provided, for example,
<strong>run_test -spec ./my_tests1.ts ./my_tests2.ts -join_specs</strong>.</p><p>Joining a number of specifications, or running them separately, can
also be accomplished with (and can be combined with) test specification
file inclusion.</p><h3>Test Specification File Inclusion</h3><p>With the term <strong>specs</strong>, a test specification can include 
other specifications. An included specification can either be joined 
with the source specification or used to produce a separate test run 
(as with start flag/option <strong>join_specs</strong> above).</p><p><em>Example:</em></p><pre>
 %% In specification file "a.spec"
 {specs, join, ["b.spec", "c.spec"]}.
 {specs, separate, ["d.spec", "e.spec"]}.
 %% Config and test terms follow
 ...</pre><p>In this example, the test terms defined in files "b.spec" and "c.spec"
are joined with the terms in source specification "a.spec"
(if any). The inclusion of specifications "d.spec" and
"e.spec" results in two separate, and independent, test runs
(one for each included specification).</p><p>Option <strong>join</strong> does not imply that the test terms
are merged, only that all tests are executed in one single test run.</p><p>Joined specifications share common configuration settings, such as
the list of <strong>config</strong> files or <strong>include</strong> directories.
For configurations that cannot be combined, such as settings for <strong>logdir</strong>
or <strong>verbosity</strong>, it is up to the user to ensure there are no clashes
when the test specifications are joined. Specifications included with
option <strong>separate</strong> do not share configuration settings with the
source specification. This is useful, for example, if there are clashing
configuration settings in included specifications, making it them impossible
to join.</p><p>If <strong>{merge_tests,true}</strong> is set in the source specification
(which is the default setting), terms in joined specifications are
merged with terms in the source specification (according to the
description of <strong>merge_tests</strong> earlier).</p><p>Notice that it is always the <strong>merge_tests</strong> setting in the source
specification that is used when joined with other specifications.
Say, for example, that a source specification A, with tests TA1 and TA2, has
<strong>{merge_tests,false}</strong> set, and that it includes another specification,
B, with tests TB1 and TB2, that has <strong>{merge_tests,true}</strong> set.
The result is that the test series <strong>TA1,TA2,merge(TB1,TB2)</strong>
is executed. The opposite <strong>merge_tests</strong> settings would result in
the test series <strong>merge(merge(TA1,TA2),TB1,TB2)</strong>.</p><p>The term <strong>specs</strong> can be used to nest specifications,
that is, have one specification include other specifications, which in turn
include others, and so no</p><h3>Test Case Groups</h3><p>When a test case group is specified, the resulting test
executes function <strong>init_per_group</strong>, followed by all test
cases and subgroups (including their configuration functions), and
finally function <strong>end_per_group</strong>. Also, if particular
test cases in a group are specified, <strong>init_per_group</strong>
and <strong>end_per_group</strong>, for the group in question, are
called. If a group defined (in <strong>Suite:group/0</strong>) as
a subgroup of another group, is specified (or if particular test
cases of a subgroup are), <strong>Common Test</strong> calls the configuration
functions for the top-level groups and for the subgroup
in question (making it possible to pass configuration data all
the way from <strong>init_per_suite</strong> down to the test cases in the
subgroup).</p><p>The test specification uses the same mechanism for specifying
test case groups through names and paths, as explained in section
<a href="./run_test_chapter#group_execution">Test Case Group Execution</a>,
with the addition of element <strong>GroupSpec</strong>.</p><p>Element <strong>GroupSpec</strong> makes it possible to specify
group execution properties that overrides those in the
group definition (that is, in <strong>groups/0</strong>). Execution properties for
subgroups might be overridden as well. This feature makes it possible to
change properties of groups at the time of execution,
without having to edit the test suite. The same feature is available for 
<strong>group</strong> elements in the <strong>Suite:all/0</strong> list. For details and examples,
see section <a href="./write_test_chapter#test_case_groups"> Test Case Groups</a>.</p><h3>Test Specification Syntax</h3><p>Test specifications can be used to run tests both in a single 
test host environment and in a distributed <strong>Common Test</strong> environment 
(Large Scale Testing). The node parameters in term <strong>init</strong> are only
relevant in the latter (see section
<a href="./ct_master_chapter#test_specifications">Test Specifications</a> 
in Large Scale Testing). For details about the various terms, see the 
corresponding sections in the User's Guide, for example, the following:
</p><ul><li>The <a href="./run_test_chapter#ct_run"> program</a> for an overview of available start flags (as most flags have a corresponding configuration term)</li><li><a href="./write_test_chapter#logging">Logging</a> (for terms <strong>verbosity</strong>, <strong>stylesheet</strong>, <strong>basic_html</strong> and <strong>esc_chars</strong>)</li><li><a href="./config_file_chapter#top">External Configuration Data</a> (for terms <strong>config</strong> and <strong>userconfig</strong>)</li><li><a href="./event_handler_chapter#event_handling">Event Handling</a> (for the <strong>event_handler</strong> term)</li><li><a href="./ct_hooks_chapter#installing">Common Test Hooks</a> (for term <strong>ct_hooks</strong>)</li></ul><p><em>Configuration terms:</em></p><pre>
 {merge_tests, Bool}.

 {define, Constant, Value}.

 {specs, InclSpecsOption, TestSpecs}.

 {node, NodeAlias, Node}.

 {init, InitOptions}.
 {init, [NodeAlias], InitOptions}.

 {label, Label}.
 {label, NodeRefs, Label}.

 {verbosity, VerbosityLevels}.
 {verbosity, NodeRefs, VerbosityLevels}.

 {stylesheet, CSSFile}.
 {stylesheet, NodeRefs, CSSFile}.

 {silent_connections, ConnTypes}.
 {silent_connections, NodeRefs, ConnTypes}.

 {multiply_timetraps, N}.
 {multiply_timetraps, NodeRefs, N}.

 {scale_timetraps, Bool}.
 {scale_timetraps, NodeRefs, Bool}.

 {cover, CoverSpecFile}.
 {cover, NodeRefs, CoverSpecFile}.

 {cover_stop, Bool}.
 {cover_stop, NodeRefs, Bool}.

 {include, IncludeDirs}.
 {include, NodeRefs, IncludeDirs}.

 {auto_compile, Bool},
 {auto_compile, NodeRefs, Bool},

 {abort_if_missing_suites, Bool},
 {abort_if_missing_suites, NodeRefs, Bool},

 {config, ConfigFiles}.
 {config, ConfigDir, ConfigBaseNames}.
 {config, NodeRefs, ConfigFiles}.
 {config, NodeRefs, ConfigDir, ConfigBaseNames}.

 {userconfig, {CallbackModule, ConfigStrings}}.
 {userconfig, NodeRefs, {CallbackModule, ConfigStrings}}.

 {logdir, LogDir}.                                        
 {logdir, NodeRefs, LogDir}.

 {logopts, LogOpts}.
 {logopts, NodeRefs, LogOpts}.

 {create_priv_dir, PrivDirOption}.
 {create_priv_dir, NodeRefs, PrivDirOption}.

 {event_handler, EventHandlers}.
 {event_handler, NodeRefs, EventHandlers}.
 {event_handler, EventHandlers, InitArgs}.
 {event_handler, NodeRefs, EventHandlers, InitArgs}.

 {ct_hooks, CTHModules}.
 {ct_hooks, NodeRefs, CTHModules}.

 {enable_builtin_hooks, Bool}.

 {basic_html, Bool}.
 {basic_html, NodeRefs, Bool}.

 {esc_chars, Bool}.
 {esc_chars, NodeRefs, Bool}.

 {release_shell, Bool}.</pre><p><em>Test terms:</em></p><pre>
 {suites, Dir, Suites}.                                
 {suites, NodeRefs, Dir, Suites}.

 {groups, Dir, Suite, Groups}.
 {groups, NodeRefs, Dir, Suite, Groups}.

 {groups, Dir, Suite, Groups, {cases,Cases}}.
 {groups, NodeRefs, Dir, Suite, Groups, {cases,Cases}}.

 {cases, Dir, Suite, Cases}.                           
 {cases, NodeRefs, Dir, Suite, Cases}.

 {skip_suites, Dir, Suites, Comment}.
 {skip_suites, NodeRefs, Dir, Suites, Comment}.

 {skip_groups, Dir, Suite, GroupNames, Comment}.
 {skip_groups, NodeRefs, Dir, Suite, GroupNames, Comment}.

 {skip_cases, Dir, Suite, Cases, Comment}.
 {skip_cases, NodeRefs, Dir, Suite, Cases, Comment}.</pre><a name="types"></a><p><em>Types:</em></p><pre>
 Bool            = true | false
 Constant        = atom()
 Value           = term()
 InclSpecsOption = join | separate
 TestSpecs       = string() | [string()]
 NodeAlias       = atom()
 Node            = node()
 NodeRef         = NodeAlias | Node | master
 NodeRefs        = all_nodes | [NodeRef] | NodeRef
 InitOptions     = term()
 Label           = atom() | string()
 VerbosityLevels = integer() | [{Category,integer()}]
 Category        = atom()
 CSSFile         = string()
 ConnTypes       = all | [atom()]
 N               = integer()
 CoverSpecFile   = string()
 IncludeDirs     = string() | [string()]
 ConfigFiles     = string() | [string()]
 ConfigDir       = string()
 ConfigBaseNames = string() | [string()]
 CallbackModule  = atom()
 ConfigStrings   = string() | [string()]
 LogDir          = string()
 LogOpts         = [term()]
 PrivDirOption   = auto_per_run | auto_per_tc | manual_per_tc
 EventHandlers   = atom() | [atom()]
 InitArgs        = [term()]
 CTHModules      = [CTHModule |
		    {CTHModule, CTHInitArgs} |
		    {CTHModule, CTHInitArgs, CTHPriority}]
 CTHModule       = atom()
 CTHInitArgs     = term()
 Dir             = string()
 Suites          = atom() | [atom()] | all
 Suite           = atom()
 Groups          = GroupPath | [GroupPath] | GroupSpec | [GroupSpec] | all
 GroupPath       = [GroupName]
 GroupSpec       = GroupName | {GroupName,Properties} | {GroupName,Properties,GroupSpec}
 GroupName       = atom()
 GroupNames      = GroupName | [GroupName]
 Cases           = atom() | [atom()] | all
 Comment         = string() | ""</pre><p>The difference between the <strong>config</strong> terms above is that with
<strong>ConfigDir</strong>, <strong>ConfigBaseNames</strong> is a list of base names,
that is, without directory paths. <strong>ConfigFiles</strong> must be full names,
including paths. For example, the following two terms have the same meaning:</p><pre>
 {config, ["/home/testuser/tests/config/nodeA.cfg",
           "/home/testuser/tests/config/nodeB.cfg"]}.

 {config, "/home/testuser/tests/config", ["nodeA.cfg","nodeB.cfg"]}.</pre><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>Any relative paths, specified in the test specification, are
relative to the directory containing the test specification file if
<strong>ct_run -spec TestSpecFile ...</strong> or
<strong>ct:run:test([{spec,TestSpecFile},...])</strong>
executes the test.</p><p>The path is relative to the top-level log directory if
<strong>ct:run:testspec(TestSpec)</strong> executes the test.</p></div><h3>Constants</h3><p>The term <strong>define</strong> introduces a constant that is used to
replace the name <strong>Constant</strong> with <strong>Value</strong>, wherever it is found in
the test specification. This replacement occurs during an initial iteration
through the test specification. Constants can be used anywhere in the test
specification, for example, in any lists and tuples, and even in strings
and inside the value part of other constant definitions. A constant can
also be part of a node name, but that is the only place where a constant
can be part of an atom.</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>For the sake of readability, the name of the constant must always
begin with an uppercase letter, or a <strong>$</strong>, <strong>?</strong>, or <strong>_</strong>.
This means that it must always be single quoted (as the constant name is 
an atom, not text).</p></div><p>The main benefit of constants is that they can be used to reduce the size
(and avoid repetition) of long strings, such as file paths.</p><p><em>Examples:</em></p><pre>
 %% 1a. no constant
 {config, "/home/testuser/tests/config", ["nodeA.cfg","nodeB.cfg"]}.
 {suites, "/home/testuser/tests/suites", all}.

 %% 1b. with constant
 {define, 'TESTDIR', "/home/testuser/tests"}.
 {config, "'TESTDIR'/config", ["nodeA.cfg","nodeB.cfg"]}.
 {suites, "'TESTDIR'/suites", all}.

 %% 2a. no constants
 {config, [testnode@host1, testnode@host2], "../config", ["nodeA.cfg","nodeB.cfg"]}.
 {suites, [testnode@host1, testnode@host2], "../suites", [x_SUITE, y_SUITE]}.

 %% 2b. with constants
 {define, 'NODE', testnode}.
 {define, 'NODES', ['NODE'@host1, 'NODE'@host2]}.
 {config, 'NODES', "../config", ["nodeA.cfg","nodeB.cfg"]}.
 {suites, 'NODES', "../suites", [x_SUITE, y_SUITE]}.</pre><p>Constants make the test specification term <strong>alias</strong>, in previous
versions of <strong>Common Test</strong>, redundant. This term is deprecated but
remains supported in upcoming <strong>Common Test</strong> releases. Replacing <strong>alias</strong>
terms with <strong>define</strong> is strongly recommended though. An example
of such replacement follows:</p><pre>
 %% using the old alias term
 {config, "/home/testuser/tests/config/nodeA.cfg"}.
 {alias, suite_dir, "/home/testuser/tests/suites"}.
 {groups, suite_dir, x_SUITE, group1}.

 %% replacing with constants
 {define, 'TestDir', "/home/testuser/tests"}.
 {define, 'CfgDir', "'TestDir'/config"}.
 {define, 'SuiteDir', "'TestDir'/suites"}.
 {config, 'CfgDir', "nodeA.cfg"}.
 {groups, 'SuiteDir', x_SUITE, group1}.</pre><p>Constants can well replace term <strong>node</strong> also, but
this still has a declarative value, mainly when used in combination
with <strong>NodeRefs == all_nodes</strong> 
(see <a href="#types">Types</a>).</p><h3>Example</h3><p>Here follows a simple test specification example:</p><pre>
 {define, 'Top', "/home/test"}.
 {define, 'T1', "'Top'/t1"}.
 {define, 'T2', "'Top'/t2"}.
 {define, 'T3', "'Top'/t3"}.
 {define, 'CfgFile', "config.cfg"}.

 {logdir, "'Top'/logs"}.

 {config, ["'T1'/'CfgFile'", "'T2'/'CfgFile'", "'T3'/'CfgFile'"]}.

 {suites, 'T1', all}.
 {skip_suites, 'T1', [t1B_SUITE,t1D_SUITE], "Not implemented"}.
 {skip_cases, 'T1', t1A_SUITE, [test3,test4], "Irrelevant"}.
 {skip_cases, 'T1', t1C_SUITE, [test1], "Ignore"}.

 {suites, 'T2', [t2B_SUITE,t2C_SUITE]}.
 {cases, 'T2', t2A_SUITE, [test4,test1,test7]}.

 {skip_suites, 'T3', all, "Not implemented"}.</pre><p>The example specifies the following:</p><ul><li>The specified <strong>logdir</strong> directory is used for storing  the HTML log files (in subdirectories tagged with node name,  date, and time).</li><li>The variables in the specified test system configuration files are imported for the test.</li><li>The first test to run includes all suites for system <strong>t1</strong>.  Suites <strong>t1B</strong> and <strong>t1D</strong> are excluded from the test. Test cases  <strong>test3</strong> and <strong>test4</strong> in <strong>t1A</strong> and <strong>test1</strong> case in <strong>t1C</strong>  are also excluded from the test.</li><li>The second test to run is for system <strong>t2</strong>. The included suites are <strong>t2B</strong> and <strong>t2C</strong>. Test cases <strong>test4</strong>, <strong>test1</strong>, and <strong>test7</strong> in suite <strong>t2A</strong> are also included. The test cases are executed in the specified order.</li><li>The last test to run is for system <strong>t3</strong>. Here, all suites are skipped and this is explicitly noted in the log files.</li></ul><h3>The init Term</h3><p>With term <strong>init</strong> it is possible to specify initialization options
for nodes defined in the test specification. There are options
to start the node and to evaluate any function on the node.
For details, see section <a href="./ct_master_chapter#ct_slave">Automatic Startup of Test Target Nodes</a> in section Using Common Test for Large Scale Testing.</p><h3>User-Specific Terms</h3><p>The user can provide a test specification including (for <strong>Common Test</strong>) 
unrecognizable terms. If this is desired, use flag <strong>-allow_user_terms</strong> 
when starting tests with <strong>ct_run</strong>. This forces <strong>Common Test</strong> to ignore 
unrecognizable terms. In this mode, <strong>Common Test</strong> is not able to check the 
specification for errors as efficiently as if the scanner runs in default mode. 
If <a href="./ct#run_test-1">ct#run_test-1</a> is used
for starting the tests, the relaxed scanner mode is enabled by tuple
<strong>{allow_user_terms,true}</strong>.</p><h3>Reading Test Specification Terms</h3><p>Terms in the current test specification
(that is, the specification that has been used to configure and run the current test)
can be looked up.
The function <a href="./ct#get_testspec_terms-0">ct#get_testspec_terms-0</a> 
returns a list of all test specification terms (both configuration terms and test terms), 
and <strong>get_testspec_terms(Tags)</strong> returns the term (or a list of terms) matching the 
tag (or tags) in <strong>Tags</strong>.</p><p>For example, in the test specification:</p><pre>
 ...
 {label, my_server_smoke_test}.
 {config, "../../my_server_setup.cfg"}.
 {config, "../../my_server_interface.cfg"}.
 ...</pre><p>And in, for example, a test suite or a <strong>Common Test Hook</strong> function:</p><pre>
 ...
 [{label,[{_Node,TestType}]}, {config,CfgFiles}] =
     ct:get_testspec_terms([label,config]),

 [verify_my_server_cfg(TestType, CfgFile) || {Node,CfgFile} &lt;- CfgFiles,
					     Node == node()];
 ...</pre><h3>Running Tests from the Web-Based GUI</h3><p>The web-based GUI, Virtual Test Server (VTS), is started with the
<a href="./run_test_chapter#ct_run">run_test_chapter#ct_run</a>
program. From the GUI, you can load configuration files and select
directories, suites, and cases to run. You can also state the
configuration files, directories, suites, and cases on the command line
when starting the web-based GUI.
</p><p><em>Examples:</em></p><ul><li><strong>ct_run -vts</strong></li><li><strong>ct_run -vts -config &lt;configfilename&gt;</strong></li><li><strong>ct_run -vts -config &lt;configfilename&gt; -suite &lt;suitewithfullpath&gt; -case &lt;casename&gt;</strong></li></ul><p>From the GUI you can run tests and view the result and the logs.
</p><p><strong>ct_run -vts</strong> tries to open the <strong>Common Test</strong> start
page in an existing web browser window, or start the browser if it is
not running. Which browser to start can be specified with
the browser start command option:</p><p><strong>ct_run -vts -browser &lt;browser_start_cmd&gt;</strong></p><p><em>Example:</em></p><p><strong>$ ct_run -vts -browser 'firefox&amp;'</strong></p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>The browser must run as a separate OS process, otherwise VTS hangs.</p></div><p>If no specific browser start command is specified, Firefox is
the default browser on Unix platforms, and Internet Explorer on Windows.
If <strong>Common Test</strong> fails to start a browser automatically, or <strong>none</strong> is
specified as the value for <strong>-browser</strong> (that is, <strong>-browser none</strong>), start your
favourite browser manually and type the URL that <strong>Common Test</strong>
displays in the shell.</p><a name="log_files"></a><h3>Log Files</h3><p>As the execution of the test suites proceed, events are logged in
the following four different ways:</p><ul><li>Text to the operator console.</li><li>Suite-related information is sent to the major log file.</li><li>Case-related information is sent to the minor log file.</li><li>The HTML overview log file is updated with test results.</li><li>A link to all runs executed from a certain directory is written in the log named <strong>all_runs.html</strong> and direct links to all tests (the latest results) are written to the top-level <strong>index.html</strong>.</li></ul><p>Typically the operator, possibly running hundreds or thousands of
test cases, does not want to fill the console with details
about, or printouts from, specific test cases. By default, the 
operator only sees the following:</p><ul><li>A confirmation that the test has started and information about how  many test cases are executed in total.</li><li>A small note about each failed test case.</li><li>A summary of all the run test cases.</li><li>A confirmation when the test run is complete.</li><li>Some special information, such as error reports, progress reports, and printouts written with <strong>erlang:display/1</strong>, or <strong>io:format/3</strong> specifically addressed to a receiver other than <strong>standard_io</strong> (for example, the default group leader process <strong>user</strong>).</li></ul><p>To dig deeper into the general results, or
the result of a specific test case, the operator can do so by
following the links in the HTML presentation and read the
major or minor log files. The "all_runs.html" page is a good
starting point. It is located in <strong>logdir</strong> and contains
a link to each test run, including a quick overview (with date and time,
node name, number of tests, test names, and test result totals).</p><p>An "index.html" page is written for each test run (that is, stored in
the <strong>ct_run</strong> directory tagged with node name, date, and time). This
file provides an overview of all individual tests performed in the 
same test run. The test names follow the following convention:</p><ul><li><strong>TopLevelDir.TestDir</strong> (all suites in <strong>TestDir</strong> executed)</li><li><strong>TopLevelDir.TestDir:suites</strong> (specific suites executed)</li><li><strong>TopLevelDir.TestDir.Suite</strong> (all cases in <strong>Suite</strong> executed)</li><li><strong>TopLevelDir.TestDir.Suite:cases</strong> (specific test cases executed)</li><li><strong>TopLevelDir.TestDir.Suite.Case</strong> (only <strong>Case</strong> executed)</li></ul><p>The "test run index" page includes a link to the <strong>Common Test</strong>
Framework Log file in which information about imported
configuration data and general test progress is written. This
log file is useful to get snapshot information about the test
run during execution. It can also be helpful when
analyzing test results or debugging test suites.</p><p>The "test run index" page indicates if a test has missing
suites (that is, suites that <strong>Common Test</strong> failed to
compile). Names of the missing suites can be found in the
<strong>Common Test</strong> Framework Log file.</p><p>The major log file shows a detailed report of the test run. It
includes test suite and test case names, execution time, the 
exact reason for failures, and so on. The information is available in both
a file with textual and with HTML representation. The HTML file shows a 
summary that gives a good overview of the test run. It also has links 
to each individual test case log file for quick viewing with an HTML 
browser.</p><p>The minor log files contain full details of every single test
case, each in a separate file. This way, it is
straightforward	to compare the latest results to that of previous
test runs, even if the set of test cases changes. If application SASL
is running, its logs are also printed to the current minor log file by the
<a href="../common_test/ct_hooks_chapter#builtin_cths"> cth_log_redirect built-in hook</a>.
</p><p>The full name of the minor log file (that is, the name of the file
including the absolute directory path) can be read during execution
of the test case. It comes as value in tuple
<strong>{tc_logfile,LogFileName}</strong> in the <strong>Config</strong> list (which means it
can also be read by a pre- or post <strong>Common Test Hook</strong> function). Also,
at the start of a test case, this data is sent with an event
to any installed event handler.	For details, see section
<a href="./event_handler_chapter#event_handling">Event Handling</a>.
</p><p>The log files are written continuously during a test run and links are
always created initially when a test starts. Thevtest progress can therefore 
be followed simply by refreshing pages in the HTML browser.
Statistics totals are not presented until a test is complete however.</p><a name="logopts"></a><h3>Log Options</h3><p>With start flag <strong>logopts</strong> options that modify some aspects 
of the logging behavior can be specified.
The following options are available:</p><dl><dt><strong>no_src</strong></dt><dd><p>The HTML version of the test suite source code is not 
generated during the test run (and is consequently not available 
in the log file system).</p></dd><dt><strong>no_nl</strong></dt><dd><p><strong>Common Test</strong> does not add a newline character <strong>(\n)</strong> 
to the end of an output string that it receives from a call to, for example, 
<strong>io:format/2</strong>, and which it prints to the test case log.</p></dd></dl><p>For example, if a test is started with:</p><p><strong>$ ct_run -suite my_SUITE -logopts no_nl</strong></p><p>then printouts during the test made by successive calls to <strong>io:format("x")</strong>,
appears in the test case log as:</p><p><strong>xxx</strong></p><p>instead of each <strong>x</strong> printed on a new line, which is the default behavior.</p><a name="table_sorting"></a><h3>Sorting HTML Table Columns</h3><p>By clicking the name in the column header of any table 
(for example, "Ok", "Case", "Time", and so on), the table rows are sorted 
in whatever order makes sense for the type of value (for example,
numerical for "Ok" or "Time", and alphabetical for "Case"). The sorting is 
performed through JavaScript code, automatically inserted into the HTML 
log files. <strong>Common Test</strong> uses the <a href="http://jquery.com">jQuery</a> 
library and the
<a href="http://tablesorter.com">tablesorter</a> plugin, 
with customized sorting functions, for this implementation.</p><h3>The Unexpected I/O Log</h3><p>The test suites overview page includes a link to the Unexpected I/O Log.
In this log, <strong>Common Test</strong> saves printouts made with
<a href="./ct#log-2">ct#log-2</a> and 
<a href="./ct#pal-2">ct#pal-2</a>, as well as captured system 
error- and progress reports, which cannot be associated with particular test cases and
therefore cannot be written to individual test case log files. This occurs,
for example, if a log printout is made from an external process (not a test 
case process), <em>or</em> if an error- or progress report comes in, during a short 
interval while <strong>Common Test</strong> is not executing a test case or configuration 
function, <em>or</em> while <strong>Common Test</strong> is currently executing a parallel 
test case group.</p><a name="pre_post_test_io_log"></a><h3>The Pre- and Post Test I/O Log</h3><p>The <strong>Common Test</strong> Framework Log page includes links to the
Pre- and Post Test I/O Log. In this log, <strong>Common Test</strong> saves printouts made 
with <strong>ct:log/1,2,3,4,5</strong> and <strong>ct:pal/1,2,3,4,5</strong>, as well as captured system error-
and progress reports, which take place before, and after, the test run.
Examples of this are printouts from a CT hook init- or terminate function, or
progress reports generated when an OTP application is started from a CT hook
init function. Another example is an error report generated because of
a failure when an external application is stopped from a CT hook terminate function.
All information in these examples ends up in the Pre- and Post Test I/O Log.
For more information on how to synchronize test runs with external user
applications, see section
<a href="./ct_hooks_chapter#synchronizing">Synchronizing</a>
in section Common Test Hooks.</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>Logging to file with <strong>ct:log/1,2,3,4,5</strong> or <strong>ct:pal/1,2,3,4,5</strong>
only works when <strong>Common Test</strong> is running. Printouts with <strong>ct:pal/1,2,3,4,5</strong>
are however always displayed on screen.</p></div><a name="delete_old_logs"></a><h3>Delete Old Logs</h3><p><strong>Common Test</strong> can automatically delete old log. This
is specified with the <strong>keep_logs</strong> option. The default
value for this option is <strong>all</strong>, which means that no
logs are deleted. If the value is set to an
integer, <strong>N</strong>, <strong>Common Test</strong> deletes
all <strong>ct_run.&lt;timestamp&gt;</strong> directories, except
the <strong>N</strong> newest.</p><a name="html_stylesheet"></a><h3>HTML Style Sheets</h3><p><strong>Common Test</strong> uses an HTML Style Sheet (CSS file) to control the look of
the HTML log files generated during test runs. If the log files are not 
displayed correctly in the browser of your choice, or you prefer a more 
primitive ("pre <strong>Common Test</strong> v1.6") look of the logs, use the start 
flag/option:</p><pre>
 basic_html</pre><p>This disables the use of style sheets and JavaScripts (see
<a href="#table_sorting">Sorting HTML Table Columns</a>).</p><p><strong>Common Test</strong> includes an <em>optional</em> feature to allow
user HTML style sheets for customizing printouts. The
functions in <strong>ct</strong> that print to a test case HTML log
file (<strong>log/3,4,5</strong> and <strong>pal/3,4,5</strong>) accept <strong>Category</strong>
as first argument. With this argument a category can be specified 
that can be mapped to a named <strong>div</strong> selector in a CSS rule-set.
This is useful, especially for coloring text
differently depending on the type of (or reason for) the
printout. Say you want one particular background color for test system
configuration information, a different one for test system
state information, and finally one for errors detected by the
test case functions. The corresponding style sheet can
look as follows:</p><pre>
 div.sys_config  { background:blue }
 div.sys_state   { background:yellow }
 div.error       { background:red }</pre><p>Common Test prints the text from <strong>ct:log/3,4,5</strong> or
<strong>ct:pal/3,4,5</strong> inside a <strong>pre</strong> element
nested under the named <strong>div</strong> element. Since the <strong>pre</strong> selector
has a predefined CSS rule (in file <strong>ct_default.css</strong>) for the attributes
<strong>color</strong>, <strong>font-family</strong> and <strong>font-size</strong>, if a user wants to
change any of the predefined attribute settings, a new rule for <strong>pre</strong>
must be added to the user stylesheet. Example:</p><pre>
div.error pre { color:white }</pre><p>Here, white text is used instead of the default black for <strong>div.error</strong>
printouts (and no other attribute settings for <strong>pre</strong> are affected).</p><p>To install the CSS file (<strong>Common Test</strong> inlines the definition in the 
HTML code), the file name can be provided when executing <strong>ct_run</strong>.</p><p><em>Example:</em></p><pre>
 $ ct_run -dir $TEST/prog -stylesheet $TEST/styles/test_categories.css</pre><p>Categories in a CSS file installed with flag <strong>-stylesheet</strong>
are on a global test level in the sense that they can be used in any 
suite that is part of the test run.</p><p>Style sheets can also be installed on a per suite and
per test case basis.</p><p><em>Example:</em></p><pre>
 -module(my_SUITE).
 ...
 suite() -&gt; [..., {stylesheet,"suite_categories.css"}, ...].
 ...
 my_testcase(_) -&gt;
     ...
     ct:log(sys_config, "Test node version: ~p", [VersionInfo]),
     ...
     ct:log(sys_state, "Connections: ~p", [ConnectionInfo]),
     ...
     ct:pal(error, "Error ~p detected! Info: ~p", [SomeFault,ErrorInfo]),
     ct:fail(SomeFault).</pre><p>If the style sheet is installed as in this example, the categories are 
private to the suite in question. They can be used by all test cases in the 
suite, but cannot be used by other suites. A suite private style sheet, 
if specified, is used in favor of a global style sheet (one specified 
with flag <strong>-stylesheet</strong>). A stylesheet tuple (as returned by <strong>suite/0</strong> 
above) can also be returned from a test case information function. In this case the 
categories specified in the style sheet can only be used in that particular 
test case. A test case private style sheet is used in favor of a suite or 
global level style sheet.
</p><p>In a tuple <strong>{stylesheet,CSSFile}</strong>, if <strong>CSSFile</strong> is specified
with a path, for example, <strong>"$TEST/styles/categories.css"</strong>, this full
name is used to locate the file. However, if only the file name is specified,
for example, <strong>categories.css</strong>, the CSS file is assumed to be located
in the data directory, <strong>data_dir</strong>, of the suite. The latter use is
recommended, as it is portable compared to hard coding path names in the 
suite.</p><p>Argument <strong>Category</strong> in the previous example can have the
value (atom) <strong>sys_config</strong> (blue background), <strong>sys_state</strong>
(yellow background), or <strong>error</strong> (white text on red background).</p><a name="repeating_tests"></a><h3>Repeating Tests</h3><p>You can order <strong>Common Test</strong> to repeat the tests you specify. You can choose
to repeat tests a number of times, repeat tests for a specific period of time, 
or repeat tests until a particular stop time is reached. If repetition is controlled by
time, an action for <strong>Common Test</strong> to take upon time-out can be specified. 
Either <strong>Common Test</strong> performs all tests in the current run 
before stopping, or it stops when the current test job is finished. Repetition 
can be activated by <strong>ct_run</strong> start flags, or tuples in the <strong>ct:run:test/1</strong>
option list argument. The flags (options in parentheses) are the following:</p><ul><li><strong>-repeat N ({repeat,N})</strong>, where <strong>N</strong> is a positive integer</li><li><strong>-duration DurTime ({duration,DurTime})</strong>, where <strong>DurTime</strong> is the duration</li><li><strong>-until StopTime ({until,StopTime})</strong>, where <strong>StopTime</strong> is finish time</li><li><strong>-force_stop ({force_stop,true})</strong></li><li><strong>-force_stop skip_rest ({force_stop,skip_rest})</strong></li></ul><dl><dt><strong>DurTime</strong></dt><dd><p>The duration time is specified as <strong>HHMMSS</strong>, for example, <strong>-duration 012030</strong> 
or <strong>{duration,"012030"}</strong></p>, which means that the tests are executed and  (if time allows) repeated until time-out occurs after 1 hour, 20 minutes, and 30 seconds. </dd><dt><strong>StopTime</strong></dt><dd><p>The finish time can be specified as <strong>HHMMSS</strong> and is then interpreted as a 
time today (or possibly tomorrow), but can also be specified as <strong>YYMoMoDDHHMMSS</strong>,
for example, <strong>-until 071001120000</strong> or <strong>{until,"071001120000"}</strong>. This means
that the tests are executed and (if time allows) repeated, until 12 o'clock on the 1st 
of October 2007.</p> </dd></dl><p>When time-out occurs, <strong>Common Test</strong> never aborts the ongoing test case,
as this can leave the SUT in an undefined, and possibly bad, state.
Instead <strong>Common Test</strong>, by default, finishes the current test
run before stopping. If flag <strong>force_stop</strong> is
specified, <strong>Common Test</strong> stops when the current test job
is finished. If flag <strong>force_stop</strong> is specified with
<strong>skip_rest</strong>, <strong>Common Test</strong> only completes the current
test case and skips the remaining tests in the test job.</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>As <strong>Common Test</strong> always finishes at least the current test case,
the time specified with <strong>duration</strong> or <strong>until</strong> is never definitive.</p></div><p>Log files from every repeated test run is saved in normal <strong>Common Test</strong> 
fashion (described earlier).</p><p><strong>Common Test</strong> might later support an optional feature to only store the last (and possibly 
the first) set of logs of repeated test runs, but for now the user must be careful not 
to run out of disk space if tests are repeated during long periods of time.</p><p>For each test run that is part of a repeated session, information about the
particular test run is printed in the <strong>Common Test</strong> Framework Log. The information
includes the repetition number, remaining time, and so on.</p><p><em>Example 1:</em></p><pre>
 $ ct_run -dir $TEST_ROOT/to1 $TEST_ROOT/to2 -duration 001000 -force_stop</pre><p>Here, the suites in test directory <strong>to1</strong>, followed by the suites in <strong>to2</strong>, are 
executed in one test run. A time-out event occurs after 10 minutes. As long as there is 
time left, <strong>Common Test</strong> repeats the test run (that is, starting over with test <strong>to1</strong>). 
After time-out, <strong>Common Test</strong> stops when the current job is finished
(because of flag <strong>force_stop</strong>). As a result, the specified test run can be 
aborted after test <strong>to1</strong> and before test <strong>to2</strong>.</p><p><em>Example 2:</em></p><pre>
 $ ct_run -dir $TEST_ROOT/to1 $TEST_ROOT/to2 -duration 001000 -forces_stop skip_rest</pre><p>Here, the same tests as in Example 1 are run, but with flag <strong>force_stop</strong> set to 
<strong>skip_rest</strong>. If time-out occurs while executing tests in directory <strong>to1</strong>, 
the remaining test cases in <strong>to1</strong> are skipped and the test is aborted without 
running the tests in <strong>to2</strong> another time. If time-out occurs while executing 
tests in directory <strong>to2</strong>, the remaining test cases in <strong>to2</strong> are skipped and 
the test is aborted.</p><p><em>Example 3:</em></p><pre>
 $ date
 Fri Sep 28 15:00:00 MEST 2007

 $ ct_run -dir $TEST_ROOT/to1 $TEST_ROOT/to2 -until 160000</pre><p>Here, the same test run as in the previous examples are executed (and possibly repeated). 
However, when the time-out occurs, after 1 hour, <strong>Common Test</strong> finishes the entire 
test run before stopping (that is, both <strong>to1</strong> and <strong>to2</strong> are always executed in 
the same test run).</p><p><em>Example 4:</em></p><pre>
 $ ct_run -dir $TEST_ROOT/to1 $TEST_ROOT/to2 -repeat 5</pre><p>Here, the test run, including both the <strong>to1</strong> and the <strong>to2</strong> test, is repeated 
five times.</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>Do not confuse this feature with the <strong>repeat</strong> property of a test
case group. The options described here are used to repeat execution of entire test runs,
while the <strong>repeat</strong> property of a test case group makes it possible to repeat
execution of sets of test cases within a suite. For more information about the latter,
see section <a href="./write_test_chapter#test_case_groups">Test Case Groups </a>
in section Writing Test Suites.</p></div><a name="silent_connections"></a><h3>Silent Connections</h3><p>The protocol handling processes in <strong>Common Test</strong>, implemented by <strong>ct_telnet</strong>,
<strong>ct_ssh</strong>, <strong>ct_ftp</strong>, and so on, do verbose printing to the test case logs. 
This can be switched off with flag <strong>-silent_connections</strong>:</p><pre>
 ct_run -silent_connections [conn_types]</pre><p>Here, <strong>conn_types</strong> specifies SSH, Telnet, FTP, RPC, and/or SNMP.</p><p><em>Example 1:</em></p><pre>
 ct_run ... -silent_connections ssh telnet</pre><p>This switches off logging for SSH and Telnet connections.</p><p><em>Example 2:</em></p><pre>
 ct_run ... -silent_connections</pre><p>This switches off logging for all connection types.</p><p>Fatal communication error and reconnection attempts are always printed, even if 
logging has been suppressed for the connection type in question. However, operations
such as sending and receiving data are performed silently.</p><p><strong>silent_connections</strong> can also be specified in a test suite. This is
accomplished by returning a tuple, <strong>{silent_connections,ConnTypes}</strong>, in the
<strong>suite/0</strong> or test case information list. If <strong>ConnTypes</strong> is a list of atoms 
(SSH, Telnet, FTP, RPC and/or SNMP), output for any corresponding connections 
are suppressed. Full logging is by default enabled for any connection of type not 
specified in <strong>ConnTypes</strong>. Hence, if <strong>ConnTypes</strong> is the empty list, logging 
is enabled for all connections.</p><p><em>Example 3:</em></p><pre>
 -module(my_SUITE).

 suite() -&gt; [..., {silent_connections,[telnet,ssh]}, ...].

 ...

 my_testcase1() -&gt;
     [{silent_connections,[ssh]}].

 my_testcase1(_) -&gt;
     ...

 my_testcase2(_) -&gt;
     ...</pre><p>In this example, <strong>suite/0</strong> tells <strong>Common Test</strong> to suppress
printouts from Telnet and SSH connections. This is valid for
all test cases. However, <strong>my_testcase1/0</strong> specifies that
for this test case, only SSH is to be silent. The result is
that <strong>my_testcase1</strong> gets Telnet information (if any) printed
in the log, but not SSH information. <strong>my_testcase2</strong> gets no
information from either connection printed.</p><p><strong>silent_connections</strong> can also be specified with a term
in a test specification
(see section <a href="./run_test_chapter#test_specifications">Test Specifications</a> in section Running Tests and Analyzing Results).
Connections provided with start	flag/option <strong>silent_connections</strong>
are merged with any connections listed in the test specification.</p><p>Start flag/option <strong>silent_connections</strong> and the test
specification term override any settings made by the information functions
inside the test suite.</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>In the current <strong>Common Test</strong> version, the
<strong>silent_connections</strong> feature only works for Telnet
and SSH connections. Support for other connection types can be added
in future <strong>Common Test</strong> versions.</p></div><a name="top"></a><h3>General</h3><p>To avoid hard-coding data values related to the test and/or System
Under Test (SUT) in the test suites, the data can instead be specified through
configuration files or strings that <strong>Common Test</strong> reads before
the start of a test run. External configuration data makes it possible to
change test properties without modifying the test suites
using the data. Examples of configuration data follows:</p><ul><li>Addresses to the test plant or other instruments</li><li>User login information</li><li>Names of files needed by the test</li><li>Names of programs to be executed during the test</li><li>Any other variable needed by the test</li></ul><h3>Syntax</h3><p>A configuration file can contain any number of elements of the type:</p><pre>
 {CfgVarName,Value}.</pre><p>where</p><pre>
 CfgVarName = atom()
 Value = term() | [{CfgVarName,Value}]</pre><h3>Requiring and Reading Configuration Data</h3><a name="require_config_data"></a><p>In a test suite, one must <em>require</em> that a configuration 
variable (<strong>CfgVarName</strong> in the previous definition) exists before
attempting to read the associated value in a test case or configuration function.</p><p><strong>require</strong> is an assert statement, which can be part of the <a href="./write_test_chapter#suite">Test Suite Information Function</a> or
<a href="./write_test_chapter#info_function">Test Case Information Function</a>. If the required variable is unavailable, the
test is skipped (unless a default value has been specified, see section
<a href="./write_test_chapter#info_function">Test Case Information Function</a> for details). Also, function
<a href="./ct#require-1">ct#require-1</a> can be called 
from a test case to check if a specific variable is available. The return 
value from this function must be checked explicitly and appropriate 
action be taken depending on the result (for example, to skip the test case
if the variable in question does not exist).</p><p>A <strong>require</strong> statement in the test suite information case or test case 
information-list is to look like
<strong>{require,CfgVarName}</strong> or <strong>{require,AliasName,CfgVarName}</strong>.
The arguments <strong>AliasName</strong> and <strong>CfgVarName</strong> are the same as the
arguments to <a href="./ct#require-1">ct#require-1</a>. 
<strong>AliasName</strong> becomes an alias for the configuration variable,
and can be used as reference to the configuration data value.
The configuration variable can be associated with any
number of alias names, but each name must be unique within
the same test suite. The two main uses for alias names follows:</p><ul><li>To identify connections (described later).</li><li>To help adapt configuration data to a test suite  (or test case) and improve readability.</li></ul><p>To read the value of a configuration variable, use function
<a href="./ct#get_config-1">ct#get_config-1</a>.
</p><p><em>Example:</em></p><pre>
 suite() -&gt; 
     [{require, domain, 'CONN_SPEC_DNS_SUFFIX'}].

 ...

 testcase(Config) -&gt;
     Domain = ct:get_config(domain),
     ...</pre><h3>Using Configuration Variables Defined in Multiple Files</h3><p>If a configuration variable is defined in multiple files and you 
want to access all possible values, use function
<a href="./ct#get_config-3">ct#get_config-3</a>
and specify <strong>all</strong> in the options list. The values are then
returned in a list and the order of the elements corresponds to the order 
that the configuration files were specified at startup.</p><h3>Encrypted Configuration Files</h3><a name="encrypted_config_files"></a><p>Configuration files containing sensitive data can be encrypted 
if they must be stored in open and shared directories.</p><p>To have <strong>Common Test</strong> encrypt a
specified file using function <strong>DES3</strong> in application <strong>Crypto</strong>,
call <a href="./ct#encrypt_config_file-2">ct#encrypt_config_file-2</a>
The encrypted file can then be used as a regular configuration file
in combination with other encrypted files or normal text files. However, the 
key for decrypting the configuration file must be provided when running the test.
This can be done with flag/option <strong>decrypt_key</strong> or
<strong>decrypt_file</strong>, or a key file in a predefined location.</p><p><strong>Common Test</strong> also provides decryption functions, 
<a href="./ct#decrypt_config_file-2">ct#decrypt_config_file-2</a>, 
for recreating the original text files.</p><h3>Opening Connections Using Configuration Data</h3><p>Two different methods for opening a connection using the support functions 
in, for example, <a href="ct_ssh">ct_ssh</a>, 
<a href="ct_ftp">ct_ftp</a>, and 
<a href="ct_telnet">ct_telnet</a> follows:</p><ul><li>Using a configuration target name (an alias) as reference.</li><li>Using the configuration variable as reference.</li></ul><p>When a target name is used for referencing the configuration data
(that specifies the connection to be opened), the same name can be used 
as connection identity in all subsequent calls related to the connection
(also for closing it). Only one open connection per target name 
is possible. If you attempt to open a new connection using a name
already associated with an open connection, <strong>Common Test</strong>
returns the already existing handle so the previously opened connection
is used. This feature makes it possible to
call the function for opening a particular connection whenever 
useful. An action like this does not necessarily open any new 
connections unless it is required (which could be the case if, for example,
the previous connection has been closed unexpectedly by the server).
Using named connections also removes the need to pass handle references 
around in the suite for these connections.
</p><p>When a configuration variable name is used as reference to the data
specifying the connection, the handle returned as a result of opening
the connection must be used in all subsequent calls (also for closing
the connection). Repeated calls to the open function with the same
variable name as reference results in multiple connections being opened. 
This can be useful, for example, if a test case needs to open
multiple connections to the same server on the target node (using the
same configuration data for each connection).
</p><h3>User-Specific Configuration Data Formats</h3><p>The user can specify configuration data on a
different format than key-value tuples in a text file, as described
so far. The data can, for example, be read from any files, fetched from
the web over HTTP, or requested from a user-specific process.
To support this, <strong>Common Test</strong> provides a callback module plugin
mechanism to handle configuration data.</p><h3>Default Callback Modules for Handling Configuration Data</h3><p><strong>Common Test</strong> includes default callback modules
for handling configuration data specified in standard configuration files
(described earlier) and in XML files as follows:</p><ul><li> <strong>ct_config_plain</strong> - for reading configuration files with key-value tuples (standard format). This handler is used to parse configuration files if no user callback is specified. </li><li> <strong>ct_config_xml</strong> - for reading configuration data from XML files. </li></ul><h3>Using XML Configuration Files</h3><p>An example of an XML configuration file follows:</p><pre>
 
 &lt;config&gt;
    &lt;ftp_host&gt;
        &lt;ftp&gt;"targethost"&lt;/ftp&gt;
        &lt;username&gt;"tester"&lt;/username&gt;
        &lt;password&gt;"letmein"&lt;/password&gt;
    &lt;/ftp_host&gt;
    &lt;lm_directory&gt;"/test/loadmodules"&lt;/lm_directory&gt;
 &lt;/config&gt;</pre><p>Once read, this file produces the same configuration
variables as the following text file:</p><pre>
 {ftp_host, [{ftp,"targethost"},
             {username,"tester"},
             {password,"letmein"}]}.

 {lm_directory, "/test/loadmodules"}.</pre><h3>Implement a User-Specific Handler</h3><p>The user-specific handler can be written to handle special
configuration file formats. The parameter can be either file
names or configuration strings (the empty list is valid).</p><p>The callback module implementing the handler is responsible for
checking the correctness of configuration strings.</p><p>To validate the configuration strings, the callback module
is to have function <strong>Callback:check_parameter/1</strong> exported.</p><p>The input argument is passed from <strong>Common Test</strong>, as defined in the test
specification, or specified as an option to <strong>ct_run</strong> or <strong>ct:run_test</strong>.</p><p>The return value is to be any of the following values, indicating if the specified
configuration parameter is valid:</p><ul><li> <strong>{ok, {file, FileName}}</strong> - the parameter is a file name and the file exists. </li><li> <strong>{ok, {config, ConfigString}}</strong> - the parameter is a configuration string and it is correct. </li><li> <strong>{error, {nofile, FileName}}</strong> - there is no file with the specified name in the current directory. </li><li> <strong>{error, {wrong_config, ConfigString}}</strong> - the configuration string is wrong. </li></ul><p>The function <strong>Callback:read_config/1</strong> is to be exported from the 
callback module to read configuration data, initially before the tests
start, or as a result of data being reloaded during test execution.
The input argument is the same as for function <strong>check_parameter/1</strong>.</p><p>The return value is to be either of the following:</p><ul><li> <strong>{ok, Config}</strong> - if the configuration variables are read successfully. </li><li> <strong>{error, {Error, ErrorDetails}}</strong> - if the callback module fails to proceed with the specified configuration parameters. </li></ul><p><strong>Config</strong> is the proper Erlang key-value list, with possible
key-value sublists as values, like the earlier configuration file
example:</p><pre>
 [{ftp_host, [{ftp, "targethost"}, {username, "tester"}, {password, "letmein"}]},
  {lm_directory, "/test/loadmodules"}]</pre><h3>Examples of Configuration Data Handling</h3><p>A configuration file for using the FTP client to access files on a remote
host can look as follows:</p><pre>
 {ftp_host, [{ftp,"targethost"},
	     {username,"tester"},
	     {password,"letmein"}]}.

 {lm_directory, "/test/loadmodules"}.</pre><p>The XML version shown earlier can also be used, but it is to be
explicitly specified that the <strong>ct_config_xml</strong> callback module is to be
used by <strong>Common Test</strong>.</p><p>The following is an example of how to assert that the configuration data is available
and can be used for an FTP session:</p><pre>
 init_per_testcase(ftptest, Config) -&gt;
     {ok,_} = ct_ftp:open(ftp),
     Config.

 end_per_testcase(ftptest, _Config) -&gt;
     ct_ftp:close(ftp).

 ftptest() -&gt;
     [{require,ftp,ftp_host},
      {require,lm_directory}].

 ftptest(Config) -&gt;
     Remote = filename:join(ct:get_config(lm_directory), "loadmodX"),
     Local = filename:join(?config(priv_dir,Config), "loadmodule"),
     ok = ct_ftp:recv(ftp, Remote, Local),
     ...</pre><p>The following is an example of how the functions in the previous example 
can be rewritten if it is necessary to open multiple connections to the 
FTP server:</p><pre>
 init_per_testcase(ftptest, Config) -&gt;
     {ok,Handle1} = ct_ftp:open(ftp_host),
     {ok,Handle2} = ct_ftp:open(ftp_host),
     [{ftp_handles,[Handle1,Handle2]} | Config].

 end_per_testcase(ftptest, Config) -&gt;
     lists:foreach(fun(Handle) -&gt; ct_ftp:close(Handle) end, 
                   ?config(ftp_handles,Config)).

 ftptest() -&gt;
     [{require,ftp_host},
      {require,lm_directory}].

 ftptest(Config) -&gt;
     Remote = filename:join(ct:get_config(lm_directory), "loadmodX"),
     Local = filename:join(?config(priv_dir,Config), "loadmodule"),
     [Handle | MoreHandles] = ?config(ftp_handles,Config),
     ok = ct_ftp:recv(Handle, Remote, Local),
     ...</pre><h3>Example of User-Specific Configuration Handler</h3><p>A simple configuration handling driver, asking an external server for
configuration data, can be implemented as follows:</p><pre>
 -module(config_driver).
 -export([read_config/1, check_parameter/1]).

 read_config(ServerName)-&gt;
     ServerModule = list_to_atom(ServerName),
     ServerModule:start(),
     ServerModule:get_config().

 check_parameter(ServerName)-&gt;
     ServerModule = list_to_atom(ServerName),
     case code:is_loaded(ServerModule) of
         {file, _}-&gt;
             {ok, {config, ServerName}};
         false-&gt;
             case code:load_file(ServerModule) of
                 {module, ServerModule}-&gt;
                     {ok, {config, ServerName}};
                 {error, nofile}-&gt;
                     {error, {wrong_config, "File not found: " ++ ServerName ++ ".beam"}}
             end
     end.</pre><p>The configuration string for this driver can be <strong>config_server</strong>, if the
<strong>config_server.erl</strong> module that follows is compiled and exists in the code path
during test execution:</p><pre>
 -module(config_server).
 -export([start/0, stop/0, init/1, get_config/0, loop/0]).

 -define(REGISTERED_NAME, ct_test_config_server).

 start()-&gt;
     case whereis(?REGISTERED_NAME) of
         undefined-&gt;
             spawn(?MODULE, init, [?REGISTERED_NAME]),
             wait();
         _Pid-&gt;
         ok
     end,
     ?REGISTERED_NAME.

 init(Name)-&gt;
     register(Name, self()),
     loop().

 get_config()-&gt;
     call(self(), get_config).

 stop()-&gt;
     call(self(), stop).

 call(Client, Request)-&gt;
     case whereis(?REGISTERED_NAME) of
         undefined-&gt;
             {error, {not_started, Request}};
         Pid-&gt;
             Pid ! {Client, Request},
             receive
                 Reply-&gt;
                     {ok, Reply}
             after 4000-&gt;
                 {error, {timeout, Request}}
             end
     end.

 loop()-&gt;
     receive
         {Pid, stop}-&gt;
             Pid ! ok;
         {Pid, get_config}-&gt;
             {D,T} = erlang:localtime(),
             Pid !
                 [{localtime, [{date, D}, {time, T}]},
                  {node, erlang:node()},
                  {now, erlang:now()},
                  {config_server_pid, self()},
                  {config_server_vsn, ?vsn}],
             ?MODULE:loop()
     end.

 wait()-&gt;
     case whereis(?REGISTERED_NAME) of
         undefined-&gt;
             wait();
         _Pid-&gt;
             ok
     end.</pre><p>Here, the handler also provides for dynamically reloading of
configuration variables. If 
<a href="./ct#reload_config-1">ct#reload_config-1</a> is called from
the test case function, all variables loaded with <strong>config_driver:read_config/1</strong>
are updated with their latest values, and the new value for variable
<strong>localtime</strong> is returned.</p><a name="cover"></a><h3>General</h3><p>Although <strong>Common Test</strong> was created primarily for
black-box testing, nothing prevents it from working perfectly as
a white-box testing tool as well. This is especially true when
the application to test is written in Erlang. Then the test
ports are easily realized with Erlang function calls.</p><p>When white-box testing an Erlang application, it is useful to
be able to measure the code coverage of the test. <strong>Common Test</strong>
provides simple access to the OTP Cover tool for this
purpose. <strong>Common Test</strong> handles all necessary communication with
the Cover tool (starting, compiling, analysing, and so on).
The <strong>Common Test</strong> user only needs to specify the extent of the
code coverage analysis.</p><h3>Use</h3><p>To specify the modules to be included in the code coverage test, 
provide a cover specification file. With this file you can point 
out specific modules or specify directories containing modules to be
included in the analysis. You can also specify modules to be excluded 
from the analysis.</p><p>If you are testing a distributed Erlang application, it is
likely that code you want included in the code coverage analysis
gets executed on another Erlang node than the one <strong>Common Test</strong>
is running on. If so, you must specify these other nodes in the 
cover specification file or add them dynamically to the code coverage 
set of nodes. For details on the latter, see module 
<a href="ct_cover">ct_cover</a>.</p><p>In the cover specification file you can also specify your
required level of the code coverage analysis; <strong>details</strong> or
<strong>overview</strong>. In detailed mode, you get a coverage overview
page, showing per module and total coverage percentages.
You also get an HTML file printed for each module included in the
analysis showing exactly what parts of the code have been
executed during the test. In overview mode, only the code
coverage overview page is printed.</p><p>You can choose to export and import code coverage data between
tests. If you specify the name of an export file in the cover
specification file, <strong>Common Test</strong> exports collected coverage
data to this file at the end of the test. You can similarly
specify previously exported data to be imported and
included in the analysis for a test (multiple import files can be specified). 
This way, the total code coverage can be analyzed without necessarily 
running all tests at once.</p><p>To activate the code coverage support, specify the name of the cover 
specification file as you start <strong>Common Test</strong>.
Do this by using flag <strong>-cover</strong> with 
<a href="ct_run">ct_run</a>, 
for example:</p><pre>
 $ ct_run -dir $TESTOBJS/db -cover $TESTOBJS/db/config/db.coverspec</pre><p>You can also pass the cover specification file name in a
call to <a href="./ct#run_test-1">ct#run_test-1</a>, 
by adding a <strong>{cover,CoverSpec}</strong> tuple to argument <strong>Opts</strong>.</p><p>You can also enable code coverage in your test specifications (see section 
<a href="./run_test_chapter#test_specifications">Test Specifications</a>
in section Running Tests and Analyzing Results).</p><a name="cover_stop"></a><h3>Stopping the Cover Tool When Tests Are Completed</h3><p>By default, the Cover tool is automatically stopped when the
tests are completed. This causes the original (non-cover
compiled) modules to be loaded back into the test node. If a
process at this point still runs old code of any of the
modules that are cover compiled, meaning that it has not done
any fully qualified function call after the cover compilation,
the process is killed. To avoid this, set the value of option 
<strong>cover_stop</strong> to <strong>false</strong>. This means that the 
modules stay cover compiled. Therefore, this is only recommended 
if the Erlang nodes under test are terminated after the test is 
completed, or if cover can be manually stopped.</p><p>The option can be set by using flag <strong>-cover_stop</strong> with
<strong>ct_run</strong>, by adding <strong>{cover_stop,true|false}</strong> to argument
<strong>Opts</strong> to 
<a href="./ct#run_test-1">ct#run_test-1</a>, 
or by adding a <strong>cover_stop</strong> term in the test specification (see section
<a href="./run_test_chapter#test_specifications">Test Specifications</a>
in section Running Tests and Analyzing Results).</p><h3>The Cover Specification File</h3><p>The following terms are allowed in a cover specification file:</p><pre>
 %% List of Nodes on which cover will be active during test.
 %% Nodes = [atom()]
 {nodes, Nodes}.       

 %% Files with previously exported cover data to include in analysis.
 %% CoverDataFiles = [string()]
 {import, CoverDataFiles}.

 %% Cover data file to export from this session.
 %% CoverDataFile = string()
 {export, CoverDataFile}.

 %% Cover analysis level.
 %% Level = details | overview
 {level, Level}.       

 %% Directories to include in cover.
 %% Dirs = [string()]
 {incl_dirs, Dirs}.

 %% Directories, including subdirectories, to include.
 {incl_dirs_r, Dirs}.

 %% Specific modules to include in cover.
 %% Mods = [atom()]
 {incl_mods, Mods}.

 %% Directories to exclude in cover.
 {excl_dirs, Dirs}.

 %% Directories, including subdirectories, to exclude.
 {excl_dirs_r, Dirs}.

 %% Specific modules to exclude in cover.
 {excl_mods, Mods}.

 %% Cross cover compilation
 %% Tag = atom(), an identifier for a test run
 %% Mod = [atom()], modules to compile for accumulated analysis
 {cross,[{Tag,Mods}]}.</pre><p>The terms <strong>incl_dirs_r</strong> and <strong>excl_dirs_r</strong> tell <strong>Common Test</strong> to search the specified directories recursively and include 
or exclude any module found during the search. The terms
<strong>incl_dirs</strong> and <strong>excl_dirs</strong> result in a
non-recursive search for modules (that is, only modules found in 
the specified directories are included or excluded).</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>Directories containing Erlang modules to be included in a code 
coverage test must exist in the code server path. Otherwise, 
the Cover tool fails to recompile the modules. It is not sufficient to 
specify these directories in the cover specification file for 
<strong>Common Test</strong>.</p></div><a name="cross_cover"></a><h3>Cross Cover Analysis</h3><p>The cross cover mechanism allows cover analysis of modules
across multiple tests. It is useful if some code, for example, a 
library module, is used by many different tests and the accumulated 
cover result is desirable.</p><p>This can also be achieved in a more customized way by
using parameter <strong>export</strong> in the cover specification and
analysing the result off line. However, the cross cover mechanism is a
built-in solution that also provides logging.</p><p>The mechanism is easiest explained by an example:</p><p>Assume that there are two systems, <strong>s1</strong> and <strong>s2</strong>,
that are tested in separate test runs. System <strong>s1</strong> contains
a library module <strong>m1</strong> tested by test run <strong>s1</strong> and 
is included in the cover specification of <strong>s1</strong> as follows:</p><pre><code class="">
 s1.cover:
   {incl_mods,[m1]}.</code></pre><p>When analysing code coverage, the result for <strong>m1</strong> can be
seen in the cover log in the <strong>s1</strong> test result.</p><p>Now, imagine that as <strong>m1</strong> is a library module, it
is also often used by system <strong>s2</strong>. Test run <strong>s2</strong>
does not specifically test <strong>m1</strong>, but it can still be
interesting to see which parts of <strong>m1</strong> that are covered 
by the <strong>s2</strong> tests. To do this, <strong>m1</strong> can be included also
in the cover specification of <strong>s2</strong> as follows:</p><pre><code class="">
 s2.cover:
   {incl_mods,[m1]}.</code></pre><p>This gives an entry for <strong>m1</strong> also in the cover log
for test run <strong>s2</strong>. The problem is that this only
reflects the coverage by <strong>s2</strong> tests, not the accumulated
result over <strong>s1</strong> and <strong>s2</strong>. This is where the cross
cover mechanism comes in handy.</p><p>If instead the cover specification for <strong>s2</strong> is like
the following:</p><pre><code class="">
 s2.cover:
   {cross,[{s1,[m1]}]}.</code></pre><p>Then <strong>m1</strong> is cover compiled in test run <strong>s2</strong>,
but not shown in the coverage log. Instead, if
<a href="./ct_cover#cross_cover_analyse-2">ct_cover#cross_cover_analyse-2</a> 
is called after both <strong>s1</strong> and <strong>s2</strong> test runs are completed, 
the accumulated result for <strong>m1</strong> is available in the cross cover 
log for test run <strong>s1</strong>.</p><p>The call to the analyze function must be as follows:</p><pre><code class="">
 ct_cover:cross_cover_analyse(Level, [{s1,S1LogDir},{s2,S2LogDir}]).</code></pre><p>Here, <strong>S1LogDir</strong> and <strong>S2LogDir</strong> are the directories
named <strong>&lt;TestName&gt;.logs</strong> for each test respectively.</p><p>Notice the tags <strong>s1</strong> and <strong>s2</strong>, which are used in the
cover specification file and in the call to
<strong>ct_cover:cross_cover_analyse/2</strong>. The purpose of these is only
to map the modules specified in the cover specification to the log
directory specified in the call to the analyze function. The tag name
has no meaning beyond this.</p><h3>Logging</h3><p>To view the result of a code coverage test, click the button
labeled "COVER LOG" in the top-level index page for the test run.</p><p>Before Erlang/OTP 17.1, if your test run consisted of
multiple tests, cover would be started and stopped for each test
within the test run. Separate logs would be available through the
"Coverage log" link on the test suite result pages. These links
are still available, but now they all point to the same page as
the button on the top-level index page. The log contains the
accumulated results for the complete test run. For details about 
this change, see the release notes.</p><p>The button takes you to the code coverage overview page. If you
have successfully performed a detailed coverage analysis,
links to each individual module coverage page are found here.</p><p>If cross cover analysis is performed, and there are
accumulated coverage results for the current test, the link
"Coverdata collected over all tests" takes you to these
results.</p><a name="general"></a><h3>General</h3><p>Large-scale automated testing requires running multiple independent 
test sessions in parallel. This is accomplished by running
some <strong>Common Test</strong> nodes on one or more hosts, testing
different target systems. Configuring, starting, and controlling the
test nodes independently can be a cumbersome operation. To aid
this kind of automated large-scale testing, <strong>Common Test</strong> offers a master 
test node component, <strong>Common Test</strong> Master, which handles central configuration and control
in a system of distributed <strong>Common Test</strong> nodes.</p><p>The <strong>Common Test</strong> Master server runs on one dedicated Erlang node and uses distributed
Erlang to communicate with any number of <strong>Common Test</strong> test nodes, each hosting a regular
<strong>Common Test</strong> server. Test specifications are used as input to specify what to test on which 
test nodes, using what configuration.</p><p>The <strong>Common Test</strong> Master server writes progress information to HTML log files similarly 
to the regular <strong>Common Test</strong> server. The logs contain test statistics and links to the 
log files written by each independent <strong>Common Test</strong> server.</p><p>The <strong>Common Test</strong> Master API is exported by module 
<a href="ct_master">ct_master</a>.</p><h3>Use</h3><p><strong>Common Test</strong> Master requires all test nodes to be on the same network and share a common 
file system. <strong>Common Test</strong> Master cannot start test nodes
automatically. The nodes must be started in advance for <strong>Common Test</strong> Master to be 
able to start test sessions on them.</p><p>Tests are started by calling 
<a href="./ct_master#run-1">ct_master#run-1</a> or 
<a href="./ct_master#run-3">ct_master#run-3</a></p><p><strong>TestSpecs</strong> is either the name of a test specification file (string) or a list 
of test specifications. If it is a list, the specifications are handled (and
the corresponding tests executed) in sequence. An element in a <strong>TestSpecs</strong> list 
can also be list of test specifications. The specifications in such a list are 
merged into one combined specification before test execution.</p><p><em>Example:</em></p><pre>
 ct_master:run(["ts1","ts2",["ts3","ts4"]])</pre><p>Here, the tests specified by "ts1" run first, then the tests specified by "ts2",
and finally the tests specified by both "ts3" and "ts4".</p><p>The <strong>InclNodes</strong> argument to <strong>run/3</strong> is a list of node names. Function
<strong>run/3</strong> runs the tests in <strong>TestSpecs</strong> just like <strong>run/1</strong>, but also 
takes any test in <strong>TestSpecs</strong>, which is not explicitly tagged with a particular 
node name, and execute it on the nodes listed in <strong>InclNodes</strong>. By using <strong>run/3</strong> 
this way, any test specification can be used, with or without node information, 
in a large-scale test environment.</p><p><strong>ExclNodes</strong> is a list of nodes to be
excluded from the test. That is, tests that are specified in the test specification 
to run on a particular node are not performed if that node is 
listed in <strong>ExclNodes</strong> at runtime.</p><p>If <strong>Common Test</strong> Master fails initially to connect to any of the test nodes specified in a 
test specification or in the <strong>InclNodes</strong> list, the operator is prompted with 
the option to either start over again (after manually checking the status of the 
nodes in question), to run without the missing nodes, or to abort the operation.</p><p>When tests start, <strong>Common Test</strong> Master displays information to console about the involved nodes.  
<strong>Common Test</strong> Master also reports when tests finish, successfully or unsuccessfully. If
connection is lost to a node, the test on that node is considered finished. <strong>Common Test</strong> Master 
does not attempt to re-establish contact with the failing node.</p><p>At any time, to get the current status of the test nodes, call function 
<a href="./ct_master#progress-0">ct_master#progress-0</a>.</p><p>To stop one or more tests, use function
<a href="./ct_master#abort-0">ct_master#abort-0</a> (to stop all) or
<a href="./ct_master#abort-1">ct_master#abort-1</a>.</p><p>For details about the <strong>Common Test</strong> Master API, see module
<a href="ct_master">ct_master</a>.</p><a name="test_specifications"></a><h3>Test Specifications</h3><p>The test specifications used as input to <strong>Common Test</strong> Master are fully compatible with the
specifications used as input to the regular <strong>Common Test</strong> server. The syntax is described in section 
<a href="./run_test_chapter#test_specifications">Test Specifications</a>
in section Running Tests and Analyzing Results.</p><p>All test specification terms can have a <strong>NodeRefs</strong> element. This element
specifies which node or nodes a configuration operation or a test is to be executed 
on. <strong>NodeRefs</strong> is defined as follows:</p><p><strong>NodeRefs = all_nodes | [NodeRef] | NodeRef</strong></p><p><strong>NodeRef = NodeAlias | node() | master</strong></p><p>A <strong>NodeAlias</strong> (<strong>atom()</strong>) is used in a test specification as a 
reference to a node name (so the node name only needs to be declared once,
which also can be achieved using constants). 
The alias is declared with a <strong>node</strong> term as follows:</p><p><strong>{node, NodeAlias, NodeName}</strong></p><p>If <strong>NodeRefs</strong> has the value <strong>all_nodes</strong>, the operation or test
is performed on all specified test nodes. (Declaring a term without a <strong>NodeRefs</strong> 
element has the same effect). If <strong>NodeRefs</strong> has the value 
<strong>master</strong>, the operation is only performed on the <strong>Common Test</strong> Master node (namely set 
the log directory or install an event handler).</p><p>Consider the example in section 
<a href="./run_test_chapter#test_specifications">Test Specifications</a>
in section Running Tests and Analysing Results,
now extended with node information and intended to be executed by
<strong>Common Test</strong> Master:</p><pre>
 {define, 'Top', "/home/test"}.
 {define, 'T1', "'Top'/t1"}.
 {define, 'T2', "'Top'/t2"}.
 {define, 'T3', "'Top'/t3"}.
 {define, 'CfgFile', "config.cfg"}.
 {define, 'Node', ct_node}.

 {node, node1, 'Node@host_x'}.
 {node, node2, 'Node@host_y'}.

 {logdir, master, "'Top'/master_logs"}.
 {logdir, "'Top'/logs"}.

 {config, node1, "'T1'/'CfgFile'"}.
 {config, node2, "'T2'/'CfgFile'"}.
 {config, "'T3'/'CfgFile'"}.

 {suites, node1, 'T1', all}.
 {skip_suites, node1, 'T1', [t1B_SUITE,t1D_SUITE], "Not implemented"}.
 {skip_cases, node1, 'T1', t1A_SUITE, [test3,test4], "Irrelevant"}.
 {skip_cases, node1, 'T1', t1C_SUITE, [test1], "Ignore"}.

 {suites, node2, 'T2', [t2B_SUITE,t2C_SUITE]}.
 {cases, node2, 'T2', t2A_SUITE, [test4,test1,test7]}.

 {skip_suites, 'T3', all, "Not implemented"}.</pre><p>This example specifies the same tests as the original example. But 
now if started with a call to <strong>ct_master:run(TestSpecName)</strong>, test 
<strong>t1</strong> is executed on node <strong>ct_node@host_x</strong> (<strong>node1</strong>), test
<strong>t2</strong> on <strong>ct_node@host_y</strong> (<strong>node2</strong>) and test <strong>t3</strong>
on both <strong>node1</strong> and <strong>node2</strong>. Configuration file <strong>t1</strong> is only read on
<strong>node1</strong> and configuration file <strong>t2</strong> only on <strong>node2</strong>, while the 
configuration file <strong>t3</strong> is read on both <strong>node1</strong> and <strong>node2</strong>. 
Both test nodes write log files to the same directory. (However, the <strong>Common Test</strong> Master 
node uses a different log directory than the test nodes.)</p><p>If the test session is instead started with a call to 
<strong>ct_master:run(TestSpecName, [ct_node@host_z], [ct_node@host_x])</strong>, 
the result is that test <strong>t1</strong> does not run on 
<strong>ct_node@host_x</strong> (or any other node) while test <strong>t3</strong> runs on both
<strong>ct_node@host_y</strong> and <strong>ct_node@host_z</strong>.</p><p>A nice feature is that a test specification that includes node 
information can still be used as input to the regular <strong>Common Test</strong> server 
(as described in section
<a href="./run_test_chapter#test_specifications">Test Specifications</a>). 
The result is that any test specified to run on a node with the same
name as the <strong>Common Test</strong> node in question (typically <strong>ct@somehost</strong> if started
with the <strong>ct_run</strong> program), is performed. Tests without explicit
node association are always performed too, of course.</p><h3>Automatic Startup of Test Target Nodes</h3><a name="ct_slave"></a><p>Initial actions can be started and performed automatically on
test target nodes using test specification term <strong>init</strong>.</p><p>Two subterms are supported, <strong>node_start</strong> and <strong>eval</strong>.</p><p><em>Example:</em></p><pre>
 {node, node1, node1@host1}.
 {node, node2, node1@host2}.
 {node, node3, node2@host2}.
 {node, node4, node1@host3}.
 {init, node1, [{node_start, [{callback_module, my_slave_callback}]}]}.
 {init, [node2, node3], {node_start, [{username, "ct_user"}, {password, "ct_password"}]}}.
 {init, node4, {eval, {module, function, []}}}.</pre><p>This test specification declares that <strong>node1@host1</strong> is to be started using
the user callback function <strong>callback_module:my_slave_callback/0</strong>, and nodes
<strong>node1@host2</strong> and <strong>node2@host2</strong> are to be started with the default callback
module <strong>ct_slave</strong>. The specified username and password are used to log on to remote
host <strong>host2</strong>. Also, function <strong>module:function/0</strong> is evaluated on
<strong>node1@host3</strong>, and the result of this call is printed to the log.</p><p>The default callback module <a href="ct_slave">ct_slave</a>,
has the following features:
</p><ul><li>Starting Erlang target nodes on local or remote hosts (application <strong>SSH</strong> is used for communication). </li><li>Ability to start an Erlang emulator with more flags (any flags supported by <strong>erl</strong> are supported). </li><li>Supervision of a node being started using internal callback functions. Used to prevent hanging nodes. (Configurable.) </li><li>Monitoring of the master node by the slaves. A slave node can be stopped if the master node terminates. (Configurable.) </li><li>Execution of user functions after a slave node is started. Functions can  be specified as a list of <strong>{Module, Function, Arguments}</strong> tuples. </li></ul><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>An <strong>eval</strong> term for the node and
<strong>startup_functions</strong> in the <strong>node_start</strong> options list can be specified. 
In this case, the node is started first, then the <strong>startup_functions</strong> are
executed, and finally functions specified with <strong>eval</strong> are called.
</p></div><a name="event_handling"></a><h3>General</h3><p>The operator of a <strong>Common Test</strong> system can receive
event notifications continuously during a test run. For example, 
<strong>Common Test</strong> reports when a test case starts and stops, 
the current count of successful, failed, and skipped cases, and so on. 
This information can be used for different purposes such as logging progress 
and results in another format than HTML, saving statistics to a database 
for report generation, and test system supervision.</p><p><strong>Common Test</strong> has a framework for event handling based on
the OTP event manager concept and <strong>gen_event</strong> behavior. 
When the <strong>Common Test</strong> server starts, it spawns an event manager. 
During test execution the manager gets a notification from the server 
when something of potential interest happens. Any event handler plugged into 
the event manager can match on events of interest, take action, or
pass the information on. The event handlers are Erlang modules
implemented by the <strong>Common Test</strong> user according to the <strong>gen_event</strong> 
behavior (for details, see module
<a href="./gen_event">stdlib/gen_event</a> and
section
<a href="./events">doc/design_principles/events</a>
in OTP Design Principles in the System Documentation).
</p><p>A <strong>Common Test</strong> server always starts an event manager. 
The server also plugs in a default event handler, which only
purpose is to relay notifications to a globally registered <strong>Common Test</strong> 
Master event manager (if a <strong>Common Test</strong> Master server is running in the system). 
The <strong>Common Test</strong> Master also spawns an event manager at startup.
Event handlers plugged into this manager receives the events from 
all the test nodes, plus information from the <strong>Common Test</strong> Master server.
</p><p>User-specific event handlers can be plugged into a <strong>Common Test</strong> event
manager, either by telling <strong>Common Test</strong> to install them before the test
run (described later), or by adding the handlers dynamically during the test
run using
<a href="../stdlib/gen_event#add_handler-3">stdlib/gen_event#add_handler-3</a> or
<a href="../stdlib/gen_event#add_sup_handler-3">stdlib/gen_event#add_sup_handler-3</a>.
In the latter scenario, the reference of the <strong>Common Test</strong> event manager is
required. To get it, call 
<a href="./ct#get_event_mgr_ref-0">ct#get_event_mgr_ref-0</a> 
or (on the <strong>Common Test</strong> Master node) 
<a href="./ct_master#get_event_mgr_ref-0">ct_master#get_event_mgr_ref-0</a>.</p><a name="usage"></a><h3>Use</h3><p>Event handlers can be installed by an <strong>event_handler</strong> start flag 
(<a href="ct_run">ct_run</a>) or option 
<a href="./ct#run_test-1">ct#run_test-1</a>, where the
argument specifies the names of one or more event handler modules.</p><p><em>Example:</em></p><p><strong>$ ct_run -suite test/my_SUITE -event_handler handlers/my_evh1  handlers/my_evh2 -pa $PWD/handlers</strong></p><p>To pass start arguments to the event handler init function, use option 
<strong>ct_run -event_handler_init</strong>  instead of
<strong>-event_handler</strong>.</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>All event handler modules must have <strong>gen_event</strong> behavior.
These modules must be precompiled and their locations must be
added explicitly to the Erlang code server search path (as in the previous
example).</p></div><p>An event_handler tuple in argument <strong>Opts</strong> has the following definition 
(see <a href="./ct#run_test-1">ct#run_test-1</a>):</p><pre>
 {event_handler,EventHandlers}

 EventHandlers = EH | [EH]
 EH = atom() | {atom(),InitArgs} | {[atom()],InitArgs}
 InitArgs = [term()]</pre><p>In the following example, two event handlers for the <strong>my_SUITE</strong> test are installed:</p><pre>
 1&gt; ct:run_test([{suite,"test/my_SUITE"},{event_handler,[my_evh1,{my_evh2,[node()]}]}]).</pre><p>Event handler <strong>my_evh1</strong> is started with <strong>[]</strong> as argument to the init function. 
Event handler <strong>my_evh2</strong> is started with the name of the current node in the init argument list.</p><p>Event handlers can also be plugged in using one of the following
<a href="./run_test_chapter#test_specifications">test specification</a> 
terms:</p><ul><li><strong>{event_handler, EventHandlers}</strong></li><li><strong>{event_handler, EventHandlers, InitArgs}</strong></li><li><strong>{event_handler, NodeRefs, EventHandlers}</strong></li><li><strong>{event_handler, NodeRefs, EventHandlers, InitArgs}</strong></li></ul><p><strong>EventHandlers</strong> is a list of module names. Before a test 
session starts, the init function of each plugged in event handler 
is called (with the <strong>InitArgs</strong> list as argument or <strong>[]</strong> if
no start arguments are specified).</p><p>To plug in a handler to the <strong>Common Test</strong> Master event manager, specify 
<strong>master</strong> as the node in <strong>NodeRefs</strong>.</p><p>To be able to match on events, the event handler module must
include the header file <strong>ct_event.hrl</strong>. An event is a record with the
following definition:</p><p><strong>#event{name, node, data}</strong></p><dl><dt><strong>name</strong></dt><dd><p>Label (type) of the event.</p></dd><dt><strong>node</strong></dt><dd><p>Name of the node that the event originated from 
(only relevant for <strong>Common Test</strong> Master event handlers).</p></dd><dt><strong>data</strong></dt><dd><p>Specific for the event.</p></dd></dl><a name="events"></a><h3>General Events</h3><p>The general events are as follows:</p><dl><dt><strong>#event{name = start_logging, data = LogDir}</strong></dt><dd> <p><strong>LogDir = string()</strong>, top-level log directory for the test run.</p> <p>This event indicates that the logging process of <strong>Common Test</strong>
has started successfully and is ready to receive I/O
messages.</p></dd><dt><strong>#event{name = stop_logging, data = []}</strong></dt><dd> <p>This event indicates that the logging process of <strong>Common Test</strong>
was shut down at the end of the test run.
</p></dd><dt><strong>#event{name = test_start, data = {StartTime,LogDir}}</strong></dt><dd> <p><strong>StartTime = {date(),time()}</strong>, test run start date and time.</p> <p><strong>LogDir = string()</strong>, top-level log directory for the test run.</p> <p>This event indicates that <strong>Common Test</strong> has finished initial preparations
and begins executing test cases.
</p></dd><dt><strong>#event{name = test_done, data = EndTime}</strong></dt><dd> <p><strong>EndTime = {date(),time()}</strong>, date and time the test run finished.</p> <p>This event indicates that the last test case has been executed and 
<strong>Common Test</strong> is shutting down.	
</p></dd><dt><strong>#event{name = start_info, data = {Tests,Suites,Cases}}</strong></dt><dd> <p><strong>Tests = integer()</strong>, number of tests.</p> <p><strong>Suites = integer()</strong>, total number of suites.</p> <p><strong>Cases = integer() | unknown</strong>, total number of test cases.</p> <p>This event gives initial test run information that can be interpreted as: 
"This test run will execute <strong>Tests</strong> separate tests, in total containing
<strong>Cases</strong> number of test cases, in <strong>Suites</strong> number of suites".
However, if a test case group with a repeat property exists in any test, 
the total number of test cases cannot be calculated (unknown).
</p></dd><dt><strong>#event{name = tc_start, data = {Suite,FuncOrGroup}}</strong></dt><dd> <p><strong>Suite = atom()</strong>, name of the test suite.</p> <p><strong>FuncOrGroup = Func | {Conf,GroupName,GroupProperties}</strong></p> <p><strong>Func = atom()</strong>, name of test case or configuration function.</p> <p><strong>Conf = init_per_group | end_per_group</strong>, group configuration function.</p> <p><strong>GroupName = atom()</strong>, name of the group.</p> <p><strong>GroupProperties = list()</strong>, list of execution properties for the group.</p> <p>This event informs about the start of a test case, or a group configuration
function. The event is sent also for <strong>init_per_suite</strong> and <strong>end_per_suite</strong>,
but not for <strong>init_per_testcase</strong> and <strong>end_per_testcase</strong>. If a group
configuration function starts, the group name and execution properties
are also specified.
</p></dd><dt><strong>#event{name = tc_logfile, data = {{Suite,Func},LogFileName}}</strong></dt><dd> <p><strong>Suite = atom()</strong>, name of the test suite.</p> <p><strong>Func = atom()</strong>, name of test case or configuration function.</p> <p><strong>LogFileName = string()</strong>, full name of the test case log file.</p> <p>This event is sent at the start of each test case (and configuration function
except <strong>init/end_per_testcase</strong>) and carries information about the
full name (that is, the file name including the absolute directory path) of
the current test case log file.
</p></dd><dt><strong>#event{name = tc_done, data = {Suite,FuncOrGroup,Result}}</strong></dt><dd> <a name="tc_done"></a> <p><strong>Suite = atom()</strong>, name of the suite.</p> <p><strong>FuncOrGroup = Func | {Conf,GroupName,GroupProperties}</strong></p> <p><strong>Func = atom()</strong>, name of test case or configuration function.</p> <p><strong>Conf = init_per_group | end_per_group</strong>, group configuration function.</p> <p><strong>GroupName = unknown | atom()</strong>, name of the group 
(unknown if init- or end function times out).</p> <p><strong>GroupProperties = list()</strong>, list of execution properties for the group.</p> <p><strong>Result = ok | {auto_skipped,SkipReason} | {skipped,SkipReason} | {failed,FailReason}</strong>,
the result.</p> <a name="skipreason"></a> <p><strong>SkipReason = {require_failed,RequireInfo} |  {require_failed_in_suite0,RequireInfo} |  {failed,{Suite,init_per_testcase,FailInfo}} |  UserTerm</strong>, 
why the case was skipped.</p> <a name="failreason"></a> <p><strong>FailReason = {error,FailInfo} |  {error,{RunTimeError,StackTrace}} |  {timetrap_timeout,integer()} |  {failed,{Suite,end_per_testcase,FailInfo}}</strong>, reason for failure.</p>	 <p><strong>RequireInfo = {not_available,atom() | tuple()}</strong>, why require failed.</p> <p><strong>FailInfo = {timetrap_timeout,integer()} |  {RunTimeError,StackTrace} |  UserTerm</strong>, 
error details.</p> <p><strong>RunTimeError = term()</strong>, a runtime error, for example, 
<strong>badmatch</strong> or <strong>undef</strong>.</p> <p><strong>StackTrace = list()</strong>, list of function calls preceding a runtime error.</p> <p><strong>UserTerm = term()</strong>, any data specified by user, or <strong>exit/1</strong> information.</p> <p>This event informs about the end of a test case or a configuration function (see event 
<strong>tc_start</strong> for details on element <strong>FuncOrGroup</strong>). With this event 
comes the final result of the function in question. It is possible to determine on the 
top level of <strong>Result</strong> if the function was successful, skipped (by the user), 
or if it failed.</p> <p>It is also possible to dig deeper and, for example, perform pattern matching 
on the various reasons for skipped or failed. Notice that <strong>{'EXIT',Reason}</strong> tuples 
are translated into <strong>{error,Reason}</strong>. 
Notice also that if a <strong>{failed,{Suite,end_per_testcase,FailInfo}</strong>
result is received, the test case was successful, but 
<strong>end_per_testcase</strong> for the case failed.
</p></dd><dt><strong>#event{name = tc_auto_skip, data = {Suite,TestName,Reason}}</strong></dt><dd> <a name="tc_auto_skip"></a> <p><strong>Suite = atom()</strong>, the name of the suite.</p> <p><strong>TestName = init_per_suite | end_per_suite | {init_per_group,GroupName} | {end_per_group,GroupName} | {FuncName,GroupName} | FuncName</strong></p> <p><strong>FuncName = atom()</strong>, the name of the test case or configuration function.</p> <p><strong>GroupName = atom()</strong>, the name of the test case group.</p> <p><strong>Reason = {failed,FailReason} | {require_failed_in_suite0,RequireInfo}</strong>, 
reason for auto-skipping <strong>Func</strong>.</p> <p><strong>FailReason = {Suite,ConfigFunc,FailInfo}} |  {Suite,FailedCaseInSequence}</strong>, reason for failure.</p>	 <p><strong>RequireInfo = {not_available,atom() | tuple()}</strong>, why require failed.</p> <p><strong>ConfigFunc = init_per_suite | init_per_group</strong></p> <p><strong>FailInfo = {timetrap_timeout,integer()} |  {RunTimeError,StackTrace} | bad_return | UserTerm</strong>, 
error details.</p> <p><strong>FailedCaseInSequence = atom()</strong>, the name of a case that failed in a sequence.</p> <p><strong>RunTimeError = term()</strong>, a runtime error, for example <strong>badmatch</strong> or
<strong>undef</strong>.</p> <p><strong>StackTrace = list()</strong>, list of function calls preceeding a runtime error.</p> <p><strong>UserTerm = term()</strong>, any data specified by user, or <strong>exit/1</strong> information.</p> <p>This event is sent for every test case or configuration function that <strong>Common Test</strong>
has skipped automatically because of either a failed <strong>init_per_suite</strong> or 
<strong>init_per_group</strong>, a failed <strong>require</strong> in <strong>suite/0</strong>, or a failed test case
in a sequence. Notice that this event is never received as a result of a test case getting
skipped because of <strong>init_per_testcase</strong> failing, as that information is carried with
event <strong>tc_done</strong>. If a failed test case belongs to a test case group, the second
data element is a tuple <strong>{FuncName,GroupName}</strong>, otherwise only the function name.
</p></dd><dt><strong>#event{name = tc_user_skip, data = {Suite,TestName,Comment}}</strong></dt><dd> <a name="tc_user_skip"></a>   <p><strong>Suite = atom()</strong>, the name of the suite.</p> <p><strong>TestName = init_per_suite | end_per_suite | {init_per_group,GroupName} | {end_per_group,GroupName} | {FuncName,GroupName} | FuncName</strong></p> <p><strong>FuncName = atom()</strong>, the name of the test case or configuration function.</p> <p><strong>GroupName = atom()</strong>, the name of the test case group.</p> <p><strong>Comment = string()</strong>, why the test case was skipped.</p> <p>This event specifies that a test case was skipped by the user. 
It is only received if the skip is declared in a test specification. 
Otherwise, user skip information is received as a <strong>{skipped,SkipReason}</strong> 
result in event <strong>tc_done</strong> for the test case. If a skipped test case belongs
to a test case group, the second data element is a tuple <strong>{FuncName,GroupName}</strong>,
otherwise only the function name.
</p></dd><dt><strong>#event{name = test_stats, data = {Ok,Failed,Skipped}}</strong></dt><dd> <p><strong>Ok = integer()</strong>, current number of successful test cases.</p> <p><strong>Failed = integer()</strong>, current number of failed test cases.</p> <p><strong>Skipped = {UserSkipped,AutoSkipped}</strong></p> <p><strong>UserSkipped = integer()</strong>, current number of user-skipped test cases.</p> <p><strong>AutoSkipped = integer()</strong>, current number of auto-skipped test cases.</p> <p>This is a statistics event with current count of successful, skipped, 
and failed test cases so far. This event is sent after the end of each test case,
immediately following event <strong>tc_done</strong>.
</p></dd></dl><h3>Internal Events</h3><p>The internal events are as follows:</p><dl><dt><strong>#event{name = start_make, data = Dir}</strong></dt><dd> <p><strong>Dir = string()</strong>, running make in this directory.</p> <p>This internal event says that <strong>Common Test</strong> starts compiling
modules in directory <strong>Dir</strong>.
</p></dd><dt><strong>#event{name = finished_make, data = Dir}</strong></dt><dd> <p><strong>Dir = string()</strong>, finished running make in this directory.</p> <p>This internal event says that <strong>Common Test</strong> is finished compiling
modules in directory <strong>Dir</strong>.
</p></dd><dt><strong>#event{name = start_write_file, data = FullNameFile}</strong></dt><dd> <p><strong>FullNameFile = string(), full name of the file.</strong></p> <p>This internal event is used by the <strong>Common Test</strong> Master process to 
synchronize particular file operations.
</p></dd><dt><strong>#event{name = finished_write_file, data = FullNameFile}</strong></dt><dd> <p><strong>FullNameFile = string(), full name of the file.</strong></p> <p>This internal event is used by the <strong>Common Test</strong> Master process to 
synchronize particular file operations.
</p></dd></dl><h3>Notes</h3><p>The events are also documented in <strong>ct_event.erl</strong>. This module
can serve as an example of what an event handler for the <strong>Common Test</strong> event 
manager can look like.</p><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>To ensure that printouts to <strong>stdout</strong> (or printouts made with
<a href="./ct#log-2">ct#log-2</a> or 
<a href="./ct#pal-2">ct#pal-2</a>) get written to the test case log
file, and not to the <strong>Common Test</strong> framework log, you can synchronize
with the <strong>Common Test</strong> server by matching on evvents <strong>tc_start</strong> and <strong>tc_done</strong>.
In the period between these events, all I/O is directed to the
test case log file. These events are sent synchronously to avoid potential
timing problems (for example, that the test case log file is closed just before
an I/O message from an external process gets through). Knowing this, you
need to be careful that your <strong>handle_event/2</strong> callback function does not
stall the test execution, possibly causing unexpected behavior as a result.</p></div><h3>General</h3><p>When creating test suites, it is strongly recommended to not
create dependencies between test cases, that is, letting test cases
depend on the result of previous test cases. There are various
reasons for this, such as, the following:</p><ul><li>It makes it impossible to run test cases individually.</li><li>It makes it impossible to run test cases in a different order.</li><li>It makes debugging difficult (as a fault can be the result of a problem in a different test case than the one failing).</li><li>There are no good and explicit ways to declare dependencies, so  it can be difficult to see and understand these in test suite  code and in test logs.</li><li>Extending, restructuring, and maintaining test suites with  test case dependencies is difficult.</li></ul><p>There are often sufficient means to work around the need for test 
case dependencies. Generally, the problem is related to the state of 
the System Under Test (SUT). The action of one test case can change the 
system state. For some other test case to run properly, this new state 
must be known.</p><p>Instead of passing data between test cases, it is recommended
that the test cases read the state from the SUT and perform assertions
(that is, let the test case run if the state is as expected, otherwise reset or fail).
It is also recommended to use the state to set variables necessary for the 
test case to execute properly. Common actions can often be implemented as 
library functions for test cases to call to set the SUT in a required state. 
(Such common actions can also be separately tested, if necessary,
to ensure that they work as expected). It is sometimes also possible, 
but not always desirable, to group tests together in one test case, that is,
let a test case perform a "scenario" test (a test consisting of subtests).</p><p>Consider, for example, a server application under test. The following 
functionality is to be tested:</p><ul><li>Starting the server</li><li>Configuring the server</li><li>Connecting a client to the server</li><li>Disconnecting a client from the server</li><li>Stopping the server</li></ul><p>There are obvious dependencies between the listed functions. The server cannot 
be configured if it has not first been started, a client connot be connectd until 
the server is properly configured, and so on. If we want to have one test 
case for each function, we might be tempted to try to always run the
test cases in the stated order and carry possible data (identities, handles,
and so on) between the cases and therefore introduce dependencies between them.</p><p>To avoid this, we can consider starting and stopping the server for every test.
We can thus implement the start and stop action as common functions to be 
called from 
<a href="./common_test#Module:init_per_testcase-2">common_test#Module:init_per_testcase-2</a> and 
<a href="./common_test#Module:end_per_testcase-2">common_test#Module:end_per_testcase-2</a>. 
(Remember to test the start and stop functionality separately.) 
The configuration can also be implemented as a common function, maybe grouped 
with the start function. Finally, the testing of connecting and disconnecting a 
client can be grouped into one test case. The resulting suite can look as
follows:</p><pre>      
 -module(my_server_SUITE).
 -compile(export_all).
 -include_lib("ct.hrl").

 %%% init and end functions...

 suite() -&gt; [{require,my_server_cfg}].

 init_per_testcase(start_and_stop, Config) -&gt;
     Config;

 init_per_testcase(config, Config) -&gt;
     [{server_pid,start_server()} | Config];

 init_per_testcase(_, Config) -&gt;
     ServerPid = start_server(),
     configure_server(),
     [{server_pid,ServerPid} | Config].

 end_per_testcase(start_and_stop, _) -&gt;
     ok;

 end_per_testcase(_, _) -&gt;
     ServerPid = ?config(server_pid),
     stop_server(ServerPid).

 %%% test cases...

 all() -&gt; [start_and_stop, config, connect_and_disconnect].

 %% test that starting and stopping works
 start_and_stop(_) -&gt;
     ServerPid = start_server(),
     stop_server(ServerPid).

 %% configuration test
 config(Config) -&gt;
     ServerPid = ?config(server_pid, Config),
     configure_server(ServerPid).

 %% test connecting and disconnecting client
 connect_and_disconnect(Config) -&gt;
     ServerPid = ?config(server_pid, Config),
     {ok,SessionId} = my_server:connect(ServerPid),
     ok = my_server:disconnect(ServerPid, SessionId).

 %%% common functions...

 start_server() -&gt;
     {ok,ServerPid} = my_server:start(),
     ServerPid.

 stop_server(ServerPid) -&gt;
     ok = my_server:stop(),
     ok.

 configure_server(ServerPid) -&gt;
     ServerCfgData = ct:get_config(my_server_cfg),
     ok = my_server:configure(ServerPid, ServerCfgData),
     ok.</pre><a name="save_config"></a><h3>Saving Configuration Data</h3><p>Sometimes it is impossible, or infeasible, to
implement independent test cases. Maybe it is not possible to read the 
SUT state. Maybe resetting the SUT is impossible and it takes too long time
to restart the system. In situations where test case dependency is necessary,
CT offers a structured way to carry data from one test case to the next. The
same mechanism can also be used to carry data from one test suite to the next.</p><p>The mechanism for passing data is called <strong>save_config</strong>. The idea is that
one test case (or suite) can save the current value of <strong>Config</strong>, or any list of
key-value tuples, so that the next executing test case (or test suite) can read it. 
The configuration data is not saved permanently but can only be passed from one 
case (or suite) to the next.</p><p>To save <strong>Config</strong> data, return tuple <strong>{save_config,ConfigList}</strong>
from <strong>end_per_testcase</strong> or from the main test case function.</p><p>To read data saved by a previous test case, use macro <strong>config</strong> with a 
<strong>saved_config</strong> key as follows:</p><p><strong>{Saver,ConfigList} = ?config(saved_config, Config)</strong></p><p><strong>Saver</strong> (<strong>atom()</strong>) is the name of the previous test case (where the
data was saved). The <strong>config</strong> macro can be used to extract particular data
also from the recalled <strong>ConfigList</strong>. It is strongly recommended that 
<strong>Saver</strong> is always matched to the expected name of the saving test case. 
This way, problems because of restructuring of the test suite can be avoided. 
Also, it makes the dependency more explicit and the test suite easier to read 
and maintain.</p><p>To pass data from one test suite to another, the same mechanism is used. The data
is to be saved by finction
<a href="./common_test#Module:end_per_suite-1">common_test#Module:end_per_suite-1</a> 
and read by function
<a href="./common_test#Module:init_per_suite-1">common_test#Module:init_per_suite-1</a>
in the suite that follows. When passing data between suites, <strong>Saver</strong> carries the 
name of the test suite.</p><p><em>Example:</em></p><pre>
 -module(server_b_SUITE).
 -compile(export_all).
 -include_lib("ct.hrl").

 %%% init and end functions...

 init_per_suite(Config) -&gt;
     %% read config saved by previous test suite
     {server_a_SUITE,OldConfig} = ?config(saved_config, Config),
     %% extract server identity (comes from server_a_SUITE)
     ServerId = ?config(server_id, OldConfig),
     SessionId = connect_to_server(ServerId),
     [{ids,{ServerId,SessionId}} | Config].

 end_per_suite(Config) -&gt;
     %% save config for server_c_SUITE (session_id and server_id)
     {save_config,Config}

 %%% test cases...

 all() -&gt; [allocate, deallocate].

 allocate(Config) -&gt;
     {ServerId,SessionId} = ?config(ids, Config),
     {ok,Handle} = allocate_resource(ServerId, SessionId),
     %% save handle for deallocation test
     NewConfig = [{handle,Handle}],
     {save_config,NewConfig}.

 deallocate(Config) -&gt;
     {ServerId,SessionId} = ?config(ids, Config),
     {allocate,OldConfig} = ?config(saved_config, Config),
     Handle = ?config(handle, OldConfig),
     ok = deallocate_resource(ServerId, SessionId, Handle).</pre><p>To save <strong>Config</strong> data from a test case that is to be
skipped, return tuple 
<strong>{skip_and_save,Reason,ConfigList}</strong>.</p><p>The result is that the test case is skipped with <strong>Reason</strong> printed to
the log file (as described earlier) and <strong>ConfigList</strong> is saved 
for the next test case. <strong>ConfigList</strong> can be read using 
<strong>?config(saved_config, Config)</strong>, as described earlier. <strong>skip_and_save</strong>
can also be returned from <strong>init_per_suite</strong>. In this case, the saved data can
be read by <strong>init_per_suite</strong> in the suite that follows.</p><a name="sequences"></a><h3>Sequences</h3><p>Sometimes test cases depend on each other so that
if one case fails, the following tests are not to be executed.
Typically, if the <strong>save_config</strong> facility is used and a test 
case that is expected to save data crashes, the following 
case cannot run. <strong>Common Test</strong> offers a way to declare such dependencies, 
called sequences.</p><p>A sequence of test cases is defined as a test case group
with a <strong>sequence</strong> property. Test case groups are defined
through function <strong>groups/0</strong> in the test suite (for details, see section
<a href="./write_test_chapter#test_case_groups">Test Case Groups</a>.</p><p>For example, to ensure that if <strong>allocate</strong>
in <strong>server_b_SUITE</strong> crashes, <strong>deallocate</strong> is skipped,
the following sequence can be defined:</p><pre>
 groups() -&gt; [{alloc_and_dealloc, [sequence], [alloc,dealloc]}].</pre><p>Assume that the suite contains the test case <strong>get_resource_status</strong> 
that is independent of the other two cases, then function <strong>all</strong> can 
look as follows:</p><pre>
 all() -&gt; [{group,alloc_and_dealloc}, get_resource_status].</pre><p>If <strong>alloc</strong> succeeds, <strong>dealloc</strong> is also executed. If <strong>alloc</strong> fails
however, <strong>dealloc</strong> is not executed but marked as <strong>SKIPPED</strong> in the HTML log. 
<strong>get_resource_status</strong> runs no matter what happens to the <strong>alloc_and_dealloc</strong>
cases.</p><p>Test cases in a sequence are executed in order until all succeed or 
one fails. If one fails, all following cases in the sequence are skipped.
The cases in the sequence that have succeeded up to that point are reported as 
successful in the log. Any number of sequences can be specified.</p><p><em>Example:</em></p><pre>
 groups() -&gt; [{scenarioA, [sequence], [testA1, testA2]}, 
              {scenarioB, [sequence], [testB1, testB2, testB3]}].

 all() -&gt; [test1, 
           test2, 
           {group,scenarioA}, 
	   test3, 
           {group,scenarioB}, 
           test4].</pre><p>A sequence group can have subgroups. Such subgroups can have 
any property, that is, they are not required to also be sequences. If you want the 
status of the subgroup to affect the sequence on the level above, return 
<strong>{return_group_result,Status}</strong> from 
<a href="./common_test#Module:end_per_group-2">common_test#Module:end_per_group-2</a>, 
as described in section
<a href="./write_test_chapter#repeated_groups">Repeated Groups</a>
in Writing Test Suites.
A failed subgroup (<strong>Status == failed</strong>) causes the execution of a 
sequence to fail in the	same way a test case does.</p><a name="general"></a><h3>General</h3><p>
The <em>Common Test Hook (CTH)</em> framework allows 
extensions of the default behavior of <strong>Common Test</strong> using hooks 
before and after all test suite calls. CTHs allow advanced <strong>Common Test</strong>
users to abstract out behavior that is common to multiple test suites
without littering all test suites with library calls. This can be used
for logging, starting, and monitoring external systems, 
building C files needed by the tests, and so on.</p><p>In brief, CTH allows you to do the following:</p><ul><li>Manipulate the runtime configuration before each suite  configuration call.</li><li>Manipulate the return of all suite configuration calls, and in  extension, the result of the tests themselves.</li></ul><p>The following sections describe how to use CTHs, when they are run,
and how to manipulate the test results in a CTH.</p><div class="alert alert-warning"><h4 class="alert-heading">Warning</h4><p>When executing within a CTH, all timetraps are shut off. So
if your CTH never returns, the entire test run is stalled.</p></div><a name="installing"></a><h3>Installing a CTH</h3><p>A CTH can be installed in multiple ways in your test run. You can do it
for all tests in a run, for specific test suites, and for specific groups 
within a test suite. If you want a CTH to be present in all test suites 
within your test run, there are three ways to accomplish that, as follows:
</p><ul><li>Add <strong>-ct_hooks</strong> as an argument to  <a href="./run_test_chapter#ct_run">ct_run</a>.  To add multiple CTHs using this method, append them to each other using the keyword <strong>and</strong>, that is,  <strong>ct_run -ct_hooks cth1 [{debug,true}] and cth2 ...</strong>.</li><li>Add tag <strong>ct_hooks</strong> to your  <a href="./run_test_chapter#test_specifications"> Test Specification</a>.</li><li>Add tag <strong>ct_hooks</strong> to your call to  <a href="./ct#run_test-1">ct:run_test/1</a>.</li></ul><p>CTHs can also be added within a test suite. This is done by returning
<strong>{ct_hooks,[CTH]}</strong> in the configuration list from 
<a href="./common_test#Module:suite-0">suite/0</a>,
<a href="./common_test#Module:init_per_suite-1"> init_per_suite/1</a>, or
<a href="./common_test#Module:init_per_group-2"> init_per_group/2</a>.</p><p>In this case, <strong>CTH</strong> can either be only the module name of the CTH 
or a tuple with the module name and the initial arguments, and optionally 
the hook priority of the CTH. For example, one of the following:</p><ul><li><strong>{ct_hooks,[my_cth_module]}</strong></li><li><strong>{ct_hooks,[{my_cth_module,[{debug,true}]}]}</strong></li><li><strong>{ct_hooks,[{my_cth_module,[{debug,true}],500}]}</strong></li></ul><h3>Overriding CTHs</h3><p>By default, each installation of a CTH causes a new instance of it
to be activated. This can cause problems if you want to override 
CTHs in test specifications while still having them in the
suite information function. The 
<a href="./ct_hooks#Module:id-1">id/1</a>
callback exists to address this problem. By returning the same
<strong>id</strong> in both places, <strong>Common Test</strong> knows that this CTH
is already installed and does not try to install it again.</p><h3>CTH Execution Order</h3><p>By default, each CTH installed is executed in the order that
they are installed for init calls, and then reversed for end calls.
This is not always desired, so <strong>Common Test</strong> allows
the user to specify a priority for each hook. The priority can either
be specified in the CTH function 
<a href="./ct_hooks#Module:init-2">init/2</a> or when 
installing the hook. The priority specified at installation overrides the 
priority returned by the CTH.</p><a name="scope"></a><h3>CTH Scope</h3><p>Once the CTH is installed into a certain test run it remains there until
its scope is expired. The scope of a CTH depends on when it is 
installed, see the following table.
Function <a href="./ct_hooks#Module:init-2">init/2</a> is 
called at the beginning of the scope and function
<a href="./ct_hooks#Module:terminate-1">terminate/1</a> 
is called when the scope ends.</p><table class="table table-bordered table-hover table-striped"><caption>Scope of a CTH</caption><tbody><tr><td><em>CTH installed in</em></td><td><em>CTH scope begins before</em></td><td><em>CTH scope ends after</em></td></tr><tr><td><a href="./run_test_chapter#ct_run">ct_run</a></td><td>the first test suite is to be run</td><td>the last test suite has been run</td></tr><tr><td><a href="./ct#run_test-1">ct:run_test</a></td><td>the first test suite is run</td><td>the last test suite has been run</td></tr><tr><td><a href="./run_test_chapter#test_specifications"> Test Specification</a></td><td>the first test suite is run</td><td>the last test suite has been run</td></tr><tr><td><a href="./common_test#Module:suite-0">suite/0 </a></td><td><a href="./ct_hooks#Module:pre_init_per_suite-3"> pre_init_per_suite/3</a> is called</td><td><a href="./ct_hooks#Module:post_end_per_suite-4"> post_end_per_suite/4</a> has been called for that test suite</td></tr><tr><td><a href="./common_test#Module:init_per_suite-1"> init_per_suite/1</a></td><td><a href="./ct_hooks#Module:post_init_per_suite-4"> post_init_per_suite/4</a> is called</td><td><a href="./ct_hooks#Module:post_end_per_suite-4"> post_end_per_suite/4</a> has been called for that test suite</td></tr><tr><td><a href="./common_test#Module:init_per_group-2"> init_per_group/2</a></td><td><a href="./ct_hooks#Module:post_init_per_group-5"> post_init_per_group/5</a> is called</td><td><a href="./ct_hooks#Module:post_end_per_group-5"> post_end_per_group/5</a> has been called for that group</td></tr></tbody></table><h3>CTH Processes and Tables</h3><p>CTHs are run with the same process scoping as normal test suites,
that is, a different process executes the <strong>init_per_suite</strong> hooks then the
<strong>init_per_group</strong> or <strong>per_testcase</strong> hooks. So if you want to spawn a 
process in the CTH, you cannot link with the CTH process, as it exits 
after the post hook ends. Also, if you for some reason need an ETS 
table with your CTH, you must spawn a process that handles it.</p><h3>External Configuration Data and Logging</h3><p>Configuration data values in the CTH can be read
by calling 
<a href="./ct#get_config-1">ct#get_config-1</a> 
(as explained in section
<a href="./config_file_chapter#require_config_data">Requiring and Reading Configuration Data</a>).
The configuration variables in question must, as always, first have been
required by a suite-, group-, or test case information function,
or by function <a href="./ct#require-1">ct#require-1</a>.
The latter can also be used in CT hook functions.</p><p>The CT hook functions can call any logging function
in the <strong>ct</strong> interface to print information to the log files, or to
add comments in the suite overview page.
</p><a name="manipulating"></a><h3>Manipulating Tests</h3><p>Through CTHs the results of tests and configuration functions can be manipulated. 
The main purpose to do this with CTHs is to allow common 
patterns to be abstracted out from test suites and applied to
multiple test suites without duplicating any code. All the callback
functions for a CTH follow a common interface described hereafter.</p><p><strong>Common Test</strong> always calls all available hook functions, even pre- 
and post hooks for configuration functions that are not implemented in the suite.
For example, <strong>pre_init_per_suite(x_SUITE, ...)</strong> and
<strong>post_init_per_suite(x_SUITE, ...)</strong> are called for test suite
<strong>x_SUITE</strong>, even if it does not export <strong>init_per_suite/1</strong>. 
With this feature hooks can be used as configuration fallbacks, and all
configuration functions can be replaced with hook functions.</p><a name="pre"></a><h3>Pre Hooks</h3><p>
In a CTH, the behavior can be hooked in before the following functions:</p><ul><li><a href="./common_test#Module:init_per_suite-1">common_test#Module:init_per_suite-1</a></li><li><a href="./common_test#Module:init_per_group-2">common_test#Module:init_per_group-2</a></li><li><a href="./common_test#Module:init_per_testcase-2">common_test#Module:init_per_testcase-2</a></li><li><a href="./common_test#Module:end_per_testcase-2">common_test#Module:end_per_testcase-2</a></li><li><a href="./common_test#Module:end_per_group-2">common_test#Module:end_per_group-2</a></li><li><a href="./common_test#Module:end_per_suite-1">common_test#Module:end_per_suite-1</a></li></ul><p>
This is done in the CTH functions called <strong>pre_&lt;name of function&gt;</strong>.
These functions take the arguments <strong>SuiteName</strong>, <strong>Name</strong> (group or test case name, if applicable), 
<strong>Config</strong>, and <strong>CTHState</strong>. The return value of the CTH function
is always a combination of a result for the suite/group/test and an 
updated <strong>CTHState</strong>.</p><p>To let the test suite continue on executing, return the configuration 
list that you want the test to use as the result.</p><p>All pre hooks, except <strong>pre_end_per_testcase/4</strong>, can
skip or fail the test by returning a tuple with <strong>skip</strong> or
<strong>fail</strong>, and a reason as the result.</p><p><em>Example:</em></p><pre><code class="">
 pre_init_per_suite(SuiteName, Config, CTHState) -&gt;
   case db:connect() of
     {error,_Reason} -&gt;
       {{fail, "Could not connect to DB"}, CTHState};
     {ok, Handle} -&gt;
       {[{db_handle, Handle} | Config], CTHState#state{ handle = Handle }}
   end.</code></pre><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>If you use multiple CTHs, the first part of the return tuple is
used as input for the next CTH. So in the previous example the next CTH can
get <strong>{fail,Reason}</strong> as the second parameter. If you have many CTHs
interacting, do not let each CTH return <strong>fail</strong> or <strong>skip</strong>. 
Instead, return that an action is to be taken through the <strong>Config</strong> 
list and implement a CTH that, at the end, takes the correct action.</p></div><a name="post"></a><h3>Post Hooks</h3><p>In a CTH, behavior can be hooked in after the following functions:</p><ul><li><a href="./common_test#Module:init_per_suite-1">common_test#Module:init_per_suite-1</a></li><li><a href="./common_test#Module:init_per_group-2">common_test#Module:init_per_group-2</a></li><li><a href="./common_test#Module:init_per_testcase-2">common_test#Module:init_per_testcase-2</a></li><li><a href="./common_test#Module:end_per_testcase-2">common_test#Module:end_per_testcase-2</a></li><li><a href="./common_test#Module:end_per_group-2">common_test#Module:end_per_group-2</a></li><li><a href="./common_test#Module:end_per_suite-1">common_test#Module:end_per_suite-1</a></li></ul><p>
This is done in the CTH functions called <strong>post_&lt;name of function&gt;</strong>. 
These functions take the arguments <strong>SuiteName</strong>, <strong>Name</strong> (group or test case name, if applicable),
<strong>Config</strong>, <strong>Return</strong>, and <strong>CTHState</strong>. <strong>Config</strong> in this
case is the same <strong>Config</strong> as the testcase is called with. 
<strong>Return</strong> is the value returned by the testcase. If the testcase 
fails by crashing, <strong>Return</strong> is
<strong>{'EXIT',{{Error,Reason},Stacktrace}}</strong>.</p><p>The return value of the CTH function is always a combination of a
result for the suite/group/test and an updated <strong>CTHState</strong>. If
you do not want the callback to affect the outcome of the test,
return the <strong>Return</strong> data as it is given to the CTH. You can also
modify the test result. By returning the <strong>Config</strong> list
with element <strong>tc_status</strong> removed, you can recover from a test 
failure. As in all the pre hooks, it is also possible to fail/skip
the test case in the post hook.</p><p><em>Example:</em></p><pre><code class="">
 post_end_per_testcase(_Suite, _TC, Config, {'EXIT',{_,_}}, CTHState) -&gt;
   case db:check_consistency() of
     true -&gt;
       %% DB is good, pass the test.
       {proplists:delete(tc_status, Config), CTHState};
     false -&gt;
       %% DB is not good, mark as skipped instead of failing
       {{skip, "DB is inconsisten!"}, CTHState}
   end;
 post_end_per_testcase(_Suite, _TC, Config, Return, CTHState) -&gt;
   %% Do nothing if tc does not crash.
   {Return, CTHState}.</code></pre><div class="alert alert-info"><h4 class="alert-heading">Note</h4><p>Do recover from a testcase failure using CTHs only a last resort. 
If used wrongly, it can be very difficult to determine which tests that 
pass or fail in a test run.</p></div><h3>Skip and Fail Hooks</h3><p>
After any post hook has been executed for all installed CTHs, 
<a href="./ct_hooks#Module:on_tc_fail-4">on_tc_fail</a>
or <a href="./ct_hooks#Module:on_tc_skip-4">on_tc_skip</a>
is called if the testcase failed or was skipped, respectively. 
You cannot affect the outcome of the tests any further at this point. 
</p><a name="synchronizing"></a><h3>Synchronizing External User Applications with Common Test</h3><p>CTHs can be used to synchronize test runs with external user applications.
The init function can, for example, start and/or communicate with an application that
has the purpose of preparing the SUT for an upcoming test run, or 
initialize a database for saving test data to during the test run. The
terminate function can similarly order such an application to reset the SUT
after the test run, and/or tell the application to finish active sessions
and terminate.
Any system error- or progress reports generated during the init- or
termination stage are saved in the 
<a href="./run_test_chapter#pre_post_test_io_log">Pre- and Post Test I/O Log</a>. 
(This is also true for any printouts made
with <strong>ct:log/2</strong> and <strong>ct:pal/2</strong>).</p><p>To ensure that <strong>Common Test</strong> does not start executing tests, or
closes its log files and shuts down, before the external application
is ready for it, <strong>Common Test</strong> can be synchronized with the application. 
During startup and shutdown, <strong>Common Test</strong> can be suspended, simply by
having a CTH evaluate a <strong>receive</strong> expression in the init- or terminate
function. The macros <strong>?CT_HOOK_INIT_PROCESS</strong> (the process executing the hook
init function) and <strong>?CT_HOOK_TERMINATE_PROCESS</strong> (the process executing
the hook terminate function) each specifies the name of the correct <strong>Common Test</strong>
process to send a message to. This is done to return from the <strong>receive</strong>.
These macros are defined in <strong>ct.hrl</strong>.
</p><a name="example"></a><h3>Example CTH</h3><p>The following CTH logs information about a test run into a format 
parseable by <a href="../kernel/file#consult-1">file:consult/1</a> 
(in Kernel):
</p><pre><code class="">
 %%% Common Test Example Common Test Hook module.
 -module(example_cth).

 %% Callbacks
 -export([id/1]).
 -export([init/2]).

 -export([pre_init_per_suite/3]).
 -export([post_init_per_suite/4]).
 -export([pre_end_per_suite/3]).
 -export([post_end_per_suite/4]).

 -export([pre_init_per_group/4]).
 -export([post_init_per_group/5]).
 -export([pre_end_per_group/4]).
 -export([post_end_per_group/5]).

 -export([pre_init_per_testcase/4]).
 -export([post_init_per_testcase/5]).
 -export([pre_end_per_testcase/4]).
 -export([post_end_per_testcase/5]).

 -export([on_tc_fail/4]).
 -export([on_tc_skip/4]).

 -export([terminate/1]).

 -record(state, { file_handle, total, suite_total, ts, tcs, data }).

 %% Return a unique id for this CTH.
 id(Opts) -&gt;
   proplists:get_value(filename, Opts, "/tmp/file.log").

 %% Always called before any other callback function. Use this to initiate
 %% any common state. 
 init(Id, Opts) -&gt;
     {ok,D} = file:open(Id,[write]),
     {ok, #state{ file_handle = D, total = 0, data = [] }}.

 %% Called before init_per_suite is called.
 pre_init_per_suite(Suite,Config,State) -&gt;
     {Config, State#state{ suite_total = 0, tcs = [] }}.

 %% Called after init_per_suite.
 post_init_per_suite(Suite,Config,Return,State) -&gt;
     {Return, State}.

 %% Called before end_per_suite.
 pre_end_per_suite(Suite,Config,State) -&gt;
     {Config, State}.

 %% Called after end_per_suite.
 post_end_per_suite(Suite,Config,Return,State) -&gt;
     Data = {suites, Suite, State#state.suite_total, lists:reverse(State#state.tcs)},
     {Return, State#state{ data = [Data | State#state.data] ,
                           total = State#state.total + State#state.suite_total } }.

 %% Called before each init_per_group.
 pre_init_per_group(Suite,Group,Config,State) -&gt;
     {Config, State}.

 %% Called after each init_per_group.
 post_init_per_group(Suite,Group,Config,Return,State) -&gt;
     {Return, State}.

 %% Called before each end_per_group.
 pre_end_per_group(Suite,Group,Config,State) -&gt;
     {Config, State}.

 %% Called after each end_per_group.
 post_end_per_group(Suite,Group,Config,Return,State) -&gt;
     {Return, State}.

 %% Called before each init_per_testcase.
 pre_init_per_testcase(Suite,TC,Config,State) -&gt;
     {Config, State#state{ ts = now(), total = State#state.suite_total + 1 } }.

 %% Called after each init_per_testcase (immediately before the test case).
 post_init_per_testcase(Suite,TC,Config,Return,State) -&gt;
     {Return, State}

%% Called before each end_per_testcase (immediately after the test case).
 pre_end_per_testcase(Suite,TC,Config,State) -&gt;
     {Config, State}.

 %% Called after each end_per_testcase.
 post_end_per_testcase(Suite,TC,Config,Return,State) -&gt;
     TCInfo = {testcase, Suite, TC, Return, timer:now_diff(now(), State#state.ts)},
     {Return, State#state{ ts = undefined, tcs = [TCInfo | State#state.tcs] } }.

 %% Called after post_init_per_suite, post_end_per_suite, post_init_per_group,
 %% post_end_per_group and post_end_per_testcase if the suite, group or test case failed.
 on_tc_fail(Suite, TC, Reason, State) -&gt;
     State.

 %% Called when a test case is skipped by either user action
 %% or due to an init function failing.  
 on_tc_skip(Suite, TC, Reason, State) -&gt;
     State.

 %% Called when the scope of the CTH is done
 terminate(State) -&gt;
     io:format(State#state.file_handle, "~p.~n",
                [{test_run, State#state.total, State#state.data}]),
     file:close(State#state.file_handle),
     ok.</code></pre><a name="builtin_cths"></a><h3>Built-In CTHs</h3><p><strong>Common Test</strong> is delivered with some general-purpose CTHs that
can be enabled by the user to provide generic testing functionality.
Some of these CTHs are enabled by default when <strong>common_test</strong> is started to run.
They can be disabled by setting <strong>enable_builtin_hooks</strong> to
<strong>false</strong> on the command line or in the test specification. The following
two CTHs are delivered with <strong>Common Test</strong>:</p><dl><dt><strong>cth_log_redirect</strong></dt><dd> <p>Built-in</p> <p>Captures all log events that would normally be printed by the default
logger handler, and prints them to the current test case log.
If an event cannot be associated with a test case, it is printed in
the <strong>Common Test</strong> framework log.
This happens for test cases running in parallel and events occuring
in-between test cases. You can configure the level of
<a href="./sasl_app">SASL</a> reports
using the normal SASL mechanisms.</p> </dd><dt><strong>cth_surefire</strong></dt><dd> <p>Not built-in</p> <p>Captures all test results and outputs them as surefire
XML into a file. The created file is by default
called <strong>junit_report.xml</strong>. The file name can be changed by
setting option <strong>path</strong> for this hook, for example:</p> <p><strong>-ct_hooks cth_surefire [{path,"/tmp/report.xml"}]</strong></p> <p>If option <strong>url_base</strong> is set, an extra
attribute named <strong>url</strong> is added to each
<strong>testsuite</strong> and <strong>testcase</strong> XML element. The value
is constructed from <strong>url_base</strong> and a relative path
to the test suite or test case log, respectively, for example:</p> <p><strong>-ct_hooks cth_surefire [{url_base, "http://myserver.com/"}]</strong></p> <p>gives an URL attribute value similar to</p> <p><strong>"http://myserver.com/ct_run.ct@myhost.2012-12-12_11.19.39/ x86_64-unknown-linux-gnu.my_test.logs/run.2012-12-12_11.19.39/suite.log.html"</strong></p> <p>Surefire XML can, for example, be used by Jenkins to display test
results.</p> </dd></dl><h3>Goals</h3><p>It is not possible to prove that a program is correct by
testing. On the contrary, it has been formally proven that it is
impossible to prove programs in general by testing. Theoretical
program proofs or plain examination of code can be viable options
for those wishing to certify that a program is correct. The test
server, as it is based on testing, cannot be used for
certification. Its intended use is instead to (cost effectively)
<em>find bugs</em>. A successful test suite is one that reveals a
bug. If a test suite results in OK, then we know very little that
we did not know before.</p><h3>What to Test</h3><p>
There are many kinds of test suites. Some concentrate on
calling every function or command (in the documented way) in 
a certain interface.
Some others do the same, but use all kinds of illegal
parameters, and verify that the server stays alive and rejects
the requests with reasonable error codes. Some test suites
simulate an application (typically consisting of a few modules of
an application), some try to do tricky requests in general, and some
test suites even test internal functions with help of special
Load Modules on target.</p><p>Another interesting category of test suites is the one
checking that fixed bugs do not reoccur. When a bugfix is introduced,
a test case that checks for that specific bug is written
and submitted to the affected test suites.</p><p>Aim for finding bugs. Write whatever test that has the highest
probability of finding a bug, now or in the future. Concentrate
more on the critical parts. Bugs in critical subsystems are much
more expensive than others.</p><p>Aim for functionality testing rather than implementation
details. Implementation details change quite often, and the test
suites are to be long lived. Implementation details often differ
on different platforms and versions. If implementation details
must be tested, try to factor them out into separate test
cases. These test cases can later be rewritten or skipped.</p><p>Also, aim for testing everything once, no less, no more. It is
not effective having every test case fail only because one
function in the interface changed.</p></body></html>